{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NER_ass2_draft.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKhhaHUvYgoa"
      },
      "source": [
        "#Named Entity Recognition\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBdXq695pyuS"
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate\n",
        "drive = None\n",
        "def authenticate():\n",
        "    global drive\n",
        "    auth.authenticate_user()\n",
        "    gauth = GoogleAuth()\n",
        "    gauth.credentials = GoogleCredentials.get_application_default()\n",
        "    drive = GoogleDrive(gauth)\n",
        "\n",
        "#Download files\n",
        "def downloadFiles(fileIds):\n",
        "    authenticate()\n",
        "    for fileId in fileIds:    \n",
        "        downloaded = drive.CreateFile({\"id\": fileId[1]})\n",
        "        downloaded.GetContentFile(fileId[0])"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egNMhYx7qFs7"
      },
      "source": [
        "#Download file if not existing\n",
        "\n",
        "try:\n",
        "  _ = open(\"train.csv\", \"r\")\n",
        "except:\n",
        "  downloadFiles([[\"train.csv\", \"1Jvl2fKMwW5ASI1VjdJp_suGDVmHyLmCs\"]])\n",
        "\n",
        "try:\n",
        "  _ = open(\"val.csv\", \"r\")\n",
        "except:\n",
        "  downloadFiles([[\"val.csv\", \"1el2udPaXY7d5T6KjPW9h12JQtWzTxw8f\"]])\n",
        "\n",
        "try:\n",
        "  _ = open(\"test_without_labels.csv\", \"r\")\n",
        "except:\n",
        "  downloadFiles([[\"test_without_labels.csv\", \"1-AvWF2a9s0PQxq_ek2KxTgt0CgFFxLme\"]])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xY4scQ1RUve"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "from sklearn.linear_model import Perceptron\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.linear_model import SGDClassifier\n",
        "# from sklearn.linear_model import PassiveAggressiveClassifier\n",
        "# from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsURxyItqNCT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "e01c1509-c92e-49df-a933-9e0f6ba184cc"
      },
      "source": [
        "dftrain = pd.read_csv(\"train.csv\")\n",
        "dfval = pd.read_csv(\"val.csv\")\n",
        "dftest = pd.read_csv(\"test_without_labels.csv\")\n",
        "\n",
        "dftrain['sents'] = dftrain['sents'].apply(lambda x: x.lower())\n",
        "dfval['sents'] = dfval['sents'].apply(lambda x: x.lower())\n",
        "dftest['sents'] = dftest['sents'].apply(lambda x: x.lower())\n",
        "dftrain.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sents</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>operation steel curtain ( arabic : ا ل ح ج ا ب...</td>\n",
              "      <td>O O O O O O O O O O O O O O O O O O O O O O O ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>the hospital has facilities for mri and ct sca...</td>\n",
              "      <td>B-Location I-Location O O O O O O O O O O O O ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>the operation was important in that it was the...</td>\n",
              "      <td>O O O O O O O O O O O O O O B-Organisation I-O...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>this was my first visit to uzbekistan and an i...</td>\n",
              "      <td>O O B-Person O O O B-Location O O O O O O O O ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>the group was founded by sheikh abu omar al - ...</td>\n",
              "      <td>B-Organisation I-Organisation O O O B-Person I...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               sents                                             labels\n",
              "0  operation steel curtain ( arabic : ا ل ح ج ا ب...  O O O O O O O O O O O O O O O O O O O O O O O ...\n",
              "1  the hospital has facilities for mri and ct sca...  B-Location I-Location O O O O O O O O O O O O ...\n",
              "2  the operation was important in that it was the...  O O O O O O O O O O O O O O B-Organisation I-O...\n",
              "3  this was my first visit to uzbekistan and an i...  O O B-Person O O O B-Location O O O O O O O O ...\n",
              "4  the group was founded by sheikh abu omar al - ...  B-Organisation I-Organisation O O O B-Person I..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GASvK913tAmB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "0c650534-b00b-4f5d-efec-248755a45ed4"
      },
      "source": [
        "dftest.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sents</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>carter thanked abadi for nearly two years of a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>analyses performed by the hospital lab include...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>the last meeting of the small group took place...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\" \" \" as we meet here , we are hoping to gener...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>one jet bombed a school for girls in a souther...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               sents\n",
              "0  carter thanked abadi for nearly two years of a...\n",
              "1  analyses performed by the hospital lab include...\n",
              "2  the last meeting of the small group took place...\n",
              "3  \" \" \" as we meet here , we are hoping to gener...\n",
              "4  one jet bombed a school for girls in a souther..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfakfv0CRwAB"
      },
      "source": [
        "## 1 Data Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8tV_Brmowa5",
        "outputId": "b794d0cb-1c1b-47e9-e197-a38d41d026c5"
      },
      "source": [
        "# Generate word_to_ix and tag_to_ix\n",
        "#from lab 09\n",
        "word_to_ix = {}\n",
        "for sentence in list(dftrain.sents)+list(dftest.sents)+list(dfval.sents):\n",
        "    for word in sentence.split():\n",
        "        if word not in word_to_ix:\n",
        "            word_to_ix[word] = len(word_to_ix)\n",
        "word_list = list(word_to_ix.keys())\n",
        "print(word_list[:5])\n",
        "\n",
        "START_TAG = \"<START>\"\n",
        "STOP_TAG = \"<STOP>\"\n",
        "tag_to_ix = {START_TAG:0, STOP_TAG:1}\n",
        "for tags in list(dftrain.labels)+list(dfval.labels):\n",
        "    for tag in tags.split():\n",
        "        if tag not in tag_to_ix:\n",
        "            tag_to_ix[tag] = len(tag_to_ix)\n",
        "print(tag_to_ix)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['operation', 'steel', 'curtain', '(', 'arabic']\n",
            "{'<START>': 0, '<STOP>': 1, 'O': 2, 'B-Organisation': 3, 'I-Organisation': 4, 'B-Temporal': 5, 'I-Temporal': 6, 'B-Nationality': 7, 'B-Location': 8, 'I-Location': 9, 'B-Person': 10, 'I-Person': 11, 'B-DocumentReference': 12, 'I-DocumentReference': 13, 'B-Money': 14, 'I-Money': 15, 'B-Quantity': 16, 'B-MilitaryPlatform': 17, 'I-MilitaryPlatform': 18, 'B-Weapon': 19, 'I-Weapon': 20, 'I-Quantity': 21, 'I-Nationality': 22}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lugzrnZCozPr",
        "outputId": "0ed47382-2443-45fb-dcfc-0938e5938c6f"
      },
      "source": [
        "#from lab 9\n",
        "def to_index(data, to_ix):\n",
        "    input_index_list = []\n",
        "    for sent in data:\n",
        "        input_index_list.append([to_ix[w] for w in sent.split()])\n",
        "    return input_index_list\n",
        "\n",
        "train_input_index =  to_index(dftrain.sents,word_to_ix)\n",
        "train_output_index = to_index(dftrain.labels,tag_to_ix)\n",
        "val_input_index = to_index(dfval.sents,word_to_ix)\n",
        "val_output_index = to_index(dfval.labels,tag_to_ix)\n",
        "test_input_index = to_index(dftest.sents,word_to_ix)\n",
        "print(train_input_index[0])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 6, 10, 6, 7, 11, 12, 7, 6, 13, 14, 15, 16, 17, 18, 19, 20, 21, 0, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 32, 38, 39, 40, 32, 41, 42, 43]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Dea616Bn39X"
      },
      "source": [
        "## 2 Input Embeddings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKjdOrHZn8tx"
      },
      "source": [
        "####2.1 Syntactic Textual Feature Embedding (PoS tag information, Depedency Path)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEDTUlOkn7yc",
        "outputId": "209b0c04-0f26-432c-ddd0-1d11ca148917"
      },
      "source": [
        "# from lab 7 and lab 6\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "class myTokenizer(object):\n",
        "    def __init__(self, vocab):\n",
        "        self.vocab = vocab\n",
        "\n",
        "    def __call__(self, text):\n",
        "        words=text.split()\n",
        "        # All tokens 'own' a subsequent space character in this tokenizer\n",
        "        spaces = [True] * len(words)\n",
        "        return spacy.tokens.doc.Doc(self.vocab, words=words, spaces=spaces)\n",
        "nlp.tokenizer = myTokenizer(nlp.vocab)\n",
        "print(nlp.pipe_names)\n",
        "\n",
        "def tagging(data):\n",
        "    POS, DEP = [],[]\n",
        "    for sent in data:\n",
        "        parse = nlp(sent)\n",
        "        assert len(parse)==len(sent.split()),\"should be the same length\"\n",
        "        postags = []\n",
        "        deptags = []\n",
        "        for token in parse:\n",
        "            postags.append(token.pos_)\n",
        "            deptags.append(token.dep_)\n",
        "        assert len(postags)==len(sent.split()),\"should be the same length\"\n",
        "        POS.append(postags)\n",
        "        DEP.append(deptags)   \n",
        "    return POS, DEP\n",
        "\n",
        "trainPOS, trainDEP = tagging(dftrain.sents)\n",
        "valPOS, valDEP = tagging(dfval.sents)\n",
        "testPOS, testDEP = tagging(dftest.sents)\n",
        "\n",
        "\n",
        "def get_tagix(taglist):\n",
        "  tmp = {}\n",
        "  for tags in taglist:\n",
        "    for tag in tags:\n",
        "      if tag not in tmp:\n",
        "        tmp[tag] = len(tmp)\n",
        "  return tmp\n",
        "tag_to_ix_POS = get_tagix(trainPOS)\n",
        "tag_to_ix_DEP = get_tagix(trainDEP)\n",
        "POS_list = list(tag_to_ix_POS.keys())\n",
        "DEP_list = list(tag_to_ix_DEP.keys())\n",
        "\n",
        "def to_index_1(data, to_ix):\n",
        "    input_index_list = []\n",
        "    for sent in data:\n",
        "        input_index_list.append([to_ix[w] for w in sent])\n",
        "    return input_index_list\n",
        "\n",
        "train_POS_index =  to_index_1(trainPOS,tag_to_ix_POS)\n",
        "train_DEP_index =  to_index_1(trainDEP,tag_to_ix_DEP)\n",
        "val_POS_index =  to_index_1(valPOS,tag_to_ix_POS)\n",
        "val_DEP_index =  to_index_1(valDEP,tag_to_ix_DEP)\n",
        "test_POS_index =  to_index_1(testPOS,tag_to_ix_POS)\n",
        "test_DEP_index =  to_index_1(testDEP,tag_to_ix_DEP)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['tagger', 'parser', 'ner']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVB9t-xrVGmq",
        "outputId": "1fab8d52-cfd6-48c9-bb27-5a1e3b3217ec"
      },
      "source": [
        "embedding_matrix_POS = []\n",
        "EMBEDDING_DIM_POS = len(POS_list)\n",
        "for i,word in enumerate(POS_list):\n",
        "  a = [0]*EMBEDDING_DIM_POS\n",
        "  a[i] = 1\n",
        "  embedding_matrix_POS.append(a)\n",
        "embedding_matrix_POS = np.array(embedding_matrix_POS)\n",
        "print(embedding_matrix_POS.shape)\n",
        "\n",
        "embedding_matrix_DEP = []\n",
        "EMBEDDING_DIM_DEP = len(DEP_list)\n",
        "for i,word in enumerate(DEP_list):\n",
        "  a = [0]*EMBEDDING_DIM_DEP\n",
        "  a[i] = 1\n",
        "  embedding_matrix_DEP.append(a)\n",
        "embedding_matrix_DEP = np.array(embedding_matrix_DEP)\n",
        "print(embedding_matrix_DEP.shape)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(17, 17)\n",
            "(44, 44)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c83NbNJMoOwQ"
      },
      "source": [
        "###2.2 Semantic Textual Feature Embedding: Word Embeddings (FasText, Pretrained Glove-twitter-25)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZWrUPGMoX0X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fefa96eb-59a3-44e2-bab3-788af2fec7ec"
      },
      "source": [
        "# FasText 100\n",
        "# For data processing\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stoplist = stopwords.words('english')\n",
        "import re\n",
        "# For parsing our XML data\n",
        "from lxml import etree \n",
        "\n",
        "# For implementing the word2vec family of algorithms\n",
        "from gensim.models import Word2Vec,FastText\n",
        "\n",
        "# Data preprocessing for word embedding does't remove stopwords, but should remove pure numbers\n",
        "id = '1B47OiEiG2Lo1jUY6hy_zMmHBxfKQuJ8-'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('ted_en-20160408.xml') \n",
        "\n",
        "# Please comment your code\n",
        "targetXML=open('ted_en-20160408.xml', 'r', encoding='UTF8')\n",
        "# Getting contents of <content> tag from the xml file\n",
        "target_text = etree.parse(targetXML)\n",
        "parse_text = '\\n'.join(target_text.xpath('//content/text()'))\n",
        "# Removing \"Sound-effect labels\" using regular expression (regex) (i.e. (Audio), (Laughter))\n",
        "content_text = re.sub(r'\\([^)]*\\)', '', parse_text)\n",
        "# Tokenising the sentence to process it by using NLTK library\n",
        "sent_text=sent_tokenize(content_text)\n",
        "\n",
        "# Removing punctuation and changing all characters to lower case\n",
        "normalized_text = []\n",
        "for string in sent_text:\n",
        "     tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
        "     normalized_text.append(tokens)\n",
        "sentences=[]\n",
        "# Tokenising each sentence to process individual word\n",
        "sentences=[[t for t in word_tokenize(sentence) if re.match(r\"[a-z]\",t) is not None] for sentence in normalized_text]\n",
        "\n",
        "# Prints only 2 (tokenised) sentences\n",
        "print(sentences[:2])\n",
        "\n",
        "EMBEDDING_DIM_Fast = 100\n",
        "fast_sg_model = FastText(sentences, size=EMBEDDING_DIM_Fast, window=5, min_count=5, workers=2, sg=1)\n",
        "\n",
        "\n",
        "embedding_matrix_fast = []\n",
        "for word in word_list:\n",
        "    try:\n",
        "        embedding_matrix_fast.append(fast_sg_model.wv[word])\n",
        "    except:\n",
        "        embedding_matrix_fast.append([0]*EMBEDDING_DIM_Fast)\n",
        "embedding_matrix_fast = np.array(embedding_matrix_fast)\n",
        "embedding_matrix_fast.shape\n",
        "# from my ass1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[['here', 'are', 'two', 'reasons', 'companies', 'fail', 'they', 'only', 'do', 'more', 'of', 'the', 'same', 'or', 'they', 'only', 'do', 'what', 's', 'new'], ['to', 'me', 'the', 'real', 'real', 'solution', 'to', 'quality', 'growth', 'is', 'figuring', 'out', 'the', 'balance', 'between', 'two', 'activities', 'exploration', 'and', 'exploitation']]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3957, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhcqveivoVni",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35b76ddb-38e7-4c07-ca05-901eaca4ab2c"
      },
      "source": [
        "# Pretrained Glove-twitter-25\n",
        "import gensim.downloader as api\n",
        "word_emb_model = api.load(\"glove-twitter-25\") \n",
        "\n",
        "EMBEDDING_DIM = 25\n",
        "\n",
        "embedding_matrix = []\n",
        "for word in word_list:\n",
        "    try:\n",
        "        embedding_matrix.append(word_emb_model.wv[word])\n",
        "    except:\n",
        "        embedding_matrix.append([0]*EMBEDDING_DIM)\n",
        "embedding_matrix = np.array(embedding_matrix)\n",
        "embedding_matrix.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 104.8/104.8MB downloaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3957, 25)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KachEg6yW8NA",
        "outputId": "fa8671a0-041f-46fa-d737-8ead49a369fa"
      },
      "source": [
        "# Pretrained Glove-twitter-50\n",
        "import gensim.downloader as api\n",
        "word_emb_model = api.load(\"glove-twitter-50\") \n",
        "\n",
        "EMBEDDING_DIM_50 = 50\n",
        "\n",
        "embedding_matrix_50 = []\n",
        "for word in word_list:\n",
        "    try:\n",
        "        embedding_matrix_50.append(word_emb_model.wv[word])\n",
        "    except:\n",
        "        embedding_matrix_50.append([0]*EMBEDDING_DIM_50)\n",
        "embedding_matrix_50 = np.array(embedding_matrix_50)\n",
        "embedding_matrix_50.shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 199.5/199.5MB downloaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3957, 50)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SkPS05OpOzi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-3SZOropP97"
      },
      "source": [
        "## 3 Baseline Bi-LSTM CRF\n",
        "Use Glove-twitter-25 as baseline input embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1ExTHve6e73"
      },
      "source": [
        "#### 3.1 help Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_nqotmmK0-g"
      },
      "source": [
        "HIDDEN_DIM = 50\n",
        "max_epoch = 20\n",
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "def decode_output(output_list):\n",
        "    ix_to_tag = {v:k for k,v in tag_to_ix.items()}\n",
        "    return [ix_to_tag[output] for output in output_list]\n",
        "    \n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5AgRWakkfmT"
      },
      "source": [
        "#from lab 9\n",
        "def cal_acc(model, input_index, output_index):\n",
        "    ground_truth = []\n",
        "    predicted = []\n",
        "    for i,idxs in enumerate(input_index):\n",
        "        ground_truth += output_index[i]\n",
        "        score, pred = model(torch.tensor(idxs, dtype=torch.long).to(device))\n",
        "        predicted += pred\n",
        "    accuracy = sum(np.array(ground_truth) == np.array(predicted))/len(ground_truth)\n",
        "    return predicted, ground_truth, accuracy\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "def argmax(vec):\n",
        "    # return the argmax as a python int\n",
        "    _, idx = torch.max(vec, 1)\n",
        "    return idx.item()\n",
        "\n",
        "\n",
        "def prepare_sequence(seq, to_ix):\n",
        "    idxs = [to_ix[w] for w in seq]\n",
        "    return torch.tensor(idxs, dtype=torch.long)\n",
        "    \n",
        "# Compute log sum exp in a numerically stable way for the forward algorithm\n",
        "def log_sum_exp(vec):\n",
        "    max_score = vec[0, argmax(vec)]\n",
        "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
        "    return max_score + \\\n",
        "        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
        "\n",
        "class BiLSTM_CRF(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim, embedding_matrix):\n",
        "        super(BiLSTM_CRF, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "        self.tagset_size = len(tag_to_ix)\n",
        "        self.embedding_matrix = embedding_matrix\n",
        "\n",
        "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        \"\"\"Here we use the embedding matrix as the initial weights of nn.Embedding\"\"\"\n",
        "        self.word_embeds.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
        "        \n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
        "                            num_layers=1, bidirectional=True)\n",
        "\n",
        "        # Maps the output of the LSTM into tag space.\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
        "\n",
        "        # Matrix of transition parameters.  Entry i,j is the score of\n",
        "        # transitioning *to* i *from* j.\n",
        "        self.transitions = nn.Parameter(\n",
        "            torch.randn(self.tagset_size, self.tagset_size))\n",
        "\n",
        "        # These two statements enforce the constraint that we never transfer\n",
        "        # to the start tag and we never transfer from the stop tag\n",
        "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
        "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
        "\n",
        "        self.hidden = self.init_hidden()\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return (torch.randn(2, 1, self.hidden_dim // 2).to(device),\n",
        "                torch.randn(2, 1, self.hidden_dim // 2).to(device))\n",
        "\n",
        "    def _forward_alg(self, feats):\n",
        "        # Do the forward algorithm to compute the partition function\n",
        "        init_alphas = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "        # START_TAG has all of the score.\n",
        "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
        "\n",
        "        # Wrap in a variable so that we will get automatic backprop\n",
        "        forward_var = init_alphas\n",
        "\n",
        "        # Iterate through the sentence\n",
        "        for feat in feats:\n",
        "            alphas_t = []  # The forward tensors at this timestep\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # broadcast the emission score: it is the same regardless of\n",
        "                # the previous tag\n",
        "                emit_score = feat[next_tag].view(\n",
        "                    1, -1).expand(1, self.tagset_size)\n",
        "                # the ith entry of trans_score is the score of transitioning to\n",
        "                # next_tag from i\n",
        "                trans_score = self.transitions[next_tag].view(1, -1)\n",
        "                # The ith entry of next_tag_var is the value for the\n",
        "                # edge (i -> next_tag) before we do log-sum-exp\n",
        "                next_tag_var = forward_var + trans_score + emit_score\n",
        "                # The forward variable for this tag is log-sum-exp of all the\n",
        "                # scores.\n",
        "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
        "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        alpha = log_sum_exp(terminal_var)\n",
        "        return alpha\n",
        "\n",
        "    def _get_lstm_features(self, sentence):\n",
        "        self.hidden = self.init_hidden()\n",
        "        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
        "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
        "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
        "        lstm_feats = self.hidden2tag(lstm_out)\n",
        "        return lstm_feats\n",
        "\n",
        "    def _score_sentence(self, feats, tags):\n",
        "        # Gives the score of a provided tag sequence\n",
        "        score = torch.zeros(1).to(device)\n",
        "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long).to(device), tags])\n",
        "        for i, feat in enumerate(feats):\n",
        "            score = score + \\\n",
        "                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
        "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
        "        return score\n",
        "\n",
        "    def _viterbi_decode(self, feats):\n",
        "        backpointers = []\n",
        "\n",
        "        # Initialize the viterbi variables in log space\n",
        "        init_vvars = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
        "\n",
        "        # forward_var at step i holds the viterbi variables for step i-1\n",
        "        forward_var = init_vvars\n",
        "        for feat in feats:\n",
        "            bptrs_t = []  # holds the backpointers for this step\n",
        "            viterbivars_t = []  # holds the viterbi variables for this step\n",
        "\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
        "                # previous step, plus the score of transitioning\n",
        "                # from tag i to next_tag.\n",
        "                # We don't include the emission scores here because the max\n",
        "                # does not depend on them (we add them in below)\n",
        "                next_tag_var = forward_var + self.transitions[next_tag]\n",
        "                best_tag_id = argmax(next_tag_var)\n",
        "                bptrs_t.append(best_tag_id)\n",
        "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
        "            # Now add in the emission scores, and assign forward_var to the set\n",
        "            # of viterbi variables we just computed\n",
        "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
        "            backpointers.append(bptrs_t)\n",
        "\n",
        "        # Transition to STOP_TAG\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        best_tag_id = argmax(terminal_var)\n",
        "        path_score = terminal_var[0][best_tag_id]\n",
        "\n",
        "        # Follow the back pointers to decode the best path.\n",
        "        best_path = [best_tag_id]\n",
        "        for bptrs_t in reversed(backpointers):\n",
        "            best_tag_id = bptrs_t[best_tag_id]\n",
        "            best_path.append(best_tag_id)\n",
        "        # Pop off the start tag (we dont want to return that to the caller)\n",
        "        start = best_path.pop()\n",
        "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
        "        best_path.reverse()\n",
        "        return path_score, best_path\n",
        "\n",
        "    def neg_log_likelihood(self, sentence, tags):\n",
        "        feats = self._get_lstm_features(sentence)\n",
        "        forward_score = self._forward_alg(feats)\n",
        "        gold_score = self._score_sentence(feats, tags)\n",
        "        return forward_score - gold_score\n",
        "\n",
        "    def forward(self, sentence):  # dont confuse this with _forward_alg above.\n",
        "        # Get the emission scores from the BiLSTM\n",
        "        lstm_feats = self._get_lstm_features(sentence)\n",
        "\n",
        "        # Find the best path, given the features.\n",
        "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
        "        return score, tag_seq"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_82UaXEOhoQQ"
      },
      "source": [
        "#### 3.2 Initialize Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyuzZ_et6FD7"
      },
      "source": [
        "#from lab9\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "HIDDEN_DIM = 50\n",
        "\n",
        "model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM,embedding_matrix).to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9UiokVOjPUn"
      },
      "source": [
        "####3.3 Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0CMFVSwlLru",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11337c83-20af-4564-8bcf-c20f4062ae92"
      },
      "source": [
        "\"\"\"Each epoch will take about 2-3 minutes\"\"\"\n",
        "\n",
        "import datetime\n",
        "\n",
        "for epoch in range(20):  \n",
        "    time1 = datetime.datetime.now()\n",
        "    train_loss = 0\n",
        "\n",
        "    model.train()\n",
        "    for i, idxs in enumerate(train_input_index):\n",
        "        tags_index = train_output_index[i]\n",
        "\n",
        "        # Step 1. Remember that Pytorch accumulates gradients.\n",
        "        # We need to clear them out before each instance\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Step 2. Get our inputs ready for the network, that is,\n",
        "        # turn them into Tensors of word indices.\n",
        "        sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "\n",
        "        # Step 3. Run our forward pass.\n",
        "        loss = model.neg_log_likelihood(sentence_in, targets)\n",
        "\n",
        "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "        # calling optimizer.step()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss+=loss.item()\n",
        "\n",
        "    model.eval()\n",
        "    _, _, train_acc = cal_acc(model,train_input_index,train_output_index)\n",
        "    _, _, val_acc = cal_acc(model,val_input_index,val_output_index)\n",
        "\n",
        "    val_loss = 0\n",
        "    for i, idxs in enumerate(val_input_index):\n",
        "        tags_index = val_output_index[i]\n",
        "        sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "        loss = model.neg_log_likelihood(sentence_in, targets)\n",
        "        val_loss+=loss.item()\n",
        "    time2 = datetime.datetime.now()\n",
        "\n",
        "    print(\"Epoch:%d, Training loss: %.2f, train acc: %.4f, val loss: %.2f, val acc: %.4f, time: %.2fs\" %(epoch+1, train_loss,train_acc, val_loss, val_acc, (time2-time1).total_seconds()))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:1, Training loss: 9071.29, train acc: 0.7905, val loss: 3699.75, val acc: 0.7357, time: 184.14s\n",
            "Epoch:2, Training loss: 6982.21, train acc: 0.8123, val loss: 3260.01, val acc: 0.7573, time: 182.79s\n",
            "Epoch:3, Training loss: 5854.00, train acc: 0.8298, val loss: 2939.77, val acc: 0.7711, time: 181.24s\n",
            "Epoch:4, Training loss: 5123.93, train acc: 0.8426, val loss: 2678.40, val acc: 0.7829, time: 181.81s\n",
            "Epoch:5, Training loss: 4525.96, train acc: 0.8532, val loss: 2552.76, val acc: 0.7884, time: 181.55s\n",
            "Epoch:6, Training loss: 4080.88, train acc: 0.8629, val loss: 2416.15, val acc: 0.7911, time: 180.66s\n",
            "Epoch:7, Training loss: 3686.76, train acc: 0.8737, val loss: 2275.35, val acc: 0.7977, time: 180.70s\n",
            "Epoch:8, Training loss: 3345.84, train acc: 0.8834, val loss: 2224.93, val acc: 0.8013, time: 181.17s\n",
            "Epoch:9, Training loss: 3047.98, train acc: 0.8924, val loss: 2188.44, val acc: 0.8039, time: 181.91s\n",
            "Epoch:10, Training loss: 2797.06, train acc: 0.8986, val loss: 2190.67, val acc: 0.7998, time: 181.92s\n",
            "Epoch:11, Training loss: 2545.58, train acc: 0.9112, val loss: 2146.17, val acc: 0.8083, time: 180.46s\n",
            "Epoch:12, Training loss: 2321.95, train acc: 0.9174, val loss: 2158.96, val acc: 0.8074, time: 180.77s\n",
            "Epoch:13, Training loss: 2138.59, train acc: 0.9231, val loss: 2179.98, val acc: 0.8075, time: 180.74s\n",
            "Epoch:14, Training loss: 1978.06, train acc: 0.9294, val loss: 2185.05, val acc: 0.8136, time: 181.42s\n",
            "Epoch:15, Training loss: 1827.70, train acc: 0.9319, val loss: 2221.33, val acc: 0.8072, time: 180.44s\n",
            "Epoch:16, Training loss: 1664.81, train acc: 0.9339, val loss: 2286.30, val acc: 0.8075, time: 180.34s\n",
            "Epoch:17, Training loss: 1539.52, train acc: 0.9377, val loss: 2316.70, val acc: 0.8108, time: 180.29s\n",
            "Epoch:18, Training loss: 1435.05, train acc: 0.9450, val loss: 2335.85, val acc: 0.8064, time: 180.23s\n",
            "Epoch:19, Training loss: 1352.46, train acc: 0.9473, val loss: 2380.91, val acc: 0.8068, time: 180.10s\n",
            "Epoch:20, Training loss: 1231.55, train acc: 0.9489, val loss: 2499.95, val acc: 0.8045, time: 180.32s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uj7ANbv_jSzI"
      },
      "source": [
        "###3.4 Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIh-mWrvi6Pw"
      },
      "source": [
        "y_true,y_pred,_ = cal_acc(model,val_input_index,val_output_index)\n",
        "\n",
        "y_true_decode = decode_output(y_true)\n",
        "y_pred_decode = decode_output(y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAnJVsyPq5kR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "999ddfcf-bc39-4e8b-acf1-16a9333b15f0"
      },
      "source": [
        "print(classification_report(y_true_decode,y_pred_decode,digits=4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                     precision    recall  f1-score   support\n",
            "\n",
            "B-DocumentReference     0.2000    0.2667    0.2286        15\n",
            "         B-Location     0.5860    0.6566    0.6193       166\n",
            " B-MilitaryPlatform     0.0625    0.2000    0.0952         5\n",
            "            B-Money     0.2000    0.5000    0.2857         2\n",
            "      B-Nationality     0.1250    0.1429    0.1333         7\n",
            "     B-Organisation     0.6857    0.7138    0.6995       269\n",
            "           B-Person     0.7745    0.8061    0.7900        98\n",
            "         B-Quantity     0.6182    0.6939    0.6538        49\n",
            "         B-Temporal     0.6809    0.6154    0.6465        52\n",
            "           B-Weapon     0.1579    0.2500    0.1935        24\n",
            "I-DocumentReference     0.2771    0.3898    0.3239        59\n",
            "         I-Location     0.4453    0.5784    0.5032       204\n",
            " I-MilitaryPlatform     0.1250    1.0000    0.2222         2\n",
            "            I-Money     0.6000    1.0000    0.7500         6\n",
            "      I-Nationality     0.0000    0.0000    0.0000         0\n",
            "     I-Organisation     0.6298    0.7507    0.6850       349\n",
            "           I-Person     0.7861    0.8718    0.8267       156\n",
            "         I-Quantity     0.2692    0.5833    0.3684        12\n",
            "         I-Temporal     0.7419    0.8070    0.7731        57\n",
            "           I-Weapon     0.1111    0.3333    0.1667        15\n",
            "                  O     0.9295    0.8530    0.8896      3727\n",
            "\n",
            "           accuracy                         0.8045      5274\n",
            "          macro avg     0.4288    0.5720    0.4693      5274\n",
            "       weighted avg     0.8337    0.8045    0.8163      5274\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_xjXrRFpcV8"
      },
      "source": [
        "## 4 Evaluate Input Embeddings\n",
        "Setting different input embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mghku7wQgGRe"
      },
      "source": [
        "####4.1 Bi-LSTM CRF with FastText"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGH5kBYigTUp",
        "outputId": "3d0cfda9-6d15-48b3-b2fa-6a600d462eef"
      },
      "source": [
        "p_EMBEDDING_DIM = EMBEDDING_DIM_Fast\n",
        "p_embedding_matrix = embedding_matrix_fast\n",
        "HIDDEN_DIM = 50\n",
        "\n",
        "\n",
        "# initialize model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, p_EMBEDDING_DIM, HIDDEN_DIM, p_embedding_matrix).to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
        "\n",
        "\n",
        "\"\"\"Each epoch will take about 2-3 minutes\"\"\"\n",
        "\n",
        "import datetime\n",
        "\n",
        "for epoch in range(20):  \n",
        "    time1 = datetime.datetime.now()\n",
        "    train_loss = 0\n",
        "\n",
        "    model.train()\n",
        "    for i, idxs in enumerate(train_input_index):\n",
        "        tags_index = train_output_index[i]\n",
        "\n",
        "        # Step 1. Remember that Pytorch accumulates gradients.\n",
        "        # We need to clear them out before each instance\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Step 2. Get our inputs ready for the network, that is,\n",
        "        # turn them into Tensors of word indices.\n",
        "        sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "\n",
        "        # Step 3. Run our forward pass.\n",
        "        loss = model.neg_log_likelihood(sentence_in, targets)\n",
        "\n",
        "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "        # calling optimizer.step()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss+=loss.item()\n",
        "\n",
        "    model.eval()\n",
        "    _, _, train_acc = cal_acc(model,train_input_index,train_output_index)\n",
        "    _, _, val_acc = cal_acc(model,val_input_index,val_output_index)\n",
        "\n",
        "    val_loss = 0\n",
        "    for i, idxs in enumerate(val_input_index):\n",
        "        tags_index = val_output_index[i]\n",
        "        sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "        loss = model.neg_log_likelihood(sentence_in, targets)\n",
        "        val_loss+=loss.item()\n",
        "    time2 = datetime.datetime.now()\n",
        "\n",
        "    print(\"Epoch:%d, Training loss: %.2f, train acc: %.4f, val loss: %.2f, val acc: %.4f, time: %.2fs\" %(epoch+1, train_loss,train_acc, val_loss, val_acc, (time2-time1).total_seconds()))\n",
        "\n",
        "y_true,y_pred,_ = cal_acc(model,val_input_index,val_output_index)\n",
        "y_true_decode = decode_output(y_true)\n",
        "y_pred_decode = decode_output(y_pred)\n",
        "\n",
        "print(classification_report(y_true_decode,y_pred_decode,digits=4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:1, Training loss: 14502.48, train acc: 0.7547, val loss: 4573.59, val acc: 0.7031, time: 180.45s\n",
            "Epoch:2, Training loss: 8867.72, train acc: 0.8143, val loss: 3619.82, val acc: 0.7518, time: 180.21s\n",
            "Epoch:3, Training loss: 6687.58, train acc: 0.8390, val loss: 3194.51, val acc: 0.7632, time: 179.21s\n",
            "Epoch:4, Training loss: 5434.80, train acc: 0.8568, val loss: 2924.22, val acc: 0.7711, time: 178.98s\n",
            "Epoch:5, Training loss: 4574.38, train acc: 0.8714, val loss: 2738.96, val acc: 0.7797, time: 178.17s\n",
            "Epoch:6, Training loss: 3907.04, train acc: 0.8897, val loss: 2567.59, val acc: 0.7929, time: 177.97s\n",
            "Epoch:7, Training loss: 3369.70, train acc: 0.9022, val loss: 2514.16, val acc: 0.7935, time: 177.58s\n",
            "Epoch:8, Training loss: 2945.96, train acc: 0.9146, val loss: 2406.40, val acc: 0.7984, time: 177.84s\n",
            "Epoch:9, Training loss: 2542.80, train acc: 0.9253, val loss: 2440.97, val acc: 0.8047, time: 177.37s\n",
            "Epoch:10, Training loss: 2208.59, train acc: 0.9341, val loss: 2491.27, val acc: 0.8036, time: 177.22s\n",
            "Epoch:11, Training loss: 1932.33, train acc: 0.9428, val loss: 2586.18, val acc: 0.8036, time: 177.17s\n",
            "Epoch:12, Training loss: 1723.94, train acc: 0.9516, val loss: 2577.85, val acc: 0.8034, time: 176.79s\n",
            "Epoch:13, Training loss: 1512.19, train acc: 0.9566, val loss: 2560.20, val acc: 0.8022, time: 176.77s\n",
            "Epoch:14, Training loss: 1375.30, train acc: 0.9564, val loss: 2703.75, val acc: 0.8011, time: 176.61s\n",
            "Epoch:15, Training loss: 1241.57, train acc: 0.9608, val loss: 2756.42, val acc: 0.8110, time: 176.85s\n",
            "Epoch:16, Training loss: 1135.70, train acc: 0.9634, val loss: 2849.35, val acc: 0.8077, time: 176.62s\n",
            "Epoch:17, Training loss: 1140.92, train acc: 0.9638, val loss: 2825.07, val acc: 0.8119, time: 176.89s\n",
            "Epoch:18, Training loss: 989.82, train acc: 0.9698, val loss: 2938.99, val acc: 0.8068, time: 176.61s\n",
            "Epoch:19, Training loss: 935.47, train acc: 0.9728, val loss: 2888.97, val acc: 0.8070, time: 176.29s\n",
            "Epoch:20, Training loss: 840.75, train acc: 0.9741, val loss: 2953.90, val acc: 0.8093, time: 176.50s\n",
            "                     precision    recall  f1-score   support\n",
            "\n",
            "B-DocumentReference     0.3000    0.6667    0.4138         9\n",
            "         B-Location     0.6290    0.6610    0.6446       177\n",
            " B-MilitaryPlatform     0.1250    0.4000    0.1905         5\n",
            "            B-Money     0.2000    0.5000    0.2857         2\n",
            "      B-Nationality     0.2500    0.4000    0.3077         5\n",
            "     B-Organisation     0.6179    0.7090    0.6603       244\n",
            "           B-Person     0.8235    0.8485    0.8358        99\n",
            "         B-Quantity     0.5455    0.6818    0.6061        44\n",
            "         B-Temporal     0.7234    0.8095    0.7640        42\n",
            "           B-Weapon     0.1842    0.4667    0.2642        15\n",
            "I-DocumentReference     0.2410    0.8000    0.3704        25\n",
            "         I-Location     0.4906    0.5830    0.5328       223\n",
            " I-MilitaryPlatform     0.1250    1.0000    0.2222         2\n",
            "            I-Money     0.4000    1.0000    0.5714         4\n",
            "      I-Nationality     0.0000    0.0000    0.0000         0\n",
            "     I-Organisation     0.6154    0.7552    0.6781       339\n",
            "           I-Person     0.7168    0.8671    0.7848       143\n",
            "         I-Quantity     0.1923    0.5000    0.2778        10\n",
            "         I-Temporal     0.8065    0.9434    0.8696        53\n",
            "           I-Weapon     0.0889    0.3636    0.1429        11\n",
            "                  O     0.9444    0.8451    0.8920      3822\n",
            "\n",
            "           accuracy                         0.8117      5274\n",
            "          macro avg     0.4295    0.6572    0.4912      5274\n",
            "       weighted avg     0.8512    0.8117    0.8267      5274\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NH7wFFwXgHJt"
      },
      "source": [
        "####4.2 Bi-LSTM CRF with Glove-twitter-25 and POS information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0L82TiOhP2p",
        "outputId": "c387e50f-35ff-4cf1-dc55-f7bae5eea9ef"
      },
      "source": [
        "p_EMBEDDING_DIM = EMBEDDING_DIM\n",
        "p_embedding_matrix = embedding_matrix\n",
        "t__EMBEDDING_DIM = EMBEDDING_DIM_POS\n",
        "t_embedding_matrix = embedding_matrix_POS\n",
        "t_vocab = embedding_matrix_POS.shape[0]\n",
        "t_train_input_index = train_POS_index\n",
        "t_val_input_index = val_POS_index\n",
        "HIDDEN_DIM = 50\n",
        "\n",
        "def cal_acc(model, input_index, syn_input_index, output_index):\n",
        "    ground_truth = []\n",
        "    predicted = []\n",
        "    for i,idxs in enumerate(input_index):\n",
        "        syn_idxs = syn_input_index[i]\n",
        "        ground_truth += output_index[i]\n",
        "        sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "        synlist_in = torch.tensor(syn_idxs, dtype=torch.long).to(device)\n",
        "        score, pred = model(sentence_in,synlist_in)\n",
        "        predicted += pred\n",
        "    accuracy = sum(np.array(ground_truth) == np.array(predicted))/len(ground_truth)\n",
        "    return predicted, ground_truth, accuracy\n",
        "\n",
        "class BiLSTM_CRF(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):\n",
        "        super(BiLSTM_CRF, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "        self.tagset_size = len(tag_to_ix)\n",
        "\n",
        "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
        "        \"\"\"Here we use the embedding matrix as the initial weights of nn.Embedding\"\"\"\n",
        "        self.word_embeds.weight.data.copy_(torch.from_numpy(p_embedding_matrix))\n",
        "\n",
        "        self.tag_embeds = nn.Embedding(t_vocab, t__EMBEDDING_DIM)\n",
        "        self.tag_embeds.weight.data.copy_(torch.from_numpy(t_embedding_matrix))\n",
        "        \n",
        "        self.lstm = nn.LSTM(embedding_dim+t__EMBEDDING_DIM, hidden_dim // 2,\n",
        "                            num_layers=1, bidirectional=True)\n",
        "\n",
        "        # Maps the output of the LSTM into tag space.\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
        "\n",
        "        # Matrix of transition parameters.  Entry i,j is the score of\n",
        "        # transitioning *to* i *from* j.\n",
        "        self.transitions = nn.Parameter(\n",
        "            torch.randn(self.tagset_size, self.tagset_size))\n",
        "\n",
        "        # These two statements enforce the constraint that we never transfer\n",
        "        # to the start tag and we never transfer from the stop tag\n",
        "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
        "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
        "\n",
        "        self.hidden = self.init_hidden()\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return (torch.randn(2, 1, self.hidden_dim // 2).to(device),\n",
        "                torch.randn(2, 1, self.hidden_dim // 2).to(device))\n",
        "\n",
        "    def _forward_alg(self, feats):\n",
        "        # Do the forward algorithm to compute the partition function\n",
        "        init_alphas = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "        # START_TAG has all of the score.\n",
        "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
        "\n",
        "        # Wrap in a variable so that we will get automatic backprop\n",
        "        forward_var = init_alphas\n",
        "\n",
        "        # Iterate through the sentence\n",
        "        for feat in feats:\n",
        "            alphas_t = []  # The forward tensors at this timestep\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # broadcast the emission score: it is the same regardless of\n",
        "                # the previous tag\n",
        "                emit_score = feat[next_tag].view(\n",
        "                    1, -1).expand(1, self.tagset_size)\n",
        "                # the ith entry of trans_score is the score of transitioning to\n",
        "                # next_tag from i\n",
        "                trans_score = self.transitions[next_tag].view(1, -1)\n",
        "                # The ith entry of next_tag_var is the value for the\n",
        "                # edge (i -> next_tag) before we do log-sum-exp\n",
        "                next_tag_var = forward_var + trans_score + emit_score\n",
        "                # The forward variable for this tag is log-sum-exp of all the\n",
        "                # scores.\n",
        "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
        "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        alpha = log_sum_exp(terminal_var)\n",
        "        return alpha\n",
        "\n",
        "    def _get_lstm_features(self, sentence, syntaglist):\n",
        "        self.hidden = self.init_hidden()\n",
        "        wordembeds = self.word_embeds(sentence)\n",
        "        tagembeds = self.tag_embeds(syntaglist)\n",
        "        embeds = torch.cat((wordembeds, tagembeds), 1).view(len(sentence), 1, -1)\n",
        "        \n",
        "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
        "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
        "        lstm_feats = self.hidden2tag(lstm_out)\n",
        "        return lstm_feats\n",
        "\n",
        "    def _score_sentence(self, feats, tags):\n",
        "        # Gives the score of a provided tag sequence\n",
        "        score = torch.zeros(1).to(device)\n",
        "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long).to(device), tags])\n",
        "        for i, feat in enumerate(feats):\n",
        "            score = score + \\\n",
        "                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
        "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
        "        return score\n",
        "\n",
        "    def _viterbi_decode(self, feats):\n",
        "        backpointers = []\n",
        "\n",
        "        # Initialize the viterbi variables in log space\n",
        "        init_vvars = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
        "\n",
        "        # forward_var at step i holds the viterbi variables for step i-1\n",
        "        forward_var = init_vvars\n",
        "        for feat in feats:\n",
        "            bptrs_t = []  # holds the backpointers for this step\n",
        "            viterbivars_t = []  # holds the viterbi variables for this step\n",
        "\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
        "                # previous step, plus the score of transitioning\n",
        "                # from tag i to next_tag.\n",
        "                # We don't include the emission scores here because the max\n",
        "                # does not depend on them (we add them in below)\n",
        "                next_tag_var = forward_var + self.transitions[next_tag]\n",
        "                best_tag_id = argmax(next_tag_var)\n",
        "                bptrs_t.append(best_tag_id)\n",
        "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
        "            # Now add in the emission scores, and assign forward_var to the set\n",
        "            # of viterbi variables we just computed\n",
        "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
        "            backpointers.append(bptrs_t)\n",
        "\n",
        "        # Transition to STOP_TAG\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        best_tag_id = argmax(terminal_var)\n",
        "        path_score = terminal_var[0][best_tag_id]\n",
        "\n",
        "        # Follow the back pointers to decode the best path.\n",
        "        best_path = [best_tag_id]\n",
        "        for bptrs_t in reversed(backpointers):\n",
        "            best_tag_id = bptrs_t[best_tag_id]\n",
        "            best_path.append(best_tag_id)\n",
        "        # Pop off the start tag (we dont want to return that to the caller)\n",
        "        start = best_path.pop()\n",
        "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
        "        best_path.reverse()\n",
        "        return path_score, best_path\n",
        "\n",
        "    def neg_log_likelihood(self, sentence, syntaglist, tags):\n",
        "        feats = self._get_lstm_features(sentence,syntaglist)\n",
        "        forward_score = self._forward_alg(feats)\n",
        "        gold_score = self._score_sentence(feats, tags)\n",
        "        return forward_score - gold_score\n",
        "\n",
        "    def forward(self, sentence, syntaglist):  # dont confuse this with _forward_alg above.\n",
        "        # Get the emission scores from the BiLSTM\n",
        "        lstm_feats = self._get_lstm_features(sentence,syntaglist)\n",
        "\n",
        "        # Find the best path, given the features.\n",
        "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
        "        return score, tag_seq\n",
        "\n",
        "# initialize model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, p_EMBEDDING_DIM, HIDDEN_DIM).to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
        "\n",
        "\n",
        "\"\"\"Each epoch will take about 2-3 minutes\"\"\"\n",
        "\n",
        "import datetime\n",
        "\n",
        "for epoch in range(20):  \n",
        "    time1 = datetime.datetime.now()\n",
        "    train_loss = 0\n",
        "\n",
        "    model.train()\n",
        "    for i, idxs in enumerate(train_input_index):\n",
        "        tags_index = train_output_index[i]\n",
        "        syn_idxs = t_train_input_index[i]\n",
        "\n",
        "        # Step 1. Remember that Pytorch accumulates gradients.\n",
        "        # We need to clear them out before each instance\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Step 2. Get our inputs ready for the network, that is,\n",
        "        # turn them into Tensors of word indices.\n",
        "        sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "        syntaglist_in = torch.tensor(syn_idxs, dtype=torch.long).to(device)\n",
        "\n",
        "        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "\n",
        "        # Step 3. Run our forward pass.\n",
        "        loss = model.neg_log_likelihood(sentence_in,syntaglist_in, targets)\n",
        "\n",
        "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "        # calling optimizer.step()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss+=loss.item()\n",
        "\n",
        "    model.eval()\n",
        "    _, _, train_acc = cal_acc(model,train_input_index,t_train_input_index,train_output_index)\n",
        "    _, _, val_acc = cal_acc(model,val_input_index,t_val_input_index,val_output_index)\n",
        "\n",
        "    val_loss = 0\n",
        "    for i, idxs in enumerate(val_input_index):\n",
        "        tags_index = val_output_index[i]\n",
        "        syn_idxs = t_val_input_index[i]\n",
        "        sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "        synlist_in = torch.tensor(syn_idxs, dtype=torch.long).to(device)\n",
        "        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "        loss = model.neg_log_likelihood(sentence_in, synlist_in, targets)\n",
        "        val_loss+=loss.item()\n",
        "    time2 = datetime.datetime.now()\n",
        "\n",
        "    print(\"Epoch:%d, Training loss: %.2f, train acc: %.4f, val loss: %.2f, val acc: %.4f, time: %.2fs\" %(epoch+1, train_loss,train_acc, val_loss, val_acc, (time2-time1).total_seconds()))\n",
        "\n",
        "y_true,y_pred,_ = cal_acc(model,val_input_index,t_val_input_index,val_output_index)\n",
        "y_true_decode = decode_output(y_true)\n",
        "y_pred_decode = decode_output(y_pred)\n",
        "\n",
        "print(classification_report(y_true_decode,y_pred_decode,digits=4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:1, Training loss: 13951.74, train acc: 0.7423, val loss: 4467.23, val acc: 0.6942, time: 182.01s\n",
            "Epoch:2, Training loss: 8346.71, train acc: 0.7810, val loss: 3581.83, val acc: 0.7222, time: 181.68s\n",
            "Epoch:3, Training loss: 6438.22, train acc: 0.8180, val loss: 3057.40, val acc: 0.7558, time: 182.00s\n",
            "Epoch:4, Training loss: 5413.80, train acc: 0.8361, val loss: 2745.62, val acc: 0.7719, time: 182.02s\n",
            "Epoch:5, Training loss: 4732.87, train acc: 0.8502, val loss: 2507.99, val acc: 0.7806, time: 181.28s\n",
            "Epoch:6, Training loss: 4212.91, train acc: 0.8612, val loss: 2386.85, val acc: 0.7874, time: 181.29s\n",
            "Epoch:7, Training loss: 3807.86, train acc: 0.8692, val loss: 2278.51, val acc: 0.7952, time: 181.07s\n",
            "Epoch:8, Training loss: 3460.80, train acc: 0.8780, val loss: 2176.11, val acc: 0.8043, time: 181.99s\n",
            "Epoch:9, Training loss: 3169.74, train acc: 0.8844, val loss: 2161.56, val acc: 0.8051, time: 181.18s\n",
            "Epoch:10, Training loss: 2918.18, train acc: 0.8878, val loss: 2130.93, val acc: 0.8096, time: 180.87s\n",
            "Epoch:11, Training loss: 2685.57, train acc: 0.8986, val loss: 2124.50, val acc: 0.8155, time: 181.39s\n",
            "Epoch:12, Training loss: 2485.18, train acc: 0.9077, val loss: 2105.59, val acc: 0.8119, time: 180.71s\n",
            "Epoch:13, Training loss: 2313.19, train acc: 0.9125, val loss: 2098.41, val acc: 0.8100, time: 181.10s\n",
            "Epoch:14, Training loss: 2206.17, train acc: 0.9112, val loss: 2159.23, val acc: 0.8129, time: 181.57s\n",
            "Epoch:15, Training loss: 2051.43, train acc: 0.9227, val loss: 2145.21, val acc: 0.8129, time: 180.78s\n",
            "Epoch:16, Training loss: 1854.32, train acc: 0.9291, val loss: 2137.26, val acc: 0.8134, time: 180.88s\n",
            "Epoch:17, Training loss: 1717.82, train acc: 0.9328, val loss: 2190.64, val acc: 0.8199, time: 181.27s\n",
            "Epoch:18, Training loss: 1592.66, train acc: 0.9380, val loss: 2237.93, val acc: 0.8170, time: 180.82s\n",
            "Epoch:19, Training loss: 1475.98, train acc: 0.9410, val loss: 2241.53, val acc: 0.8168, time: 180.99s\n",
            "Epoch:20, Training loss: 1352.58, train acc: 0.9465, val loss: 2305.08, val acc: 0.8174, time: 181.41s\n",
            "                     precision    recall  f1-score   support\n",
            "\n",
            "B-DocumentReference     0.1500    0.3333    0.2069         9\n",
            "         B-Location     0.6667    0.6458    0.6561       192\n",
            " B-MilitaryPlatform     0.0625    0.3333    0.1053         3\n",
            "            B-Money     0.2000    0.5000    0.2857         2\n",
            "      B-Nationality     0.2500    0.3333    0.2857         6\n",
            "     B-Organisation     0.6536    0.6778    0.6655       270\n",
            "           B-Person     0.8431    0.8037    0.8230       107\n",
            "         B-Quantity     0.7091    0.6500    0.6783        60\n",
            "         B-Temporal     0.7447    0.8537    0.7955        41\n",
            "           B-Weapon     0.1842    0.2800    0.2222        25\n",
            "I-DocumentReference     0.1928    0.7619    0.3077        21\n",
            "         I-Location     0.5585    0.6727    0.6103       220\n",
            " I-MilitaryPlatform     0.0625    0.5000    0.1111         2\n",
            "            I-Money     0.4000    1.0000    0.5714         4\n",
            "      I-Nationality     0.0000    0.0000    0.0000         0\n",
            "     I-Organisation     0.6010    0.7184    0.6545       348\n",
            "           I-Person     0.7572    0.8506    0.8012       154\n",
            "         I-Quantity     0.2692    0.5385    0.3590        13\n",
            "         I-Temporal     0.7258    0.9375    0.8182        48\n",
            "           I-Weapon     0.1111    0.3333    0.1667        15\n",
            "                  O     0.9380    0.8591    0.8968      3734\n",
            "\n",
            "           accuracy                         0.8146      5274\n",
            "          macro avg     0.4324    0.5992    0.4772      5274\n",
            "       weighted avg     0.8481    0.8146    0.8280      5274\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQTus35xgHTt"
      },
      "source": [
        "####4.3 Bi-LSTM CRF with Glove-twitter-25 and Dependency information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGJ1K2agp2VT",
        "outputId": "0263bd17-97ab-4752-c86f-8b1797015c40"
      },
      "source": [
        "p_EMBEDDING_DIM = EMBEDDING_DIM\n",
        "p_embedding_matrix = embedding_matrix\n",
        "t__EMBEDDING_DIM = EMBEDDING_DIM_DEP\n",
        "t_embedding_matrix = embedding_matrix_DEP\n",
        "t_vocab = embedding_matrix_DEP.shape[0]\n",
        "t_train_input_index = train_DEP_index\n",
        "t_val_input_index = val_DEP_index\n",
        "\n",
        "def cal_acc(model, input_index, syn_input_index, output_index):\n",
        "    ground_truth = []\n",
        "    predicted = []\n",
        "    for i,idxs in enumerate(input_index):\n",
        "        syn_idxs = syn_input_index[i]\n",
        "        ground_truth += output_index[i]\n",
        "        sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "        synlist_in = torch.tensor(syn_idxs, dtype=torch.long).to(device)\n",
        "        score, pred = model(sentence_in,synlist_in)\n",
        "        predicted += pred\n",
        "    accuracy = sum(np.array(ground_truth) == np.array(predicted))/len(ground_truth)\n",
        "    return predicted, ground_truth, accuracy\n",
        "\n",
        "class BiLSTM_CRF(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):\n",
        "        super(BiLSTM_CRF, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "        self.tagset_size = len(tag_to_ix)\n",
        "\n",
        "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
        "        \"\"\"Here we use the embedding matrix as the initial weights of nn.Embedding\"\"\"\n",
        "        self.word_embeds.weight.data.copy_(torch.from_numpy(p_embedding_matrix))\n",
        "\n",
        "        self.tag_embeds = nn.Embedding(t_vocab, t__EMBEDDING_DIM)\n",
        "        self.tag_embeds.weight.data.copy_(torch.from_numpy(t_embedding_matrix))\n",
        "        \n",
        "        self.lstm = nn.LSTM(embedding_dim+t__EMBEDDING_DIM, hidden_dim // 2,\n",
        "                            num_layers=1, bidirectional=True)\n",
        "\n",
        "        # Maps the output of the LSTM into tag space.\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
        "\n",
        "        # Matrix of transition parameters.  Entry i,j is the score of\n",
        "        # transitioning *to* i *from* j.\n",
        "        self.transitions = nn.Parameter(\n",
        "            torch.randn(self.tagset_size, self.tagset_size))\n",
        "\n",
        "        # These two statements enforce the constraint that we never transfer\n",
        "        # to the start tag and we never transfer from the stop tag\n",
        "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
        "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
        "\n",
        "        self.hidden = self.init_hidden()\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return (torch.randn(2, 1, self.hidden_dim // 2).to(device),\n",
        "                torch.randn(2, 1, self.hidden_dim // 2).to(device))\n",
        "\n",
        "    def _forward_alg(self, feats):\n",
        "        # Do the forward algorithm to compute the partition function\n",
        "        init_alphas = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "        # START_TAG has all of the score.\n",
        "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
        "\n",
        "        # Wrap in a variable so that we will get automatic backprop\n",
        "        forward_var = init_alphas\n",
        "\n",
        "        # Iterate through the sentence\n",
        "        for feat in feats:\n",
        "            alphas_t = []  # The forward tensors at this timestep\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # broadcast the emission score: it is the same regardless of\n",
        "                # the previous tag\n",
        "                emit_score = feat[next_tag].view(\n",
        "                    1, -1).expand(1, self.tagset_size)\n",
        "                # the ith entry of trans_score is the score of transitioning to\n",
        "                # next_tag from i\n",
        "                trans_score = self.transitions[next_tag].view(1, -1)\n",
        "                # The ith entry of next_tag_var is the value for the\n",
        "                # edge (i -> next_tag) before we do log-sum-exp\n",
        "                next_tag_var = forward_var + trans_score + emit_score\n",
        "                # The forward variable for this tag is log-sum-exp of all the\n",
        "                # scores.\n",
        "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
        "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        alpha = log_sum_exp(terminal_var)\n",
        "        return alpha\n",
        "\n",
        "    def _get_lstm_features(self, sentence, syntaglist):\n",
        "        self.hidden = self.init_hidden()\n",
        "        wordembeds = self.word_embeds(sentence)\n",
        "        tagembeds = self.tag_embeds(syntaglist)\n",
        "        embeds = torch.cat((wordembeds, tagembeds), 1).view(len(sentence), 1, -1)\n",
        "        \n",
        "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
        "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
        "        lstm_feats = self.hidden2tag(lstm_out)\n",
        "        return lstm_feats\n",
        "\n",
        "    def _score_sentence(self, feats, tags):\n",
        "        # Gives the score of a provided tag sequence\n",
        "        score = torch.zeros(1).to(device)\n",
        "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long).to(device), tags])\n",
        "        for i, feat in enumerate(feats):\n",
        "            score = score + \\\n",
        "                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
        "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
        "        return score\n",
        "\n",
        "    def _viterbi_decode(self, feats):\n",
        "        backpointers = []\n",
        "\n",
        "        # Initialize the viterbi variables in log space\n",
        "        init_vvars = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
        "\n",
        "        # forward_var at step i holds the viterbi variables for step i-1\n",
        "        forward_var = init_vvars\n",
        "        for feat in feats:\n",
        "            bptrs_t = []  # holds the backpointers for this step\n",
        "            viterbivars_t = []  # holds the viterbi variables for this step\n",
        "\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
        "                # previous step, plus the score of transitioning\n",
        "                # from tag i to next_tag.\n",
        "                # We don't include the emission scores here because the max\n",
        "                # does not depend on them (we add them in below)\n",
        "                next_tag_var = forward_var + self.transitions[next_tag]\n",
        "                best_tag_id = argmax(next_tag_var)\n",
        "                bptrs_t.append(best_tag_id)\n",
        "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
        "            # Now add in the emission scores, and assign forward_var to the set\n",
        "            # of viterbi variables we just computed\n",
        "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
        "            backpointers.append(bptrs_t)\n",
        "\n",
        "        # Transition to STOP_TAG\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        best_tag_id = argmax(terminal_var)\n",
        "        path_score = terminal_var[0][best_tag_id]\n",
        "\n",
        "        # Follow the back pointers to decode the best path.\n",
        "        best_path = [best_tag_id]\n",
        "        for bptrs_t in reversed(backpointers):\n",
        "            best_tag_id = bptrs_t[best_tag_id]\n",
        "            best_path.append(best_tag_id)\n",
        "        # Pop off the start tag (we dont want to return that to the caller)\n",
        "        start = best_path.pop()\n",
        "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
        "        best_path.reverse()\n",
        "        return path_score, best_path\n",
        "\n",
        "    def neg_log_likelihood(self, sentence, syntaglist, tags):\n",
        "        feats = self._get_lstm_features(sentence,syntaglist)\n",
        "        forward_score = self._forward_alg(feats)\n",
        "        gold_score = self._score_sentence(feats, tags)\n",
        "        return forward_score - gold_score\n",
        "\n",
        "    def forward(self, sentence, syntaglist):  # dont confuse this with _forward_alg above.\n",
        "        # Get the emission scores from the BiLSTM\n",
        "        lstm_feats = self._get_lstm_features(sentence,syntaglist)\n",
        "\n",
        "        # Find the best path, given the features.\n",
        "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
        "        return score, tag_seq\n",
        "\n",
        "# initialize model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, p_EMBEDDING_DIM, HIDDEN_DIM).to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
        "\n",
        "\n",
        "\"\"\"Each epoch will take about 2-3 minutes\"\"\"\n",
        "\n",
        "import datetime\n",
        "\n",
        "for epoch in range(max_epoch):  \n",
        "    time1 = datetime.datetime.now()\n",
        "    train_loss = 0\n",
        "\n",
        "    model.train()\n",
        "    for i, idxs in enumerate(train_input_index):\n",
        "        tags_index = train_output_index[i]\n",
        "        syn_idxs = t_train_input_index[i]\n",
        "\n",
        "        # Step 1. Remember that Pytorch accumulates gradients.\n",
        "        # We need to clear them out before each instance\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Step 2. Get our inputs ready for the network, that is,\n",
        "        # turn them into Tensors of word indices.\n",
        "        sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "        syntaglist_in = torch.tensor(syn_idxs, dtype=torch.long).to(device)\n",
        "\n",
        "        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "\n",
        "        # Step 3. Run our forward pass.\n",
        "        loss = model.neg_log_likelihood(sentence_in,syntaglist_in, targets)\n",
        "\n",
        "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "        # calling optimizer.step()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss+=loss.item()\n",
        "\n",
        "    model.eval()\n",
        "    _, _, train_acc = cal_acc(model,train_input_index,t_train_input_index,train_output_index)\n",
        "    _, _, val_acc = cal_acc(model,val_input_index,t_val_input_index,val_output_index)\n",
        "\n",
        "    val_loss = 0\n",
        "    for i, idxs in enumerate(val_input_index):\n",
        "        tags_index = val_output_index[i]\n",
        "        syn_idxs = t_val_input_index[i]\n",
        "        sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "        synlist_in = torch.tensor(syn_idxs, dtype=torch.long).to(device)\n",
        "        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "        loss = model.neg_log_likelihood(sentence_in, synlist_in, targets)\n",
        "        val_loss+=loss.item()\n",
        "    time2 = datetime.datetime.now()\n",
        "\n",
        "    print(\"Epoch:%d, Training loss: %.2f, train acc: %.4f, val loss: %.2f, val acc: %.4f, time: %.2fs\" %(epoch+1, train_loss,train_acc, val_loss, val_acc, (time2-time1).total_seconds()))\n",
        "\n",
        "y_true,y_pred,_ = cal_acc(model,val_input_index,t_val_input_index,val_output_index)\n",
        "y_true_decode = decode_output(y_true)\n",
        "y_pred_decode = decode_output(y_pred)\n",
        "\n",
        "print(classification_report(y_true_decode,y_pred_decode,digits=4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:1, Training loss: 14555.92, train acc: 0.7416, val loss: 4528.22, val acc: 0.6881, time: 175.18s\n",
            "Epoch:2, Training loss: 8348.77, train acc: 0.7980, val loss: 3488.86, val acc: 0.7347, time: 175.62s\n",
            "Epoch:3, Training loss: 6404.40, train acc: 0.8257, val loss: 2920.13, val acc: 0.7672, time: 175.18s\n",
            "Epoch:4, Training loss: 5449.75, train acc: 0.8443, val loss: 2553.14, val acc: 0.7799, time: 175.35s\n",
            "Epoch:5, Training loss: 4776.67, train acc: 0.8578, val loss: 2371.16, val acc: 0.7882, time: 174.12s\n",
            "Epoch:6, Training loss: 4297.62, train acc: 0.8715, val loss: 2216.87, val acc: 0.8011, time: 174.86s\n",
            "Epoch:7, Training loss: 3887.90, train acc: 0.8763, val loss: 2144.47, val acc: 0.8049, time: 175.19s\n",
            "Epoch:8, Training loss: 3514.81, train acc: 0.8872, val loss: 2075.80, val acc: 0.8098, time: 175.36s\n",
            "Epoch:9, Training loss: 3210.18, train acc: 0.8895, val loss: 2097.20, val acc: 0.8089, time: 175.70s\n",
            "Epoch:10, Training loss: 2930.49, train acc: 0.8939, val loss: 2066.72, val acc: 0.8087, time: 175.66s\n",
            "Epoch:11, Training loss: 2692.67, train acc: 0.8994, val loss: 2061.54, val acc: 0.8134, time: 175.78s\n",
            "Epoch:12, Training loss: 2484.29, train acc: 0.9053, val loss: 2097.45, val acc: 0.8110, time: 175.15s\n",
            "Epoch:13, Training loss: 2285.46, train acc: 0.9120, val loss: 2125.44, val acc: 0.8110, time: 174.76s\n",
            "Epoch:14, Training loss: 2065.45, train acc: 0.9134, val loss: 2144.70, val acc: 0.8024, time: 175.61s\n",
            "Epoch:15, Training loss: 1944.94, train acc: 0.9256, val loss: 2117.88, val acc: 0.8074, time: 175.62s\n",
            "Epoch:16, Training loss: 1816.24, train acc: 0.9298, val loss: 2171.46, val acc: 0.8130, time: 175.82s\n",
            "Epoch:17, Training loss: 1604.35, train acc: 0.9342, val loss: 2216.91, val acc: 0.8110, time: 175.61s\n",
            "Epoch:18, Training loss: 1472.28, train acc: 0.9398, val loss: 2212.76, val acc: 0.8085, time: 174.25s\n",
            "Epoch:19, Training loss: 1348.18, train acc: 0.9443, val loss: 2251.50, val acc: 0.8034, time: 174.10s\n",
            "Epoch:20, Training loss: 1254.50, train acc: 0.9485, val loss: 2337.98, val acc: 0.8039, time: 174.84s\n",
            "                     precision    recall  f1-score   support\n",
            "\n",
            "B-DocumentReference     0.3500    0.5000    0.4118        14\n",
            "         B-Location     0.6398    0.6723    0.6556       177\n",
            " B-MilitaryPlatform     0.1250    0.3333    0.1818         6\n",
            "            B-Money     0.2000    0.3333    0.2500         3\n",
            "      B-Nationality     0.2500    0.2857    0.2667         7\n",
            "     B-Organisation     0.6393    0.6885    0.6630       260\n",
            "           B-Person     0.7843    0.8081    0.7960        99\n",
            "         B-Quantity     0.7091    0.7800    0.7429        50\n",
            "         B-Temporal     0.7021    0.7174    0.7097        46\n",
            "           B-Weapon     0.2368    0.3462    0.2812        26\n",
            "I-DocumentReference     0.2530    0.6562    0.3652        32\n",
            "         I-Location     0.5132    0.6355    0.5678       214\n",
            " I-MilitaryPlatform     0.1250    1.0000    0.2222         2\n",
            "            I-Money     0.4000    0.8000    0.5333         5\n",
            "      I-Nationality     0.0000    0.0000    0.0000         0\n",
            "     I-Organisation     0.6298    0.6566    0.6429       399\n",
            "           I-Person     0.6647    0.8712    0.7541       132\n",
            "         I-Quantity     0.2308    0.5455    0.3243        11\n",
            "         I-Temporal     0.8226    0.7846    0.8031        65\n",
            "           I-Weapon     0.1778    0.3810    0.2424        21\n",
            "                  O     0.9281    0.8567    0.8909      3705\n",
            "\n",
            "           accuracy                         0.8058      5274\n",
            "          macro avg     0.4467    0.6025    0.4907      5274\n",
            "       weighted avg     0.8336    0.8058    0.8168      5274\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "388yEDXHNlZI"
      },
      "source": [
        "###4.4 Bi-LSTM CRF with Glove-twitter-25 and POS & Dependency information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_hYDNY5NrhQ",
        "outputId": "452ef846-e189-4229-a7a2-ded9dd19a8cc"
      },
      "source": [
        "p_EMBEDDING_DIM = EMBEDDING_DIM\n",
        "p_embedding_matrix = embedding_matrix\n",
        "\n",
        "def cal_acc(model, input_index, val_POS_index ,val_DEP_index, output_index):\n",
        "    ground_truth = []\n",
        "    predicted = []\n",
        "    for i,idxs in enumerate(input_index):\n",
        "        ground_truth += output_index[i]\n",
        "        pos_idxs = val_POS_index[i]\n",
        "        dep_idxs = val_DEP_index[i]\n",
        "\n",
        "        postaglist_in = torch.tensor(pos_idxs, dtype=torch.long).to(device)\n",
        "        deptaglist_in = torch.tensor(dep_idxs, dtype=torch.long).to(device)\n",
        "        sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "\n",
        "        score, pred = model(sentence_in,postaglist_in,deptaglist_in)\n",
        "        predicted += pred\n",
        "    accuracy = sum(np.array(ground_truth) == np.array(predicted))/len(ground_truth)\n",
        "    return predicted, ground_truth, accuracy\n",
        "\n",
        "class BiLSTM_CRF(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):\n",
        "        super(BiLSTM_CRF, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "        self.tagset_size = len(tag_to_ix)\n",
        "\n",
        "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
        "        \"\"\"Here we use the embedding matrix as the initial weights of nn.Embedding\"\"\"\n",
        "        self.word_embeds.weight.data.copy_(torch.from_numpy(p_embedding_matrix))\n",
        "\n",
        "        self.postag_embeds = nn.Embedding(embedding_matrix_POS.shape[0], EMBEDDING_DIM_POS)\n",
        "        self.postag_embeds.weight.data.copy_(torch.from_numpy(embedding_matrix_POS))\n",
        "\n",
        "        self.deptag_embeds = nn.Embedding(embedding_matrix_DEP.shape[0], EMBEDDING_DIM_DEP)\n",
        "        self.deptag_embeds.weight.data.copy_(torch.from_numpy(embedding_matrix_DEP))\n",
        "        \n",
        "        self.lstm = nn.LSTM(embedding_dim+EMBEDDING_DIM_POS+EMBEDDING_DIM_DEP, hidden_dim // 2,\n",
        "                            num_layers=1, bidirectional=True)\n",
        "\n",
        "        # Maps the output of the LSTM into tag space.\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
        "\n",
        "        # Matrix of transition parameters.  Entry i,j is the score of\n",
        "        # transitioning *to* i *from* j.\n",
        "        self.transitions = nn.Parameter(\n",
        "            torch.randn(self.tagset_size, self.tagset_size))\n",
        "\n",
        "        # These two statements enforce the constraint that we never transfer\n",
        "        # to the start tag and we never transfer from the stop tag\n",
        "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
        "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
        "\n",
        "        self.hidden = self.init_hidden()\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return (torch.randn(2, 1, self.hidden_dim // 2).to(device),\n",
        "                torch.randn(2, 1, self.hidden_dim // 2).to(device))\n",
        "\n",
        "    def _forward_alg(self, feats):\n",
        "        # Do the forward algorithm to compute the partition function\n",
        "        init_alphas = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "        # START_TAG has all of the score.\n",
        "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
        "\n",
        "        # Wrap in a variable so that we will get automatic backprop\n",
        "        forward_var = init_alphas\n",
        "\n",
        "        # Iterate through the sentence\n",
        "        for feat in feats:\n",
        "            alphas_t = []  # The forward tensors at this timestep\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # broadcast the emission score: it is the same regardless of\n",
        "                # the previous tag\n",
        "                emit_score = feat[next_tag].view(\n",
        "                    1, -1).expand(1, self.tagset_size)\n",
        "                # the ith entry of trans_score is the score of transitioning to\n",
        "                # next_tag from i\n",
        "                trans_score = self.transitions[next_tag].view(1, -1)\n",
        "                # The ith entry of next_tag_var is the value for the\n",
        "                # edge (i -> next_tag) before we do log-sum-exp\n",
        "                next_tag_var = forward_var + trans_score + emit_score\n",
        "                # The forward variable for this tag is log-sum-exp of all the\n",
        "                # scores.\n",
        "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
        "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        alpha = log_sum_exp(terminal_var)\n",
        "        return alpha\n",
        "\n",
        "    def _get_lstm_features(self, sentence, postaglist, deptaglist):\n",
        "        self.hidden = self.init_hidden()\n",
        "        wordembeds = self.word_embeds(sentence)\n",
        "        postagembeds = self.postag_embeds(postaglist)\n",
        "        deptagembeds = self.deptag_embeds(deptaglist)\n",
        "        embeds = torch.cat((wordembeds, postagembeds, deptagembeds), 1).view(len(sentence), 1, -1)\n",
        "        \n",
        "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
        "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
        "        lstm_feats = self.hidden2tag(lstm_out)\n",
        "        return lstm_feats\n",
        "\n",
        "    def _score_sentence(self, feats, tags):\n",
        "        # Gives the score of a provided tag sequence\n",
        "        score = torch.zeros(1).to(device)\n",
        "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long).to(device), tags])\n",
        "        for i, feat in enumerate(feats):\n",
        "            score = score + \\\n",
        "                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
        "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
        "        return score\n",
        "\n",
        "    def _viterbi_decode(self, feats):\n",
        "        backpointers = []\n",
        "\n",
        "        # Initialize the viterbi variables in log space\n",
        "        init_vvars = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
        "\n",
        "        # forward_var at step i holds the viterbi variables for step i-1\n",
        "        forward_var = init_vvars\n",
        "        for feat in feats:\n",
        "            bptrs_t = []  # holds the backpointers for this step\n",
        "            viterbivars_t = []  # holds the viterbi variables for this step\n",
        "\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
        "                # previous step, plus the score of transitioning\n",
        "                # from tag i to next_tag.\n",
        "                # We don't include the emission scores here because the max\n",
        "                # does not depend on them (we add them in below)\n",
        "                next_tag_var = forward_var + self.transitions[next_tag]\n",
        "                best_tag_id = argmax(next_tag_var)\n",
        "                bptrs_t.append(best_tag_id)\n",
        "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
        "            # Now add in the emission scores, and assign forward_var to the set\n",
        "            # of viterbi variables we just computed\n",
        "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
        "            backpointers.append(bptrs_t)\n",
        "\n",
        "        # Transition to STOP_TAG\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        best_tag_id = argmax(terminal_var)\n",
        "        path_score = terminal_var[0][best_tag_id]\n",
        "\n",
        "        # Follow the back pointers to decode the best path.\n",
        "        best_path = [best_tag_id]\n",
        "        for bptrs_t in reversed(backpointers):\n",
        "            best_tag_id = bptrs_t[best_tag_id]\n",
        "            best_path.append(best_tag_id)\n",
        "        # Pop off the start tag (we dont want to return that to the caller)\n",
        "        start = best_path.pop()\n",
        "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
        "        best_path.reverse()\n",
        "        return path_score, best_path\n",
        "\n",
        "    def neg_log_likelihood(self, sentence, postaglist,deptaglist, tags):\n",
        "        feats = self._get_lstm_features(sentence,postaglist,deptaglist)\n",
        "        forward_score = self._forward_alg(feats)\n",
        "        gold_score = self._score_sentence(feats, tags)\n",
        "        return forward_score - gold_score\n",
        "\n",
        "    def forward(self, sentence, postaglist,deptaglist):  # dont confuse this with _forward_alg above.\n",
        "        # Get the emission scores from the BiLSTM\n",
        "        lstm_feats = self._get_lstm_features(sentence,postaglist,deptaglist)\n",
        "\n",
        "        # Find the best path, given the features.\n",
        "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
        "        return score, tag_seq\n",
        "\n",
        "# initialize model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, p_EMBEDDING_DIM, HIDDEN_DIM).to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
        "\n",
        "\n",
        "\"\"\"Each epoch will take about 2-3 minutes\"\"\"\n",
        "\n",
        "import datetime\n",
        "\n",
        "for epoch in range(max_epoch):  \n",
        "    time1 = datetime.datetime.now()\n",
        "    train_loss = 0\n",
        "\n",
        "    model.train()\n",
        "    for i, idxs in enumerate(train_input_index):\n",
        "        tags_index = train_output_index[i]\n",
        "        pos_idxs = train_POS_index[i]\n",
        "        dep_idxs = train_DEP_index[i]\n",
        "\n",
        "        # Step 1. Remember that Pytorch accumulates gradients.\n",
        "        # We need to clear them out before each instance\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Step 2. Get our inputs ready for the network, that is,\n",
        "        # turn them into Tensors of word indices.\n",
        "        sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "        postaglist_in = torch.tensor(pos_idxs, dtype=torch.long).to(device)\n",
        "        deptaglist_in = torch.tensor(dep_idxs, dtype=torch.long).to(device)\n",
        "\n",
        "        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "\n",
        "        # Step 3. Run our forward pass.\n",
        "        loss = model.neg_log_likelihood(sentence_in,postaglist_in,deptaglist_in, targets)\n",
        "\n",
        "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "        # calling optimizer.step()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss+=loss.item()\n",
        "\n",
        "    model.eval()\n",
        "    _, _, train_acc = cal_acc(model,train_input_index,train_POS_index,train_DEP_index,train_output_index)\n",
        "    _, _, val_acc = cal_acc(model,val_input_index,val_POS_index,val_DEP_index,val_output_index)\n",
        "\n",
        "    val_loss = 0\n",
        "    for i, idxs in enumerate(val_input_index):\n",
        "        tags_index = val_output_index[i]\n",
        "        pos_idxs = val_POS_index[i]\n",
        "        dep_idxs = val_DEP_index[i]\n",
        "        sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "        postaglist_in = torch.tensor(pos_idxs, dtype=torch.long).to(device)\n",
        "        deptaglist_in = torch.tensor(dep_idxs, dtype=torch.long).to(device)\n",
        "        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "        loss = model.neg_log_likelihood(sentence_in, postaglist_in,deptaglist_in, targets)\n",
        "        val_loss+=loss.item()\n",
        "    time2 = datetime.datetime.now()\n",
        "\n",
        "    print(\"Epoch:%d, Training loss: %.2f, train acc: %.4f, val loss: %.2f, val acc: %.4f, time: %.2fs\" %(epoch+1, train_loss,train_acc, val_loss, val_acc, (time2-time1).total_seconds()))\n",
        "\n",
        "y_true,y_pred,_ = cal_acc(model,val_input_index,val_POS_index,val_DEP_index,val_output_index)\n",
        "y_true_decode = decode_output(y_true)\n",
        "y_pred_decode = decode_output(y_pred)\n",
        "\n",
        "print(classification_report(y_true_decode,y_pred_decode,digits=4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:1, Training loss: 14156.66, train acc: 0.7435, val loss: 4384.63, val acc: 0.6892, time: 176.46s\n",
            "Epoch:2, Training loss: 7907.80, train acc: 0.7907, val loss: 3482.87, val acc: 0.7328, time: 172.61s\n",
            "Epoch:3, Training loss: 6180.36, train acc: 0.8097, val loss: 3090.25, val acc: 0.7482, time: 171.93s\n",
            "Epoch:4, Training loss: 5265.32, train acc: 0.8390, val loss: 2701.72, val acc: 0.7778, time: 176.32s\n",
            "Epoch:5, Training loss: 4621.83, train acc: 0.8487, val loss: 2521.27, val acc: 0.7878, time: 178.57s\n",
            "Epoch:6, Training loss: 4147.45, train acc: 0.8658, val loss: 2286.06, val acc: 0.7969, time: 178.10s\n",
            "Epoch:7, Training loss: 3713.32, train acc: 0.8773, val loss: 2148.69, val acc: 0.8041, time: 178.05s\n",
            "Epoch:8, Training loss: 3371.51, train acc: 0.8863, val loss: 2081.74, val acc: 0.8129, time: 178.24s\n",
            "Epoch:9, Training loss: 3107.78, train acc: 0.8948, val loss: 2031.29, val acc: 0.8130, time: 178.18s\n",
            "Epoch:10, Training loss: 2840.75, train acc: 0.9032, val loss: 2001.92, val acc: 0.8191, time: 177.99s\n",
            "Epoch:11, Training loss: 2605.93, train acc: 0.9097, val loss: 2001.73, val acc: 0.8172, time: 178.12s\n",
            "Epoch:12, Training loss: 2381.67, train acc: 0.9150, val loss: 2021.94, val acc: 0.8187, time: 177.84s\n",
            "Epoch:13, Training loss: 2186.03, train acc: 0.9219, val loss: 2029.20, val acc: 0.8178, time: 177.88s\n",
            "Epoch:14, Training loss: 1999.85, train acc: 0.9299, val loss: 2045.73, val acc: 0.8166, time: 178.10s\n",
            "Epoch:15, Training loss: 1818.52, train acc: 0.9317, val loss: 2114.63, val acc: 0.8174, time: 177.90s\n",
            "Epoch:16, Training loss: 1683.36, train acc: 0.9385, val loss: 2117.28, val acc: 0.8214, time: 177.97s\n",
            "Epoch:17, Training loss: 1499.19, train acc: 0.9428, val loss: 2156.53, val acc: 0.8157, time: 177.95s\n",
            "Epoch:18, Training loss: 1407.56, train acc: 0.9451, val loss: 2244.44, val acc: 0.8165, time: 178.01s\n",
            "Epoch:19, Training loss: 1311.97, train acc: 0.9478, val loss: 2287.10, val acc: 0.8168, time: 178.02s\n",
            "Epoch:20, Training loss: 1207.06, train acc: 0.9573, val loss: 2307.60, val acc: 0.8144, time: 177.79s\n",
            "                     precision    recall  f1-score   support\n",
            "\n",
            "B-DocumentReference     0.1000    0.2000    0.1333        10\n",
            "         B-Location     0.6129    0.6706    0.6404       170\n",
            " B-MilitaryPlatform     0.1250    0.5000    0.2000         4\n",
            "            B-Money     0.2000    0.3333    0.2500         3\n",
            "      B-Nationality     0.3750    0.3750    0.3750         8\n",
            "     B-Organisation     0.6536    0.6803    0.6667       269\n",
            "           B-Person     0.8333    0.8500    0.8416       100\n",
            "         B-Quantity     0.7091    0.7647    0.7358        51\n",
            "         B-Temporal     0.7234    0.8095    0.7640        42\n",
            "           B-Weapon     0.1053    0.2667    0.1509        15\n",
            "I-DocumentReference     0.2169    0.4865    0.3000        37\n",
            "         I-Location     0.5623    0.6507    0.6032       229\n",
            " I-MilitaryPlatform     0.1250    1.0000    0.2222         2\n",
            "            I-Money     0.4000    1.0000    0.5714         4\n",
            "      I-Nationality     0.0000    0.0000    0.0000         0\n",
            "     I-Organisation     0.6178    0.7219    0.6658       356\n",
            "           I-Person     0.7803    0.9060    0.8385       149\n",
            "         I-Quantity     0.3077    0.6667    0.4211        12\n",
            "         I-Temporal     0.7097    0.8980    0.7928        49\n",
            "           I-Weapon     0.0222    0.1429    0.0385         7\n",
            "                  O     0.9389    0.8547    0.8948      3757\n",
            "\n",
            "           accuracy                         0.8146      5274\n",
            "          macro avg     0.4342    0.6084    0.4812      5274\n",
            "       weighted avg     0.8490    0.8146    0.8286      5274\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqGgFmGtXvRp"
      },
      "source": [
        "###4.5 Bi-LSTM CRF with Glove-twitter-50 and POS & Dependency information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UF8aDskhXrub",
        "outputId": "6cb1bdc6-1b32-412e-dcd1-9ef6613d9db4"
      },
      "source": [
        "p_EMBEDDING_DIM = EMBEDDING_DIM_50\n",
        "p_embedding_matrix = embedding_matrix_50\n",
        "\n",
        "def cal_acc(model, input_index, val_POS_index ,val_DEP_index, output_index):\n",
        "    ground_truth = []\n",
        "    predicted = []\n",
        "    for i,idxs in enumerate(input_index):\n",
        "        ground_truth += output_index[i]\n",
        "        pos_idxs = val_POS_index[i]\n",
        "        dep_idxs = val_DEP_index[i]\n",
        "\n",
        "        postaglist_in = torch.tensor(pos_idxs, dtype=torch.long).to(device)\n",
        "        deptaglist_in = torch.tensor(dep_idxs, dtype=torch.long).to(device)\n",
        "        sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "\n",
        "        score, pred = model(sentence_in,postaglist_in,deptaglist_in)\n",
        "        predicted += pred\n",
        "    accuracy = sum(np.array(ground_truth) == np.array(predicted))/len(ground_truth)\n",
        "    return predicted, ground_truth, accuracy\n",
        "\n",
        "class BiLSTM_CRF(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):\n",
        "        super(BiLSTM_CRF, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "        self.tagset_size = len(tag_to_ix)\n",
        "\n",
        "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
        "        \"\"\"Here we use the embedding matrix as the initial weights of nn.Embedding\"\"\"\n",
        "        self.word_embeds.weight.data.copy_(torch.from_numpy(p_embedding_matrix))\n",
        "\n",
        "        self.postag_embeds = nn.Embedding(embedding_matrix_POS.shape[0], EMBEDDING_DIM_POS)\n",
        "        self.postag_embeds.weight.data.copy_(torch.from_numpy(embedding_matrix_POS))\n",
        "\n",
        "        self.deptag_embeds = nn.Embedding(embedding_matrix_DEP.shape[0], EMBEDDING_DIM_DEP)\n",
        "        self.deptag_embeds.weight.data.copy_(torch.from_numpy(embedding_matrix_DEP))\n",
        "        \n",
        "        self.lstm = nn.LSTM(embedding_dim+EMBEDDING_DIM_POS+EMBEDDING_DIM_DEP, hidden_dim // 2,\n",
        "                            num_layers=1, bidirectional=True)\n",
        "\n",
        "        # Maps the output of the LSTM into tag space.\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
        "\n",
        "        # Matrix of transition parameters.  Entry i,j is the score of\n",
        "        # transitioning *to* i *from* j.\n",
        "        self.transitions = nn.Parameter(\n",
        "            torch.randn(self.tagset_size, self.tagset_size))\n",
        "\n",
        "        # These two statements enforce the constraint that we never transfer\n",
        "        # to the start tag and we never transfer from the stop tag\n",
        "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
        "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
        "\n",
        "        self.hidden = self.init_hidden()\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return (torch.randn(2, 1, self.hidden_dim // 2).to(device),\n",
        "                torch.randn(2, 1, self.hidden_dim // 2).to(device))\n",
        "\n",
        "    def _forward_alg(self, feats):\n",
        "        # Do the forward algorithm to compute the partition function\n",
        "        init_alphas = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "        # START_TAG has all of the score.\n",
        "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
        "\n",
        "        # Wrap in a variable so that we will get automatic backprop\n",
        "        forward_var = init_alphas\n",
        "\n",
        "        # Iterate through the sentence\n",
        "        for feat in feats:\n",
        "            alphas_t = []  # The forward tensors at this timestep\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # broadcast the emission score: it is the same regardless of\n",
        "                # the previous tag\n",
        "                emit_score = feat[next_tag].view(\n",
        "                    1, -1).expand(1, self.tagset_size)\n",
        "                # the ith entry of trans_score is the score of transitioning to\n",
        "                # next_tag from i\n",
        "                trans_score = self.transitions[next_tag].view(1, -1)\n",
        "                # The ith entry of next_tag_var is the value for the\n",
        "                # edge (i -> next_tag) before we do log-sum-exp\n",
        "                next_tag_var = forward_var + trans_score + emit_score\n",
        "                # The forward variable for this tag is log-sum-exp of all the\n",
        "                # scores.\n",
        "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
        "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        alpha = log_sum_exp(terminal_var)\n",
        "        return alpha\n",
        "\n",
        "    def _get_lstm_features(self, sentence, postaglist, deptaglist):\n",
        "        self.hidden = self.init_hidden()\n",
        "        wordembeds = self.word_embeds(sentence)\n",
        "        postagembeds = self.postag_embeds(postaglist)\n",
        "        deptagembeds = self.deptag_embeds(deptaglist)\n",
        "        embeds = torch.cat((wordembeds, postagembeds, deptagembeds), 1).view(len(sentence), 1, -1)\n",
        "        \n",
        "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
        "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
        "        lstm_feats = self.hidden2tag(lstm_out)\n",
        "        return lstm_feats\n",
        "\n",
        "    def _score_sentence(self, feats, tags):\n",
        "        # Gives the score of a provided tag sequence\n",
        "        score = torch.zeros(1).to(device)\n",
        "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long).to(device), tags])\n",
        "        for i, feat in enumerate(feats):\n",
        "            score = score + \\\n",
        "                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
        "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
        "        return score\n",
        "\n",
        "    def _viterbi_decode(self, feats):\n",
        "        backpointers = []\n",
        "\n",
        "        # Initialize the viterbi variables in log space\n",
        "        init_vvars = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
        "\n",
        "        # forward_var at step i holds the viterbi variables for step i-1\n",
        "        forward_var = init_vvars\n",
        "        for feat in feats:\n",
        "            bptrs_t = []  # holds the backpointers for this step\n",
        "            viterbivars_t = []  # holds the viterbi variables for this step\n",
        "\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
        "                # previous step, plus the score of transitioning\n",
        "                # from tag i to next_tag.\n",
        "                # We don't include the emission scores here because the max\n",
        "                # does not depend on them (we add them in below)\n",
        "                next_tag_var = forward_var + self.transitions[next_tag]\n",
        "                best_tag_id = argmax(next_tag_var)\n",
        "                bptrs_t.append(best_tag_id)\n",
        "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
        "            # Now add in the emission scores, and assign forward_var to the set\n",
        "            # of viterbi variables we just computed\n",
        "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
        "            backpointers.append(bptrs_t)\n",
        "\n",
        "        # Transition to STOP_TAG\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        best_tag_id = argmax(terminal_var)\n",
        "        path_score = terminal_var[0][best_tag_id]\n",
        "\n",
        "        # Follow the back pointers to decode the best path.\n",
        "        best_path = [best_tag_id]\n",
        "        for bptrs_t in reversed(backpointers):\n",
        "            best_tag_id = bptrs_t[best_tag_id]\n",
        "            best_path.append(best_tag_id)\n",
        "        # Pop off the start tag (we dont want to return that to the caller)\n",
        "        start = best_path.pop()\n",
        "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
        "        best_path.reverse()\n",
        "        return path_score, best_path\n",
        "\n",
        "    def neg_log_likelihood(self, sentence, postaglist,deptaglist, tags):\n",
        "        feats = self._get_lstm_features(sentence,postaglist,deptaglist)\n",
        "        forward_score = self._forward_alg(feats)\n",
        "        gold_score = self._score_sentence(feats, tags)\n",
        "        return forward_score - gold_score\n",
        "\n",
        "    def forward(self, sentence, postaglist,deptaglist):  # dont confuse this with _forward_alg above.\n",
        "        # Get the emission scores from the BiLSTM\n",
        "        lstm_feats = self._get_lstm_features(sentence,postaglist,deptaglist)\n",
        "\n",
        "        # Find the best path, given the features.\n",
        "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
        "        return score, tag_seq\n",
        "\n",
        "# initialize model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, p_EMBEDDING_DIM, HIDDEN_DIM).to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
        "\n",
        "\n",
        "\"\"\"Each epoch will take about 2-3 minutes\"\"\"\n",
        "\n",
        "import datetime\n",
        "\n",
        "for epoch in range(max_epoch):  \n",
        "    time1 = datetime.datetime.now()\n",
        "    train_loss = 0\n",
        "\n",
        "    model.train()\n",
        "    for i, idxs in enumerate(train_input_index):\n",
        "        tags_index = train_output_index[i]\n",
        "        pos_idxs = train_POS_index[i]\n",
        "        dep_idxs = train_DEP_index[i]\n",
        "\n",
        "        # Step 1. Remember that Pytorch accumulates gradients.\n",
        "        # We need to clear them out before each instance\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Step 2. Get our inputs ready for the network, that is,\n",
        "        # turn them into Tensors of word indices.\n",
        "        sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "        postaglist_in = torch.tensor(pos_idxs, dtype=torch.long).to(device)\n",
        "        deptaglist_in = torch.tensor(dep_idxs, dtype=torch.long).to(device)\n",
        "\n",
        "        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "\n",
        "        # Step 3. Run our forward pass.\n",
        "        loss = model.neg_log_likelihood(sentence_in,postaglist_in,deptaglist_in, targets)\n",
        "\n",
        "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "        # calling optimizer.step()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss+=loss.item()\n",
        "\n",
        "    model.eval()\n",
        "    _, _, train_acc = cal_acc(model,train_input_index,train_POS_index,train_DEP_index,train_output_index)\n",
        "    _, _, val_acc = cal_acc(model,val_input_index,val_POS_index,val_DEP_index,val_output_index)\n",
        "\n",
        "    val_loss = 0\n",
        "    for i, idxs in enumerate(val_input_index):\n",
        "        tags_index = val_output_index[i]\n",
        "        pos_idxs = val_POS_index[i]\n",
        "        dep_idxs = val_DEP_index[i]\n",
        "        sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "        postaglist_in = torch.tensor(pos_idxs, dtype=torch.long).to(device)\n",
        "        deptaglist_in = torch.tensor(dep_idxs, dtype=torch.long).to(device)\n",
        "        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "        loss = model.neg_log_likelihood(sentence_in, postaglist_in,deptaglist_in, targets)\n",
        "        val_loss+=loss.item()\n",
        "    time2 = datetime.datetime.now()\n",
        "\n",
        "    print(\"Epoch:%d, Training loss: %.2f, train acc: %.4f, val loss: %.2f, val acc: %.4f, time: %.2fs\" %(epoch+1, train_loss,train_acc, val_loss, val_acc, (time2-time1).total_seconds()))\n",
        "\n",
        "y_true,y_pred,_ = cal_acc(model,val_input_index,val_POS_index,val_DEP_index,val_output_index)\n",
        "y_true_decode = decode_output(y_true)\n",
        "y_pred_decode = decode_output(y_pred)\n",
        "\n",
        "print(classification_report(y_true_decode,y_pred_decode,digits=4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:1, Training loss: 13822.45, train acc: 0.7527, val loss: 4337.94, val acc: 0.6976, time: 93.83s\n",
            "Epoch:2, Training loss: 7556.03, train acc: 0.7953, val loss: 3375.18, val acc: 0.7397, time: 94.76s\n",
            "Epoch:3, Training loss: 5821.77, train acc: 0.8234, val loss: 2811.22, val acc: 0.7651, time: 95.31s\n",
            "Epoch:4, Training loss: 4894.83, train acc: 0.8535, val loss: 2453.44, val acc: 0.7901, time: 95.44s\n",
            "Epoch:5, Training loss: 4262.00, train acc: 0.8639, val loss: 2282.00, val acc: 0.7956, time: 98.15s\n",
            "Epoch:6, Training loss: 3752.40, train acc: 0.8734, val loss: 2164.16, val acc: 0.8053, time: 96.61s\n",
            "Epoch:7, Training loss: 3354.75, train acc: 0.8837, val loss: 2118.13, val acc: 0.8093, time: 96.89s\n",
            "Epoch:8, Training loss: 2974.44, train acc: 0.8946, val loss: 2073.51, val acc: 0.8115, time: 96.96s\n",
            "Epoch:9, Training loss: 2686.66, train acc: 0.9038, val loss: 2036.06, val acc: 0.8113, time: 97.14s\n",
            "Epoch:10, Training loss: 2379.47, train acc: 0.9071, val loss: 2082.58, val acc: 0.8108, time: 96.75s\n",
            "Epoch:11, Training loss: 2166.34, train acc: 0.9150, val loss: 2101.26, val acc: 0.8113, time: 98.09s\n",
            "Epoch:12, Training loss: 1954.33, train acc: 0.9254, val loss: 2084.50, val acc: 0.8220, time: 97.57s\n",
            "Epoch:13, Training loss: 1802.38, train acc: 0.9298, val loss: 2094.92, val acc: 0.8214, time: 96.69s\n",
            "Epoch:14, Training loss: 1635.33, train acc: 0.9228, val loss: 2323.34, val acc: 0.8098, time: 97.04s\n",
            "Epoch:15, Training loss: 1556.79, train acc: 0.9421, val loss: 2121.33, val acc: 0.8216, time: 96.40s\n",
            "Epoch:16, Training loss: 1343.36, train acc: 0.9441, val loss: 2248.74, val acc: 0.8182, time: 94.90s\n",
            "Epoch:17, Training loss: 1231.18, train acc: 0.9530, val loss: 2195.06, val acc: 0.8201, time: 97.31s\n",
            "Epoch:18, Training loss: 1147.09, train acc: 0.9537, val loss: 2287.01, val acc: 0.8172, time: 96.93s\n",
            "Epoch:19, Training loss: 1047.28, train acc: 0.9551, val loss: 2276.22, val acc: 0.8214, time: 96.36s\n",
            "Epoch:20, Training loss: 1001.63, train acc: 0.9615, val loss: 2318.07, val acc: 0.8214, time: 96.70s\n",
            "                     precision    recall  f1-score   support\n",
            "\n",
            "B-DocumentReference     0.2500    0.3846    0.3030        13\n",
            "         B-Location     0.6613    0.6649    0.6631       185\n",
            " B-MilitaryPlatform     0.1250    0.4000    0.1905         5\n",
            "            B-Money     0.2000    0.3333    0.2500         3\n",
            "      B-Nationality     0.2500    0.4000    0.3077         5\n",
            "     B-Organisation     0.6643    0.7181    0.6902       259\n",
            "           B-Person     0.8137    0.7830    0.7981       106\n",
            "         B-Quantity     0.7818    0.7414    0.7611        58\n",
            "         B-Temporal     0.7447    0.7292    0.7368        48\n",
            "           B-Weapon     0.2368    0.2727    0.2535        33\n",
            "I-DocumentReference     0.3735    0.7561    0.5000        41\n",
            "         I-Location     0.5321    0.6211    0.5732       227\n",
            " I-MilitaryPlatform     0.1250    1.0000    0.2222         2\n",
            "            I-Money     0.5000    0.8333    0.6250         6\n",
            "      I-Nationality     0.0000    0.0000    0.0000         0\n",
            "     I-Organisation     0.6971    0.7713    0.7323       376\n",
            "           I-Person     0.7052    0.9037    0.7922       135\n",
            "         I-Quantity     0.3462    0.6923    0.4615        13\n",
            "         I-Temporal     0.8548    0.8983    0.8760        59\n",
            "           I-Weapon     0.2000    0.4091    0.2687        22\n",
            "                  O     0.9325    0.8670    0.8986      3678\n",
            "\n",
            "           accuracy                         0.8229      5274\n",
            "          macro avg     0.4759    0.6276    0.5192      5274\n",
            "       weighted avg     0.8458    0.8229    0.8318      5274\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PYhgvtgpB7J"
      },
      "source": [
        "Save model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0x8TX5uo_Pi",
        "outputId": "6839ebe1-75ea-4b1b-a29e-c5a668570e40"
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnPwiqTLo_Rv"
      },
      "source": [
        "# Save the Model\n",
        "torch.save(model, '/content/gdrive/MyDrive/2021-comp5046-a2/BestInputEmbedding_NER_Model.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6-LMzO06aII"
      },
      "source": [
        "load model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cA9lg53-4Rha"
      },
      "source": [
        "# Load the model\n",
        "# Code to download file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "id = '1BbOFJee2_-cZzrBQb0_gSn3Ft5XG8NxM'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('BestInputEmbedding_NER_Model.pt')\n",
        "\n",
        "BestInputEmbedding_NER_Model = torch.load('BestInputEmbedding_NER_Model.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhhMQoI248lN",
        "outputId": "aea7a755-ebe7-4176-8afc-0507669d86d7"
      },
      "source": [
        "y_true,y_pred,_ = cal_acc(BestInputEmbedding_NER_Model,val_input_index,val_POS_index,val_DEP_index,val_output_index)\n",
        "y_true_decode = decode_output(y_true)\n",
        "y_pred_decode = decode_output(y_pred)\n",
        "\n",
        "print(classification_report(y_true_decode,y_pred_decode,digits=4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                     precision    recall  f1-score   support\n",
            "\n",
            "B-DocumentReference     0.1000    0.2500    0.1429         8\n",
            "         B-Location     0.6452    0.6742    0.6593       178\n",
            " B-MilitaryPlatform     0.1250    0.3333    0.1818         6\n",
            "            B-Money     0.2000    0.5000    0.2857         2\n",
            "      B-Nationality     0.1250    0.5000    0.2000         2\n",
            "     B-Organisation     0.6750    0.7269    0.7000       260\n",
            "           B-Person     0.8333    0.8173    0.8252       104\n",
            "         B-Quantity     0.8000    0.6984    0.7458        63\n",
            "         B-Temporal     0.7872    0.7115    0.7475        52\n",
            "           B-Weapon     0.1316    0.2000    0.1587        25\n",
            "I-DocumentReference     0.2048    0.6071    0.3063        28\n",
            "         I-Location     0.5509    0.6348    0.5899       230\n",
            " I-MilitaryPlatform     0.0625    1.0000    0.1176         1\n",
            "            I-Money     0.6000    1.0000    0.7500         6\n",
            "      I-Nationality     0.0000    0.0000    0.0000         0\n",
            "     I-Organisation     0.6538    0.7640    0.7047       356\n",
            "           I-Person     0.7746    0.8481    0.8097       158\n",
            "         I-Quantity     0.3462    0.8182    0.4865        11\n",
            "         I-Temporal     0.8548    0.6543    0.7413        81\n",
            "           I-Weapon     0.1556    0.4118    0.2258        17\n",
            "                  O     0.9342    0.8668    0.8992      3686\n",
            "\n",
            "           accuracy                         0.8203      5274\n",
            "          macro avg     0.4552    0.6198    0.4894      5274\n",
            "       weighted avg     0.8503    0.8203    0.8323      5274\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-KTSfezgKKG"
      },
      "source": [
        "#### Discussion\n",
        "Which one is the optimal (according to the accuracy and weighted average F1 score)?\n",
        "\n",
        "In this section, we test four models that use Bi-LSTM CRF architecture and vary the input embeddings. \n",
        "\n",
        "1. FastText vs. Glove-twitter-25: \\\n",
        "Comparing Bi-LSTM CRF with FastText and baseline model which uses Glove-twitter-25 pretrained embeddings, we can find using Glove-twitter-25 can achieve better performance. \n",
        "\n",
        "2. Baseline vs. Incorporating POS or Dependency information:\\\n",
        "Incorporating POS or Dependency information can improve performance.\n",
        "\n",
        "3. POS information vs. Dependency information vs. Combining all:\\\n",
        "Using POS information achieved better performance than using Dependency information.\\\n",
        "Using both POS and Dependecy information achieved the best performance.\n",
        "\n",
        "4. Glove-twitter-25 vs. Glove-twitter-50\n",
        "Using Glove-twitter-50 incorporating POS and Dependecy information  achieved the best performance.\n",
        "\n",
        "\n",
        "Therefore, the optimal input embedding model is ***Glove-twitter-50 incorporating POS and Dependency information***. The subsequent experiments are using the optimal input embedding model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fN501LyHf8wr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1Yb5lWqf03-"
      },
      "source": [
        "## 5 Stacked Bi-LSTM CRF\n",
        "provide the optimal number of stacked layers\\\n",
        "Note: You should justify the optimal number of stacked layers. (NOTE: at least 2 different numbers should be tested)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbV8coUtfCWa"
      },
      "source": [
        "Use Glove-twitter-25 incorporating POS and Dependency information as input embedding. because Glove-twitter-25 can save more time than Glove-twitter-50.\\\n",
        "Setting different number of stacked layers: ***2, 3***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NEhM8U3I5Pn"
      },
      "source": [
        "Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFb8-PU9I6-v"
      },
      "source": [
        "\n",
        "def cal_acc(model, input_index, val_POS_index ,val_DEP_index, output_index):\n",
        "    ground_truth = []\n",
        "    predicted = []\n",
        "    for i,idxs in enumerate(input_index):\n",
        "        ground_truth += output_index[i]\n",
        "        pos_idxs = val_POS_index[i]\n",
        "        dep_idxs = val_DEP_index[i]\n",
        "\n",
        "        postaglist_in = torch.tensor(pos_idxs, dtype=torch.long).to(device)\n",
        "        deptaglist_in = torch.tensor(dep_idxs, dtype=torch.long).to(device)\n",
        "        sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "\n",
        "        score, pred = model(sentence_in,postaglist_in,deptaglist_in)\n",
        "        predicted += pred\n",
        "    accuracy = sum(np.array(ground_truth) == np.array(predicted))/len(ground_truth)\n",
        "    return predicted, ground_truth, accuracy\n",
        "\n",
        "class BiLSTM_CRF(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim, NUM_LAYERS ):\n",
        "        super(BiLSTM_CRF, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "        self.tagset_size = len(tag_to_ix)\n",
        "        self.NUM_LAYERS = NUM_LAYERS\n",
        "\n",
        "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
        "        \"\"\"Here we use the embedding matrix as the initial weights of nn.Embedding\"\"\"\n",
        "        self.word_embeds.weight.data.copy_(torch.from_numpy(p_embedding_matrix))\n",
        "\n",
        "        self.postag_embeds = nn.Embedding(embedding_matrix_POS.shape[0], EMBEDDING_DIM_POS)\n",
        "        self.postag_embeds.weight.data.copy_(torch.from_numpy(embedding_matrix_POS))\n",
        "\n",
        "        self.deptag_embeds = nn.Embedding(embedding_matrix_DEP.shape[0], EMBEDDING_DIM_DEP)\n",
        "        self.deptag_embeds.weight.data.copy_(torch.from_numpy(embedding_matrix_DEP))\n",
        "        \n",
        "        self.lstm = nn.LSTM(embedding_dim+EMBEDDING_DIM_POS+EMBEDDING_DIM_DEP, hidden_dim // 2,\n",
        "                            num_layers=NUM_LAYERS, bidirectional=True)\n",
        "\n",
        "        # Maps the output of the LSTM into tag space.\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
        "\n",
        "        # Matrix of transition parameters.  Entry i,j is the score of\n",
        "        # transitioning *to* i *from* j.\n",
        "        self.transitions = nn.Parameter(\n",
        "            torch.randn(self.tagset_size, self.tagset_size))\n",
        "\n",
        "        # These two statements enforce the constraint that we never transfer\n",
        "        # to the start tag and we never transfer from the stop tag\n",
        "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
        "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
        "\n",
        "        self.hidden = self.init_hidden()\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return (torch.randn(2*NUM_LAYERS, 1, self.hidden_dim // 2).to(device),\n",
        "                torch.randn(2*NUM_LAYERS, 1, self.hidden_dim // 2).to(device))\n",
        "\n",
        "    def _forward_alg(self, feats):\n",
        "        # Do the forward algorithm to compute the partition function\n",
        "        init_alphas = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "        # START_TAG has all of the score.\n",
        "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
        "\n",
        "        # Wrap in a variable so that we will get automatic backprop\n",
        "        forward_var = init_alphas\n",
        "\n",
        "        # Iterate through the sentence\n",
        "        for feat in feats:\n",
        "            alphas_t = []  # The forward tensors at this timestep\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # broadcast the emission score: it is the same regardless of\n",
        "                # the previous tag\n",
        "                emit_score = feat[next_tag].view(\n",
        "                    1, -1).expand(1, self.tagset_size)\n",
        "                # the ith entry of trans_score is the score of transitioning to\n",
        "                # next_tag from i\n",
        "                trans_score = self.transitions[next_tag].view(1, -1)\n",
        "                # The ith entry of next_tag_var is the value for the\n",
        "                # edge (i -> next_tag) before we do log-sum-exp\n",
        "                next_tag_var = forward_var + trans_score + emit_score\n",
        "                # The forward variable for this tag is log-sum-exp of all the\n",
        "                # scores.\n",
        "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
        "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        alpha = log_sum_exp(terminal_var)\n",
        "        return alpha\n",
        "\n",
        "    def _get_lstm_features(self, sentence, postaglist, deptaglist):\n",
        "        self.hidden = self.init_hidden()\n",
        "        wordembeds = self.word_embeds(sentence)\n",
        "        postagembeds = self.postag_embeds(postaglist)\n",
        "        deptagembeds = self.deptag_embeds(deptaglist)\n",
        "        embeds = torch.cat((wordembeds, postagembeds, deptagembeds), 1).view(len(sentence), 1, -1)\n",
        "        \n",
        "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
        "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
        "        lstm_feats = self.hidden2tag(lstm_out)\n",
        "        return lstm_feats\n",
        "\n",
        "    def _score_sentence(self, feats, tags):\n",
        "        # Gives the score of a provided tag sequence\n",
        "        score = torch.zeros(1).to(device)\n",
        "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long).to(device), tags])\n",
        "        for i, feat in enumerate(feats):\n",
        "            score = score + \\\n",
        "                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
        "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
        "        return score\n",
        "\n",
        "    def _viterbi_decode(self, feats):\n",
        "        backpointers = []\n",
        "\n",
        "        # Initialize the viterbi variables in log space\n",
        "        init_vvars = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
        "\n",
        "        # forward_var at step i holds the viterbi variables for step i-1\n",
        "        forward_var = init_vvars\n",
        "        for feat in feats:\n",
        "            bptrs_t = []  # holds the backpointers for this step\n",
        "            viterbivars_t = []  # holds the viterbi variables for this step\n",
        "\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
        "                # previous step, plus the score of transitioning\n",
        "                # from tag i to next_tag.\n",
        "                # We don't include the emission scores here because the max\n",
        "                # does not depend on them (we add them in below)\n",
        "                next_tag_var = forward_var + self.transitions[next_tag]\n",
        "                best_tag_id = argmax(next_tag_var)\n",
        "                bptrs_t.append(best_tag_id)\n",
        "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
        "            # Now add in the emission scores, and assign forward_var to the set\n",
        "            # of viterbi variables we just computed\n",
        "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
        "            backpointers.append(bptrs_t)\n",
        "\n",
        "        # Transition to STOP_TAG\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        best_tag_id = argmax(terminal_var)\n",
        "        path_score = terminal_var[0][best_tag_id]\n",
        "\n",
        "        # Follow the back pointers to decode the best path.\n",
        "        best_path = [best_tag_id]\n",
        "        for bptrs_t in reversed(backpointers):\n",
        "            best_tag_id = bptrs_t[best_tag_id]\n",
        "            best_path.append(best_tag_id)\n",
        "        # Pop off the start tag (we dont want to return that to the caller)\n",
        "        start = best_path.pop()\n",
        "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
        "        best_path.reverse()\n",
        "        return path_score, best_path\n",
        "\n",
        "    def neg_log_likelihood(self, sentence, postaglist,deptaglist, tags):\n",
        "        feats = self._get_lstm_features(sentence,postaglist,deptaglist)\n",
        "        forward_score = self._forward_alg(feats)\n",
        "        gold_score = self._score_sentence(feats, tags)\n",
        "        return forward_score - gold_score\n",
        "\n",
        "    def forward(self, sentence, postaglist,deptaglist):  # dont confuse this with _forward_alg above.\n",
        "        # Get the emission scores from the BiLSTM\n",
        "        lstm_feats = self._get_lstm_features(sentence,postaglist,deptaglist)\n",
        "\n",
        "        # Find the best path, given the features.\n",
        "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
        "        return score, tag_seq\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"Each epoch will take about 2-3 minutes\"\"\"\n",
        "import datetime\n",
        "def model_performance(model):\n",
        "\n",
        "  for epoch in range(max_epoch):  \n",
        "      time1 = datetime.datetime.now()\n",
        "      train_loss = 0\n",
        "\n",
        "      model.train()\n",
        "      for i, idxs in enumerate(train_input_index):\n",
        "          tags_index = train_output_index[i]\n",
        "          pos_idxs = train_POS_index[i]\n",
        "          dep_idxs = train_DEP_index[i]\n",
        "\n",
        "          # Step 1. Remember that Pytorch accumulates gradients.\n",
        "          # We need to clear them out before each instance\n",
        "          model.zero_grad()\n",
        "\n",
        "          # Step 2. Get our inputs ready for the network, that is,\n",
        "          # turn them into Tensors of word indices.\n",
        "          sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "          postaglist_in = torch.tensor(pos_idxs, dtype=torch.long).to(device)\n",
        "          deptaglist_in = torch.tensor(dep_idxs, dtype=torch.long).to(device)\n",
        "\n",
        "          targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "\n",
        "          # Step 3. Run our forward pass.\n",
        "          loss = model.neg_log_likelihood(sentence_in,postaglist_in,deptaglist_in, targets)\n",
        "\n",
        "          # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "          # calling optimizer.step()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          train_loss+=loss.item()\n",
        "\n",
        "      model.eval()\n",
        "      _, _, train_acc = cal_acc(model,train_input_index,train_POS_index,train_DEP_index,train_output_index)\n",
        "      _, _, val_acc = cal_acc(model,val_input_index,val_POS_index,val_DEP_index,val_output_index)\n",
        "\n",
        "      val_loss = 0\n",
        "      for i, idxs in enumerate(val_input_index):\n",
        "          tags_index = val_output_index[i]\n",
        "          pos_idxs = val_POS_index[i]\n",
        "          dep_idxs = val_DEP_index[i]\n",
        "          sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "          postaglist_in = torch.tensor(pos_idxs, dtype=torch.long).to(device)\n",
        "          deptaglist_in = torch.tensor(dep_idxs, dtype=torch.long).to(device)\n",
        "          targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "          loss = model.neg_log_likelihood(sentence_in, postaglist_in,deptaglist_in, targets)\n",
        "          val_loss+=loss.item()\n",
        "      time2 = datetime.datetime.now()\n",
        "\n",
        "      print(\"Epoch:%d, Training loss: %.2f, train acc: %.4f, val loss: %.2f, val acc: %.4f, time: %.2fs\" %(epoch+1, train_loss,train_acc, val_loss, val_acc, (time2-time1).total_seconds()))\n",
        "\n",
        "  y_true,y_pred,_ = cal_acc(model,val_input_index,val_POS_index,val_DEP_index,val_output_index)\n",
        "  y_true_decode = decode_output(y_true)\n",
        "  y_pred_decode = decode_output(y_pred)\n",
        "\n",
        "  print(classification_report(y_true_decode,y_pred_decode,digits=4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIzeKTMquRIO"
      },
      "source": [
        "####5.1: 2 layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MDTz8RyfJuG",
        "outputId": "3176a13e-59b6-46a9-cd03-838c2fdc9cbe"
      },
      "source": [
        "p_EMBEDDING_DIM = EMBEDDING_DIM\n",
        "p_embedding_matrix = embedding_matrix\n",
        "NUM_LAYERS = 2\n",
        "\n",
        "\n",
        "# initialize model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, p_EMBEDDING_DIM, HIDDEN_DIM, NUM_LAYERS).to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
        "\n",
        "\"\"\"Each epoch will take about 2-3 minutes\"\"\"\n",
        "#performance\n",
        "model_performance(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:1, Training loss: 15860.98, train acc: 0.7145, val loss: 5151.72, val acc: 0.6640, time: 104.28s\n",
            "Epoch:2, Training loss: 9815.57, train acc: 0.7564, val loss: 3988.28, val acc: 0.7000, time: 104.31s\n",
            "Epoch:3, Training loss: 7439.39, train acc: 0.7945, val loss: 3450.17, val acc: 0.7395, time: 104.44s\n",
            "Epoch:4, Training loss: 6191.40, train acc: 0.8175, val loss: 3078.48, val acc: 0.7617, time: 105.22s\n",
            "Epoch:5, Training loss: 5391.57, train acc: 0.8292, val loss: 2777.27, val acc: 0.7681, time: 104.80s\n",
            "Epoch:6, Training loss: 4764.60, train acc: 0.8456, val loss: 2529.09, val acc: 0.7802, time: 106.53s\n",
            "Epoch:7, Training loss: 4296.01, train acc: 0.8537, val loss: 2371.35, val acc: 0.7880, time: 105.70s\n",
            "Epoch:8, Training loss: 3871.04, train acc: 0.8665, val loss: 2220.47, val acc: 0.7971, time: 105.99s\n",
            "Epoch:9, Training loss: 3554.33, train acc: 0.8755, val loss: 2137.31, val acc: 0.8043, time: 104.64s\n",
            "Epoch:10, Training loss: 3240.16, train acc: 0.8853, val loss: 2068.16, val acc: 0.8032, time: 105.75s\n",
            "Epoch:11, Training loss: 2940.09, train acc: 0.8913, val loss: 2045.32, val acc: 0.8089, time: 104.87s\n",
            "Epoch:12, Training loss: 2683.09, train acc: 0.8951, val loss: 2058.99, val acc: 0.8074, time: 104.12s\n",
            "Epoch:13, Training loss: 2452.49, train acc: 0.9047, val loss: 2049.01, val acc: 0.8174, time: 104.84s\n",
            "Epoch:14, Training loss: 2237.58, train acc: 0.9030, val loss: 2116.90, val acc: 0.8045, time: 106.84s\n",
            "Epoch:15, Training loss: 2048.80, train acc: 0.9104, val loss: 2205.84, val acc: 0.8115, time: 105.60s\n",
            "Epoch:16, Training loss: 1873.80, train acc: 0.9131, val loss: 2288.20, val acc: 0.8115, time: 105.89s\n",
            "Epoch:17, Training loss: 1689.03, train acc: 0.9187, val loss: 2189.67, val acc: 0.8146, time: 105.14s\n",
            "Epoch:18, Training loss: 1550.83, train acc: 0.9260, val loss: 2272.06, val acc: 0.8115, time: 104.60s\n",
            "Epoch:19, Training loss: 1410.28, train acc: 0.9380, val loss: 2313.72, val acc: 0.8102, time: 105.17s\n",
            "Epoch:20, Training loss: 1308.75, train acc: 0.9356, val loss: 2424.56, val acc: 0.8119, time: 104.70s\n",
            "                     precision    recall  f1-score   support\n",
            "\n",
            "B-DocumentReference     0.2000    0.5000    0.2857         8\n",
            "         B-Location     0.5806    0.6708    0.6225       161\n",
            " B-MilitaryPlatform     0.0625    0.3333    0.1053         3\n",
            "            B-Money     0.2000    0.5000    0.2857         2\n",
            "      B-Nationality     0.2500    0.3333    0.2857         6\n",
            "     B-Organisation     0.6714    0.6690    0.6702       281\n",
            "           B-Person     0.8235    0.7850    0.8038       107\n",
            "         B-Quantity     0.7091    0.7358    0.7222        53\n",
            "         B-Temporal     0.7021    0.7500    0.7253        44\n",
            "           B-Weapon     0.1579    0.3333    0.2143        18\n",
            "I-DocumentReference     0.2169    0.6429    0.3243        28\n",
            "         I-Location     0.4377    0.7296    0.5472       159\n",
            " I-MilitaryPlatform     0.0625    1.0000    0.1176         1\n",
            "            I-Money     0.4000    1.0000    0.5714         4\n",
            "      I-Nationality     0.0000    0.0000    0.0000         0\n",
            "     I-Organisation     0.6178    0.7515    0.6781       342\n",
            "           I-Person     0.7919    0.8896    0.8379       154\n",
            "         I-Quantity     0.2308    0.5455    0.3243        11\n",
            "         I-Temporal     0.6452    0.8163    0.7207        49\n",
            "           I-Weapon     0.0889    0.3077    0.1379        13\n",
            "                  O     0.9480    0.8465    0.8943      3830\n",
            "\n",
            "           accuracy                         0.8136      5274\n",
            "          macro avg     0.4189    0.6257    0.4702      5274\n",
            "       weighted avg     0.8574    0.8136    0.8303      5274\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsQlxlFvuVB2"
      },
      "source": [
        "####5.2: 3 layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZx0_-vJ_uFl",
        "outputId": "79e5d6e5-8aa9-4dab-965c-bb822ceaa1c8"
      },
      "source": [
        "p_EMBEDDING_DIM = EMBEDDING_DIM\n",
        "p_embedding_matrix = embedding_matrix\n",
        "NUM_LAYERS = 3\n",
        "\n",
        "# initialize model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, p_EMBEDDING_DIM, HIDDEN_DIM, NUM_LAYERS).to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
        "\n",
        "#performance\n",
        "model_performance(model)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:1, Training loss: 16740.55, train acc: 0.6999, val loss: 5743.02, val acc: 0.6485, time: 180.96s\n",
            "Epoch:2, Training loss: 12467.94, train acc: 0.7173, val loss: 4554.24, val acc: 0.6629, time: 180.45s\n",
            "Epoch:3, Training loss: 9435.74, train acc: 0.7366, val loss: 3937.10, val acc: 0.6885, time: 180.63s\n",
            "Epoch:4, Training loss: 8102.19, train acc: 0.7563, val loss: 3575.14, val acc: 0.7052, time: 180.00s\n",
            "Epoch:5, Training loss: 7051.72, train acc: 0.7851, val loss: 3216.82, val acc: 0.7251, time: 180.31s\n",
            "Epoch:6, Training loss: 6127.77, train acc: 0.8048, val loss: 2970.64, val acc: 0.7473, time: 179.60s\n",
            "Epoch:7, Training loss: 5423.33, train acc: 0.8246, val loss: 2673.93, val acc: 0.7666, time: 179.83s\n",
            "Epoch:8, Training loss: 4841.56, train acc: 0.8419, val loss: 2424.91, val acc: 0.7852, time: 180.05s\n",
            "Epoch:9, Training loss: 4372.80, train acc: 0.8563, val loss: 2271.90, val acc: 0.7926, time: 179.80s\n",
            "Epoch:10, Training loss: 4016.38, train acc: 0.8690, val loss: 2129.14, val acc: 0.8011, time: 180.14s\n",
            "Epoch:11, Training loss: 3719.66, train acc: 0.8742, val loss: 2076.58, val acc: 0.8045, time: 179.78s\n",
            "Epoch:12, Training loss: 3407.47, train acc: 0.8840, val loss: 2027.67, val acc: 0.8111, time: 180.47s\n",
            "Epoch:13, Training loss: 3147.29, train acc: 0.8910, val loss: 2007.60, val acc: 0.8098, time: 180.04s\n",
            "Epoch:14, Training loss: 2959.42, train acc: 0.8980, val loss: 1964.84, val acc: 0.8083, time: 180.24s\n",
            "Epoch:15, Training loss: 2683.02, train acc: 0.9038, val loss: 2033.91, val acc: 0.8111, time: 180.07s\n",
            "Epoch:16, Training loss: 2513.16, train acc: 0.9062, val loss: 2009.66, val acc: 0.8096, time: 180.06s\n",
            "Epoch:17, Training loss: 2287.30, train acc: 0.9114, val loss: 2125.57, val acc: 0.8117, time: 179.90s\n",
            "Epoch:18, Training loss: 2140.50, train acc: 0.9136, val loss: 2151.97, val acc: 0.8079, time: 179.81s\n",
            "Epoch:19, Training loss: 1925.98, train acc: 0.9242, val loss: 2219.11, val acc: 0.8077, time: 179.92s\n",
            "Epoch:20, Training loss: 1821.43, train acc: 0.9243, val loss: 2182.86, val acc: 0.8083, time: 179.82s\n",
            "                     precision    recall  f1-score   support\n",
            "\n",
            "B-DocumentReference     0.0000    0.0000    0.0000         0\n",
            "         B-Location     0.5699    0.6272    0.5972       169\n",
            " B-MilitaryPlatform     0.0625    1.0000    0.1176         1\n",
            "            B-Money     0.2000    0.5000    0.2857         2\n",
            "      B-Nationality     0.1250    0.5000    0.2000         2\n",
            "     B-Organisation     0.6500    0.7027    0.6753       259\n",
            "           B-Person     0.8431    0.8515    0.8473       101\n",
            "         B-Quantity     0.7273    0.7407    0.7339        54\n",
            "         B-Temporal     0.7021    0.7174    0.7097        46\n",
            "           B-Weapon     0.0000    0.0000    0.0000         0\n",
            "I-DocumentReference     0.1325    1.0000    0.2340        11\n",
            "         I-Location     0.4453    0.6114    0.5153       193\n",
            " I-MilitaryPlatform     0.0625    1.0000    0.1176         1\n",
            "            I-Money     0.4000    1.0000    0.5714         4\n",
            "      I-Nationality     0.0000    0.0000    0.0000         0\n",
            "     I-Organisation     0.6442    0.6889    0.6658       389\n",
            "           I-Person     0.7514    0.9091    0.8228       143\n",
            "         I-Quantity     0.2692    0.2692    0.2692        26\n",
            "         I-Temporal     0.6774    0.8400    0.7500        50\n",
            "           I-Weapon     0.0000    0.0000    0.0000         0\n",
            "                  O     0.9494    0.8493    0.8966      3823\n",
            "\n",
            "           accuracy                         0.8111      5274\n",
            "          macro avg     0.3910    0.6099    0.4290      5274\n",
            "       weighted avg     0.8608    0.8111    0.8320      5274\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1JyDGOGuY1O"
      },
      "source": [
        "#### Discussion\n",
        "Which one is the optimal?\n",
        "\n",
        "In this section, we test 2 models that set 2 or 3 hidden layers, using Glove-twitter-25 incorporating POS and Dependency information as input embedding. \n",
        "\n",
        "Comparing with the one with only one hidden layer, we can find stacking 2 or 3 hidden layers doesn't achieve better performance.\n",
        "\n",
        "The optimal input embedding model is the one that set one hidden layer. The subsequent experiments are using the optimal input embedding model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nu7_KQvfqa_X"
      },
      "source": [
        "## 6 Attention\n",
        "consider to use attention\\\n",
        "Note: You may need to justify your attention score calculation method (at least 3 methods should be tested), the position, and the number of attention you used. This is really depends on your model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44GekPiffOKg"
      },
      "source": [
        "Use Glove-twitter-25 incorporating POS and Dependency information as input embedding. because Glove-twitter-25 can save more time than Glove-twitter-50\\\n",
        "Set the optimal number of stacked layers: 1. \\\n",
        "Setting different attention score calculation methods: ***dot product, scaled dot product, and cosine similarity***."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzFUSGsJvcow"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "p_EMBEDDING_DIM = EMBEDDING_DIM\n",
        "p_embedding_matrix = embedding_matrix\n",
        "NUM_LAYERS = 1\n",
        "\n",
        "def cal_acc(model, input_index, val_POS_index ,val_DEP_index, output_index,method):\n",
        "    ground_truth = []\n",
        "    predicted = []\n",
        "    for i,idxs in enumerate(input_index):\n",
        "        ground_truth += output_index[i]\n",
        "        pos_idxs = val_POS_index[i]\n",
        "        dep_idxs = val_DEP_index[i]\n",
        "\n",
        "        postaglist_in = torch.tensor(pos_idxs, dtype=torch.long).to(device)\n",
        "        deptaglist_in = torch.tensor(dep_idxs, dtype=torch.long).to(device)\n",
        "        sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "\n",
        "        score, pred = model(sentence_in,postaglist_in,deptaglist_in,method)\n",
        "        predicted += pred\n",
        "    accuracy = sum(np.array(ground_truth) == np.array(predicted))/len(ground_truth)\n",
        "    return predicted, ground_truth, accuracy\n",
        "\n",
        "class BiLSTM_CRF(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):\n",
        "        super(BiLSTM_CRF, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "        self.tagset_size = len(tag_to_ix)\n",
        "\n",
        "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
        "        \"\"\"Here we use the embedding matrix as the initial weights of nn.Embedding\"\"\"\n",
        "        self.word_embeds.weight.data.copy_(torch.from_numpy(p_embedding_matrix))\n",
        "\n",
        "        self.postag_embeds = nn.Embedding(embedding_matrix_POS.shape[0], EMBEDDING_DIM_POS)\n",
        "        self.postag_embeds.weight.data.copy_(torch.from_numpy(embedding_matrix_POS))\n",
        "\n",
        "        self.deptag_embeds = nn.Embedding(embedding_matrix_DEP.shape[0], EMBEDDING_DIM_DEP)\n",
        "        self.deptag_embeds.weight.data.copy_(torch.from_numpy(embedding_matrix_DEP))\n",
        "        \n",
        "        self.lstm = nn.LSTM(embedding_dim+EMBEDDING_DIM_POS+EMBEDDING_DIM_DEP, hidden_dim // 2,\n",
        "                            num_layers=NUM_LAYERS, bidirectional=True)\n",
        "\n",
        "        # Maps the output of the LSTM into tag space.\n",
        "        self.hidden2tag = nn.Linear(hidden_dim*2, self.tagset_size)\n",
        "\n",
        "        # Matrix of transition parameters.  Entry i,j is the score of\n",
        "        # transitioning *to* i *from* j.\n",
        "        self.transitions = nn.Parameter(\n",
        "            torch.randn(self.tagset_size, self.tagset_size))\n",
        "\n",
        "        # These two statements enforce the constraint that we never transfer\n",
        "        # to the start tag and we never transfer from the stop tag\n",
        "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
        "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
        "\n",
        "        self.hidden = self.init_hidden()\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return (torch.randn(2*NUM_LAYERS, 1, self.hidden_dim // 2).to(device),\n",
        "                torch.randn(2*NUM_LAYERS, 1, self.hidden_dim // 2).to(device))\n",
        "    # from lab 10    \n",
        "    def cal_attention(self, decoder_hiddens, encoder_hiddens, method):\n",
        "        if method == \"Dot Product\":\n",
        "            # bmm: https://pytorch.org/docs/master/generated/torch.bmm.html\n",
        "            attn_weights = F.softmax(torch.bmm(decoder_hiddens.unsqueeze(0), encoder_hiddens.T.unsqueeze(0)),dim=-1)\n",
        "            attn_output = torch.bmm(attn_weights, encoder_hiddens.unsqueeze(0))\n",
        "            concat_output = torch.cat((attn_output[0], decoder_hiddens), 1)\n",
        "\n",
        "        elif method == \"Scaled Dot Product\":\n",
        "            # COMPLETE THIS PART - Scale Dot Product calculation method\n",
        "\n",
        "            attn_weights = F.softmax(1/np.sqrt(self.hidden_dim)*torch.bmm(decoder_hiddens.unsqueeze(0), encoder_hiddens.T.unsqueeze(0)),dim=-1)\n",
        "            attn_output = torch.bmm(attn_weights, encoder_hiddens.unsqueeze(0))\n",
        "            concat_output = torch.cat((attn_output[0], decoder_hiddens), 1)\n",
        "\n",
        "\n",
        "        elif method == \"Cosine Similarity\":\n",
        "          attn_weights = torch.cosine_similarity(decoder_hiddens.unsqueeze(1), decoder_hiddens.unsqueeze(0), dim=-1)\n",
        "          attn_output = torch.bmm(attn_weights.unsqueeze(0), encoder_hiddens.unsqueeze(0))\n",
        "          concat_output = torch.cat((attn_output[0], decoder_hiddens), 1)\n",
        "\n",
        "        return concat_output\n",
        "    def _forward_alg(self, feats):\n",
        "        # Do the forward algorithm to compute the partition function\n",
        "        init_alphas = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "        # START_TAG has all of the score.\n",
        "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
        "\n",
        "        # Wrap in a variable so that we will get automatic backprop\n",
        "        forward_var = init_alphas\n",
        "\n",
        "        # Iterate through the sentence\n",
        "        for feat in feats:\n",
        "            alphas_t = []  # The forward tensors at this timestep\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # broadcast the emission score: it is the same regardless of\n",
        "                # the previous tag\n",
        "                emit_score = feat[next_tag].view(\n",
        "                    1, -1).expand(1, self.tagset_size)\n",
        "                # the ith entry of trans_score is the score of transitioning to\n",
        "                # next_tag from i\n",
        "                trans_score = self.transitions[next_tag].view(1, -1)\n",
        "                # The ith entry of next_tag_var is the value for the\n",
        "                # edge (i -> next_tag) before we do log-sum-exp\n",
        "                next_tag_var = forward_var + trans_score + emit_score\n",
        "                # The forward variable for this tag is log-sum-exp of all the\n",
        "                # scores.\n",
        "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
        "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        alpha = log_sum_exp(terminal_var)\n",
        "        return alpha\n",
        "\n",
        "    def _get_lstm_features(self, sentence, postaglist, deptaglist, method):\n",
        "        self.hidden = self.init_hidden()\n",
        "        wordembeds = self.word_embeds(sentence)\n",
        "        postagembeds = self.postag_embeds(postaglist)\n",
        "        deptagembeds = self.deptag_embeds(deptaglist)\n",
        "        embeds = torch.cat((wordembeds, postagembeds, deptagembeds), 1).view(len(sentence), 1, -1)\n",
        "        \n",
        "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
        "\n",
        "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
        "        concat_output = self.cal_attention(lstm_out, lstm_out, method) #concat lstm output and attention output\n",
        "\n",
        "        lstm_feats = self.hidden2tag(concat_output)\n",
        "        return lstm_feats\n",
        "\n",
        "    def _score_sentence(self, feats, tags):\n",
        "        # Gives the score of a provided tag sequence\n",
        "        score = torch.zeros(1).to(device)\n",
        "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long).to(device), tags])\n",
        "        for i, feat in enumerate(feats):\n",
        "            score = score + \\\n",
        "                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
        "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
        "        return score\n",
        "\n",
        "    def _viterbi_decode(self, feats):\n",
        "        backpointers = []\n",
        "\n",
        "        # Initialize the viterbi variables in log space\n",
        "        init_vvars = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
        "\n",
        "        # forward_var at step i holds the viterbi variables for step i-1\n",
        "        forward_var = init_vvars\n",
        "        for feat in feats:\n",
        "            bptrs_t = []  # holds the backpointers for this step\n",
        "            viterbivars_t = []  # holds the viterbi variables for this step\n",
        "\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
        "                # previous step, plus the score of transitioning\n",
        "                # from tag i to next_tag.\n",
        "                # We don't include the emission scores here because the max\n",
        "                # does not depend on them (we add them in below)\n",
        "                next_tag_var = forward_var + self.transitions[next_tag]\n",
        "                best_tag_id = argmax(next_tag_var)\n",
        "                bptrs_t.append(best_tag_id)\n",
        "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
        "            # Now add in the emission scores, and assign forward_var to the set\n",
        "            # of viterbi variables we just computed\n",
        "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
        "            backpointers.append(bptrs_t)\n",
        "\n",
        "        # Transition to STOP_TAG\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        best_tag_id = argmax(terminal_var)\n",
        "        path_score = terminal_var[0][best_tag_id]\n",
        "\n",
        "        # Follow the back pointers to decode the best path.\n",
        "        best_path = [best_tag_id]\n",
        "        for bptrs_t in reversed(backpointers):\n",
        "            best_tag_id = bptrs_t[best_tag_id]\n",
        "            best_path.append(best_tag_id)\n",
        "        # Pop off the start tag (we dont want to return that to the caller)\n",
        "        start = best_path.pop()\n",
        "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
        "        best_path.reverse()\n",
        "        return path_score, best_path\n",
        "\n",
        "    def neg_log_likelihood(self, sentence, postaglist,deptaglist, tags,method):\n",
        "        feats = self._get_lstm_features(sentence,postaglist,deptaglist,method)\n",
        "        forward_score = self._forward_alg(feats)\n",
        "        gold_score = self._score_sentence(feats, tags)\n",
        "        return forward_score - gold_score\n",
        "\n",
        "    def forward(self, sentence, postaglist,deptaglist,method):  # dont confuse this with _forward_alg above.\n",
        "        # Get the emission scores from the BiLSTM\n",
        "        lstm_feats = self._get_lstm_features(sentence,postaglist,deptaglist,method)\n",
        "\n",
        "        # Find the best path, given the features.\n",
        "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
        "        return score, tag_seq\n",
        "\n",
        "\n",
        "import datetime\n",
        "\"\"\"Each epoch will take about 2-3 minutes\"\"\"\n",
        "def model_performance_attention(model, ATTENTION_METHOD):\n",
        "\n",
        "  for epoch in range(max_epoch):  \n",
        "      time1 = datetime.datetime.now()\n",
        "      train_loss = 0\n",
        "\n",
        "      model.train()\n",
        "      for i, idxs in enumerate(train_input_index):\n",
        "          tags_index = train_output_index[i]\n",
        "          pos_idxs = train_POS_index[i]\n",
        "          dep_idxs = train_DEP_index[i]\n",
        "\n",
        "          # Step 1. Remember that Pytorch accumulates gradients.\n",
        "          # We need to clear them out before each instance\n",
        "          model.zero_grad()\n",
        "\n",
        "          # Step 2. Get our inputs ready for the network, that is,\n",
        "          # turn them into Tensors of word indices.\n",
        "          sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "          postaglist_in = torch.tensor(pos_idxs, dtype=torch.long).to(device)\n",
        "          deptaglist_in = torch.tensor(dep_idxs, dtype=torch.long).to(device)\n",
        "\n",
        "          targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "\n",
        "          # Step 3. Run our forward pass.\n",
        "          loss = model.neg_log_likelihood(sentence_in,postaglist_in,deptaglist_in,targets,ATTENTION_METHOD)\n",
        "\n",
        "          # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "          # calling optimizer.step()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          train_loss+=loss.item()\n",
        "\n",
        "      model.eval()\n",
        "      _, _, train_acc = cal_acc(model,train_input_index,train_POS_index,train_DEP_index,train_output_index,ATTENTION_METHOD)\n",
        "      _, _, val_acc = cal_acc(model,val_input_index,val_POS_index,val_DEP_index,val_output_index,ATTENTION_METHOD)\n",
        "\n",
        "      val_loss = 0\n",
        "      for i, idxs in enumerate(val_input_index):\n",
        "          tags_index = val_output_index[i]\n",
        "          pos_idxs = val_POS_index[i]\n",
        "          dep_idxs = val_DEP_index[i]\n",
        "          sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "          postaglist_in = torch.tensor(pos_idxs, dtype=torch.long).to(device)\n",
        "          deptaglist_in = torch.tensor(dep_idxs, dtype=torch.long).to(device)\n",
        "          targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "          loss = model.neg_log_likelihood(sentence_in, postaglist_in, deptaglist_in, targets, ATTENTION_METHOD)\n",
        "          val_loss+=loss.item()\n",
        "      time2 = datetime.datetime.now()\n",
        "\n",
        "      print(\"Epoch:%d, Training loss: %.2f, train acc: %.4f, val loss: %.2f, val acc: %.4f, time: %.2fs\" %(epoch+1, train_loss,train_acc, val_loss, val_acc, (time2-time1).total_seconds()))\n",
        "\n",
        "  y_true,y_pred,_ = cal_acc(model,val_input_index,val_POS_index,val_DEP_index,val_output_index,ATTENTION_METHOD)\n",
        "  y_true_decode = decode_output(y_true)\n",
        "  y_pred_decode = decode_output(y_pred)\n",
        "\n",
        "  print(classification_report(y_true_decode,y_pred_decode,digits=4))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52f8ZZhYt6DF"
      },
      "source": [
        "#### Dot Product"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wO0gSKqovlUd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adf95dc8-4cb1-4c1b-9edf-0a54034589a4"
      },
      "source": [
        "ATTENTION_METHOD = \"Dot Product\"\n",
        "\n",
        "# initialize model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_Dot_Product= BiLSTM_CRF(len(word_to_ix), tag_to_ix, p_EMBEDDING_DIM, HIDDEN_DIM).to(device)\n",
        "optimizer = optim.SGD(model_Dot_Product.parameters(), lr=0.01, weight_decay=1e-4)\n",
        "\n",
        "#model performance dot product\n",
        "model_performance_attention(model_Dot_Product,ATTENTION_METHOD)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:1, Training loss: 17592.84, train acc: 0.6999, val loss: 6022.23, val acc: 0.6485, time: 112.52s\n",
            "Epoch:2, Training loss: 13087.11, train acc: 0.7190, val loss: 4715.50, val acc: 0.6701, time: 113.72s\n",
            "Epoch:3, Training loss: 9697.44, train acc: 0.7525, val loss: 3822.24, val acc: 0.6951, time: 114.42s\n",
            "Epoch:4, Training loss: 8114.91, train acc: 0.7696, val loss: 3409.19, val acc: 0.7196, time: 114.36s\n",
            "Epoch:5, Training loss: 7011.42, train acc: 0.7946, val loss: 3126.66, val acc: 0.7353, time: 114.48s\n",
            "Epoch:6, Training loss: 6091.08, train acc: 0.8139, val loss: 2874.91, val acc: 0.7474, time: 114.37s\n",
            "Epoch:7, Training loss: 5395.33, train acc: 0.8261, val loss: 2638.07, val acc: 0.7700, time: 114.24s\n",
            "Epoch:8, Training loss: 4832.35, train acc: 0.8446, val loss: 2406.10, val acc: 0.7819, time: 114.69s\n",
            "Epoch:9, Training loss: 4379.74, train acc: 0.8557, val loss: 2302.64, val acc: 0.7867, time: 114.43s\n",
            "Epoch:10, Training loss: 3953.69, train acc: 0.8697, val loss: 2167.67, val acc: 0.7947, time: 114.83s\n",
            "Epoch:11, Training loss: 3681.89, train acc: 0.8760, val loss: 2133.26, val acc: 0.7994, time: 115.01s\n",
            "Epoch:12, Training loss: 3287.84, train acc: 0.8856, val loss: 2094.94, val acc: 0.8015, time: 118.79s\n",
            "Epoch:13, Training loss: 3003.20, train acc: 0.8917, val loss: 2084.04, val acc: 0.8096, time: 118.26s\n",
            "Epoch:14, Training loss: 2736.74, train acc: 0.8979, val loss: 2066.08, val acc: 0.8100, time: 118.99s\n",
            "Epoch:15, Training loss: 2525.24, train acc: 0.9036, val loss: 2083.42, val acc: 0.8038, time: 117.64s\n",
            "Epoch:16, Training loss: 2268.76, train acc: 0.9079, val loss: 2137.45, val acc: 0.8159, time: 118.50s\n",
            "Epoch:17, Training loss: 2101.36, train acc: 0.9167, val loss: 2155.41, val acc: 0.8066, time: 118.59s\n",
            "Epoch:18, Training loss: 1952.47, train acc: 0.9276, val loss: 2168.06, val acc: 0.8146, time: 120.20s\n",
            "Epoch:19, Training loss: 1684.48, train acc: 0.9273, val loss: 2215.64, val acc: 0.8115, time: 118.90s\n",
            "Epoch:20, Training loss: 1541.21, train acc: 0.9364, val loss: 2255.27, val acc: 0.8121, time: 118.68s\n",
            "                     precision    recall  f1-score   support\n",
            "\n",
            "B-DocumentReference     0.1500    0.7500    0.2500         4\n",
            "         B-Location     0.6344    0.6310    0.6327       187\n",
            " B-MilitaryPlatform     0.0000    0.0000    0.0000         6\n",
            "            B-Money     0.2000    0.3333    0.2500         3\n",
            "      B-Nationality     0.1250    0.2500    0.1667         4\n",
            "     B-Organisation     0.6643    0.6863    0.6751       271\n",
            "           B-Person     0.7843    0.7921    0.7882       101\n",
            "         B-Quantity     0.8000    0.7333    0.7652        60\n",
            "         B-Temporal     0.7447    0.7000    0.7216        50\n",
            "           B-Weapon     0.1316    0.5000    0.2083        10\n",
            "I-DocumentReference     0.1205    0.8333    0.2105        12\n",
            "         I-Location     0.4528    0.6522    0.5345       184\n",
            " I-MilitaryPlatform     0.0625    0.5000    0.1111         2\n",
            "            I-Money     0.4000    0.8000    0.5333         5\n",
            "      I-Nationality     0.0000    0.0000    0.0000         0\n",
            "     I-Organisation     0.6635    0.6603    0.6619       418\n",
            "           I-Person     0.7457    0.8836    0.8088       146\n",
            "         I-Quantity     0.2692    0.3684    0.3111        19\n",
            "         I-Temporal     0.7742    0.7742    0.7742        62\n",
            "           I-Weapon     0.1111    1.0000    0.2000         5\n",
            "                  O     0.9377    0.8609    0.8977      3725\n",
            "\n",
            "           accuracy                         0.8115      5274\n",
            "          macro avg     0.4177    0.6052    0.4524      5274\n",
            "       weighted avg     0.8506    0.8115    0.8276      5274\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5av6F3Ht6jd"
      },
      "source": [
        "#### Scaled Dot Product"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSw-3-FNgRIu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7c1f31e-05ef-4d75-de95-e649f6643bab"
      },
      "source": [
        "ATTENTION_METHOD = \"Scaled Dot Product\"\n",
        "\n",
        "# initialize model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_Scaled_Dot_Product = BiLSTM_CRF(len(word_to_ix), tag_to_ix, p_EMBEDDING_DIM, HIDDEN_DIM).to(device)\n",
        "optimizer = optim.SGD(model_Scaled_Dot_Product.parameters(), lr=0.01, weight_decay=1e-4)\n",
        "\n",
        "#model performance dot product\n",
        "model_performance_attention(model_Scaled_Dot_Product,ATTENTION_METHOD)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:1, Training loss: 17534.20, train acc: 0.6999, val loss: 5936.52, val acc: 0.6485, time: 114.33s\n",
            "Epoch:2, Training loss: 12771.68, train acc: 0.7204, val loss: 4632.85, val acc: 0.6665, time: 114.55s\n",
            "Epoch:3, Training loss: 9545.10, train acc: 0.7491, val loss: 3878.73, val acc: 0.7067, time: 115.31s\n",
            "Epoch:4, Training loss: 8064.55, train acc: 0.7771, val loss: 3487.94, val acc: 0.7243, time: 116.27s\n",
            "Epoch:5, Training loss: 7123.36, train acc: 0.7945, val loss: 3201.57, val acc: 0.7387, time: 115.98s\n",
            "Epoch:6, Training loss: 6311.17, train acc: 0.8131, val loss: 2949.42, val acc: 0.7543, time: 116.11s\n",
            "Epoch:7, Training loss: 5529.25, train acc: 0.8276, val loss: 2750.50, val acc: 0.7645, time: 115.25s\n",
            "Epoch:8, Training loss: 4965.70, train acc: 0.8376, val loss: 2597.05, val acc: 0.7700, time: 115.85s\n",
            "Epoch:9, Training loss: 4477.49, train acc: 0.8478, val loss: 2398.49, val acc: 0.7812, time: 115.73s\n",
            "Epoch:10, Training loss: 4069.45, train acc: 0.8560, val loss: 2305.41, val acc: 0.7821, time: 116.29s\n",
            "Epoch:11, Training loss: 3735.68, train acc: 0.8612, val loss: 2284.58, val acc: 0.7892, time: 115.76s\n",
            "Epoch:12, Training loss: 3419.46, train acc: 0.8690, val loss: 2214.60, val acc: 0.7901, time: 115.88s\n",
            "Epoch:13, Training loss: 3176.26, train acc: 0.8808, val loss: 2130.95, val acc: 0.7988, time: 115.38s\n",
            "Epoch:14, Training loss: 2892.89, train acc: 0.8823, val loss: 2154.83, val acc: 0.7988, time: 116.28s\n",
            "Epoch:15, Training loss: 2676.17, train acc: 0.8923, val loss: 2149.48, val acc: 0.8038, time: 115.62s\n",
            "Epoch:16, Training loss: 2533.40, train acc: 0.9083, val loss: 2130.98, val acc: 0.8060, time: 116.23s\n",
            "Epoch:17, Training loss: 2284.37, train acc: 0.9081, val loss: 2130.67, val acc: 0.8041, time: 115.10s\n",
            "Epoch:18, Training loss: 2059.45, train acc: 0.9110, val loss: 2140.17, val acc: 0.8024, time: 114.90s\n",
            "Epoch:19, Training loss: 1961.50, train acc: 0.9159, val loss: 2207.43, val acc: 0.8000, time: 115.45s\n",
            "Epoch:20, Training loss: 1832.42, train acc: 0.9125, val loss: 2325.32, val acc: 0.7984, time: 116.44s\n",
            "                     precision    recall  f1-score   support\n",
            "\n",
            "B-DocumentReference     0.1500    0.5000    0.2308         6\n",
            "         B-Location     0.5376    0.6579    0.5917       152\n",
            " B-MilitaryPlatform     0.0000    0.0000    0.0000         1\n",
            "            B-Money     0.2000    0.3333    0.2500         3\n",
            "      B-Nationality     0.0000    0.0000    0.0000         0\n",
            "     B-Organisation     0.5857    0.7193    0.6457       228\n",
            "           B-Person     0.8922    0.7222    0.7982       126\n",
            "         B-Quantity     0.7455    0.6833    0.7130        60\n",
            "         B-Temporal     0.6383    0.6522    0.6452        46\n",
            "           B-Weapon     0.0526    0.3333    0.0909         6\n",
            "I-DocumentReference     0.1807    0.6522    0.2830        23\n",
            "         I-Location     0.3472    0.7023    0.4646       131\n",
            " I-MilitaryPlatform     0.0000    0.0000    0.0000         1\n",
            "            I-Money     0.4000    0.3636    0.3810        11\n",
            "      I-Nationality     0.0000    0.0000    0.0000         0\n",
            "     I-Organisation     0.5240    0.7543    0.6184       289\n",
            "           I-Person     0.7919    0.7569    0.7740       181\n",
            "         I-Quantity     0.1538    0.2000    0.1739        20\n",
            "         I-Temporal     0.7097    0.8627    0.7788        51\n",
            "           I-Weapon     0.0000    0.0000    0.0000         3\n",
            "                  O     0.9523    0.8275    0.8855      3936\n",
            "\n",
            "           accuracy                         0.7969      5274\n",
            "          macro avg     0.3744    0.4629    0.3964      5274\n",
            "       weighted avg     0.8608    0.7969    0.8214      5274\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyJQk1cJt6x2"
      },
      "source": [
        "#### Cosine Similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7k-UmjVsga9m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "375e779d-c45c-4363-828c-347bc8f025e3"
      },
      "source": [
        "\n",
        "ATTENTION_METHOD = \"Cosine Similarity\"\n",
        "\n",
        "# initialize model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_Cosine_Similarity = BiLSTM_CRF(len(word_to_ix), tag_to_ix, p_EMBEDDING_DIM, HIDDEN_DIM).to(device)\n",
        "optimizer = optim.SGD(model_Cosine_Similarity.parameters(), lr=0.01, weight_decay=1e-4)\n",
        "\n",
        "\n",
        "model_performance_attention(model_Cosine_Similarity, ATTENTION_METHOD)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:1, Training loss: 552502.34, train acc: 0.6546, val loss: 9736.20, val acc: 0.6166, time: 110.89s\n",
            "Epoch:2, Training loss: 17699.41, train acc: 0.6872, val loss: 5784.57, val acc: 0.6314, time: 110.67s\n",
            "Epoch:3, Training loss: 13730.71, train acc: 0.6781, val loss: 5393.62, val acc: 0.6333, time: 111.36s\n",
            "Epoch:4, Training loss: 12815.46, train acc: 0.6934, val loss: 5384.46, val acc: 0.6458, time: 110.78s\n",
            "Epoch:5, Training loss: 11776.24, train acc: 0.6885, val loss: 4544.03, val acc: 0.6543, time: 111.01s\n",
            "Epoch:6, Training loss: 11064.05, train acc: 0.6923, val loss: 4334.23, val acc: 0.6371, time: 111.10s\n",
            "Epoch:7, Training loss: 10456.40, train acc: 0.6961, val loss: 4334.06, val acc: 0.6454, time: 110.64s\n",
            "Epoch:8, Training loss: 10090.16, train acc: 0.6953, val loss: 3977.95, val acc: 0.6439, time: 111.34s\n",
            "Epoch:9, Training loss: 9765.88, train acc: 0.7039, val loss: 3906.54, val acc: 0.6481, time: 111.24s\n",
            "Epoch:10, Training loss: 9624.21, train acc: 0.6835, val loss: 3994.84, val acc: 0.6358, time: 111.30s\n",
            "Epoch:11, Training loss: 9262.59, train acc: 0.7007, val loss: 3606.13, val acc: 0.6475, time: 110.93s\n",
            "Epoch:12, Training loss: 9172.71, train acc: 0.6997, val loss: 3988.80, val acc: 0.6524, time: 110.84s\n",
            "Epoch:13, Training loss: 9377.78, train acc: 0.6987, val loss: 3703.71, val acc: 0.6460, time: 111.97s\n",
            "Epoch:14, Training loss: 9257.64, train acc: 0.7014, val loss: 3484.50, val acc: 0.6597, time: 110.89s\n",
            "Epoch:15, Training loss: 8786.92, train acc: 0.7026, val loss: 3587.17, val acc: 0.6523, time: 111.60s\n",
            "Epoch:16, Training loss: 8595.50, train acc: 0.7054, val loss: 3482.36, val acc: 0.6578, time: 111.70s\n",
            "Epoch:17, Training loss: 8519.25, train acc: 0.7104, val loss: 3408.43, val acc: 0.6623, time: 110.71s\n",
            "Epoch:18, Training loss: 8393.43, train acc: 0.7102, val loss: 3314.72, val acc: 0.6633, time: 113.88s\n",
            "Epoch:19, Training loss: 8235.67, train acc: 0.7029, val loss: 3326.44, val acc: 0.6519, time: 111.11s\n",
            "Epoch:20, Training loss: 8141.43, train acc: 0.7190, val loss: 3273.79, val acc: 0.6629, time: 110.63s\n",
            "                     precision    recall  f1-score   support\n",
            "\n",
            "B-DocumentReference     0.0000    0.0000    0.0000         0\n",
            "         B-Location     0.2688    0.3759    0.3135       133\n",
            " B-MilitaryPlatform     0.0000    0.0000    0.0000         0\n",
            "            B-Money     0.0000    0.0000    0.0000         0\n",
            "      B-Nationality     0.0000    0.0000    0.0000         0\n",
            "     B-Organisation     0.0714    0.3390    0.1180        59\n",
            "           B-Person     0.2157    0.4490    0.2914        49\n",
            "         B-Quantity     0.0182    1.0000    0.0357         1\n",
            "         B-Temporal     0.0000    0.0000    0.0000         2\n",
            "           B-Weapon     0.0000    0.0000    0.0000         0\n",
            "I-DocumentReference     0.0000    0.0000    0.0000         0\n",
            "         I-Location     0.1660    0.1667    0.1664       264\n",
            " I-MilitaryPlatform     0.0000    0.0000    0.0000         0\n",
            "            I-Money     0.0000    0.0000    0.0000         0\n",
            "      I-Nationality     0.0000    0.0000    0.0000         0\n",
            "     I-Organisation     0.1082    0.2500    0.1510       180\n",
            "           I-Person     0.3873    0.6262    0.4786       107\n",
            "         I-Quantity     0.0000    0.0000    0.0000         0\n",
            "         I-Temporal     0.0000    0.0000    0.0000         6\n",
            "           I-Weapon     0.0000    0.0000    0.0000         0\n",
            "                  O     0.9515    0.7275    0.8245      4473\n",
            "\n",
            "           accuracy                         0.6642      5274\n",
            "          macro avg     0.1041    0.1873    0.1133      5274\n",
            "       weighted avg     0.8364    0.6642    0.7344      5274\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-bwaiTx57fv"
      },
      "source": [
        "#### Discussion\n",
        "Which one is the optimal?\n",
        "\n",
        "In this section, we test 3 models that set different attention score calculation methods: ***dot product, scaled dot product, and cosine similarity, using Glove-twitter-25 incorporating POS and Dependency information as input embedding. \n",
        "\n",
        "The optimal  attention score calculation methods is dot product, which get 0.8115 accuracy, which is higher than others."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7Y5GdIHqbKO"
      },
      "source": [
        "## 7 CRF Attachment\n",
        "NOTE: You should test your NER model with CRF/ without CRF."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnWIscKpfhgw"
      },
      "source": [
        "Use Glove-twitter-50 as baseline input embedding.\\\n",
        "Set the optimal number of stacked layers = 1\\\n",
        "Setting BiLSTM model without CRF \\"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TkVylN2qXlH"
      },
      "source": [
        "# Stacked Attentional Bi-LSTM without CRF\n",
        "p_EMBEDDING_DIM = EMBEDDING_DIM_50\n",
        "p_embedding_matrix = embedding_matrix_50\n",
        "\n",
        "def cal_acc(model, input_index, val_POS_index ,val_DEP_index, output_index):\n",
        "    ground_truth = []\n",
        "    predicted = []\n",
        "    truth = 0\n",
        "    for i,idxs in enumerate(input_index):\n",
        "        ground_truth += output_index[i]\n",
        "        pos_idxs = val_POS_index[i]\n",
        "        dep_idxs = val_DEP_index[i]\n",
        "\n",
        "        postaglist_in = torch.tensor(pos_idxs, dtype=torch.long).to(device)\n",
        "        deptaglist_in = torch.tensor(dep_idxs, dtype=torch.long).to(device)\n",
        "        sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "\n",
        "        _, pred = model(sentence_in,postaglist_in,deptaglist_in)\n",
        "        predicted += list(pred.cpu().numpy())\n",
        "        \n",
        "    accuracy = sum(np.array(ground_truth) == np.array(predicted))/len(ground_truth)\n",
        "    return predicted, ground_truth, accuracy\n",
        "\n",
        "class BiLSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):\n",
        "        super(BiLSTM, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "        self.tagset_size = len(tag_to_ix)\n",
        "\n",
        "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
        "        \"\"\"Here we use the embedding matrix as the initial weights of nn.Embedding\"\"\"\n",
        "        self.word_embeds.weight.data.copy_(torch.from_numpy(p_embedding_matrix))\n",
        "\n",
        "        self.postag_embeds = nn.Embedding(embedding_matrix_POS.shape[0], EMBEDDING_DIM_POS)\n",
        "        self.postag_embeds.weight.data.copy_(torch.from_numpy(embedding_matrix_POS))\n",
        "\n",
        "        self.deptag_embeds = nn.Embedding(embedding_matrix_DEP.shape[0], EMBEDDING_DIM_DEP)\n",
        "        self.deptag_embeds.weight.data.copy_(torch.from_numpy(embedding_matrix_DEP))\n",
        "        \n",
        "        self.lstm = nn.LSTM(embedding_dim+EMBEDDING_DIM_POS+EMBEDDING_DIM_DEP, hidden_dim // 2,\n",
        "                            num_layers=1, bidirectional=True)\n",
        "\n",
        "        # Maps the output of the LSTM into tag space.\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
        "\n",
        "        # Matrix of transition parameters.  Entry i,j is the score of\n",
        "        # transitioning *to* i *from* j.\n",
        "        self.transitions = nn.Parameter(\n",
        "            torch.randn(self.tagset_size, self.tagset_size))\n",
        "\n",
        "        # These two statements enforce the constraint that we never transfer\n",
        "        # to the start tag and we never transfer from the stop tag\n",
        "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
        "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
        "\n",
        "        self.hidden = self.init_hidden()\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return (torch.randn(2, 1, self.hidden_dim // 2).to(device),\n",
        "                torch.randn(2, 1, self.hidden_dim // 2).to(device))\n",
        "\n",
        "    def _get_lstm_features(self, sentence, postaglist, deptaglist):\n",
        "        self.hidden = self.init_hidden()\n",
        "        wordembeds = self.word_embeds(sentence)\n",
        "        postagembeds = self.postag_embeds(postaglist)\n",
        "        deptagembeds = self.deptag_embeds(deptaglist)\n",
        "        embeds = torch.cat((wordembeds, postagembeds, deptagembeds), 1).view(len(sentence), 1, -1)\n",
        "        \n",
        "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
        "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
        "        lstm_feats = self.hidden2tag(lstm_out)\n",
        "        return lstm_feats\n",
        "\n",
        "    def forward(self, sentence, postaglist,deptaglist):  # dont confuse this with _forward_alg above.\n",
        "        # Get the emission scores from the BiLSTM\n",
        "        lstm_feats = self._get_lstm_features(sentence,postaglist,deptaglist)\n",
        "        return lstm_feats,torch.argmax(lstm_feats,dim=-1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELnJ9GZVo5Dq"
      },
      "source": [
        "# initialize model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = BiLSTM(len(word_to_ix), tag_to_ix, p_EMBEDDING_DIM, HIDDEN_DIM).to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbulsFLro7kn",
        "outputId": "efcc3fa1-39b5-45af-84c5-74b88b88bce1"
      },
      "source": [
        "\"\"\"Each epoch will take about 2-3 minutes\"\"\"\n",
        "\n",
        "import datetime\n",
        "\n",
        "for epoch in range(max_epoch):  \n",
        "    time1 = datetime.datetime.now()\n",
        "    train_loss = 0\n",
        "\n",
        "    model.train()\n",
        "    for i, idxs in enumerate(train_input_index):\n",
        "        tags_index = train_output_index[i]\n",
        "        pos_idxs = train_POS_index[i]\n",
        "        dep_idxs = train_DEP_index[i]\n",
        "\n",
        "        # Step 1. Remember that Pytorch accumulates gradients.\n",
        "        # We need to clear them out before each instance\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Step 2. Get our inputs ready for the network, that is,\n",
        "        # turn them into Tensors of word indices.\n",
        "        sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "        postaglist_in = torch.tensor(pos_idxs, dtype=torch.long).to(device)\n",
        "        deptaglist_in = torch.tensor(dep_idxs, dtype=torch.long).to(device)\n",
        "\n",
        "        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "        #print(targets.shape)\n",
        "        # Step 3. Run our forward pass.\n",
        "        pred, _ = model(sentence_in,postaglist_in,deptaglist_in)\n",
        "        #print(pred.shape)\n",
        "        loss = criterion(pred,targets)\n",
        "\n",
        "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "        # calling optimizer.step()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss+=loss.item()\n",
        "\n",
        "    model.eval()\n",
        "    _, _, train_acc = cal_acc(model,train_input_index,train_POS_index,train_DEP_index,train_output_index)\n",
        "    _, _, val_acc = cal_acc(model,val_input_index,val_POS_index,val_DEP_index,val_output_index)\n",
        "\n",
        "    val_loss = 0\n",
        "    for i, idxs in enumerate(val_input_index):\n",
        "        tags_index = val_output_index[i]\n",
        "        pos_idxs = val_POS_index[i]\n",
        "        dep_idxs = val_DEP_index[i]\n",
        "        sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "        postaglist_in = torch.tensor(pos_idxs, dtype=torch.long).to(device)\n",
        "        deptaglist_in = torch.tensor(dep_idxs, dtype=torch.long).to(device)\n",
        "        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "        pred,_ = model(sentence_in,postaglist_in,deptaglist_in)\n",
        "        loss = criterion(pred,targets)\n",
        "        val_loss+=loss.item()\n",
        "    time2 = datetime.datetime.now()\n",
        "\n",
        "    print(\"Epoch:%d, Training loss: %.2f, train acc: %.4f, val loss: %.2f, val acc: %.4f, time: %.2fs\" %(epoch+1, train_loss,train_acc, val_loss, val_acc, (time2-time1).total_seconds()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:1, Training loss: 913.49, train acc: 0.6998, val loss: 281.71, val acc: 0.6485, time: 9.42s\n",
            "Epoch:2, Training loss: 736.66, train acc: 0.6999, val loss: 264.97, val acc: 0.6485, time: 10.20s\n",
            "Epoch:3, Training loss: 692.16, train acc: 0.6999, val loss: 253.58, val acc: 0.6485, time: 11.34s\n",
            "Epoch:4, Training loss: 660.24, train acc: 0.7010, val loss: 244.82, val acc: 0.6492, time: 9.87s\n",
            "Epoch:5, Training loss: 633.34, train acc: 0.7068, val loss: 237.61, val acc: 0.6530, time: 9.37s\n",
            "Epoch:6, Training loss: 611.03, train acc: 0.7135, val loss: 231.65, val acc: 0.6585, time: 9.18s\n",
            "Epoch:7, Training loss: 589.94, train acc: 0.7189, val loss: 225.96, val acc: 0.6642, time: 9.24s\n",
            "Epoch:8, Training loss: 574.24, train acc: 0.7254, val loss: 221.33, val acc: 0.6706, time: 9.56s\n",
            "Epoch:9, Training loss: 558.52, train acc: 0.7307, val loss: 216.74, val acc: 0.6786, time: 9.19s\n",
            "Epoch:10, Training loss: 548.77, train acc: 0.7350, val loss: 213.60, val acc: 0.6824, time: 9.48s\n",
            "Epoch:11, Training loss: 534.23, train acc: 0.7394, val loss: 208.28, val acc: 0.6877, time: 9.37s\n",
            "Epoch:12, Training loss: 517.42, train acc: 0.7454, val loss: 205.15, val acc: 0.6934, time: 9.27s\n",
            "Epoch:13, Training loss: 506.65, train acc: 0.7507, val loss: 199.73, val acc: 0.7006, time: 8.97s\n",
            "Epoch:14, Training loss: 491.24, train acc: 0.7589, val loss: 195.78, val acc: 0.7046, time: 9.10s\n",
            "Epoch:15, Training loss: 478.18, train acc: 0.7642, val loss: 193.23, val acc: 0.7089, time: 8.83s\n",
            "Epoch:16, Training loss: 468.45, train acc: 0.7698, val loss: 190.48, val acc: 0.7139, time: 8.97s\n",
            "Epoch:17, Training loss: 455.45, train acc: 0.7760, val loss: 185.92, val acc: 0.7158, time: 9.31s\n",
            "Epoch:18, Training loss: 445.84, train acc: 0.7820, val loss: 183.89, val acc: 0.7209, time: 9.43s\n",
            "Epoch:19, Training loss: 439.16, train acc: 0.7839, val loss: 180.45, val acc: 0.7241, time: 9.68s\n",
            "Epoch:20, Training loss: 428.21, train acc: 0.7898, val loss: 180.13, val acc: 0.7287, time: 9.31s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56qKg6zTo-X0",
        "outputId": "e00e6d3b-894c-44de-dd46-a8f22ea73a89"
      },
      "source": [
        "y_true,y_pred,_ = cal_acc(model,val_input_index,val_POS_index,val_DEP_index,val_output_index)\n",
        "y_true_decode = decode_output(y_true)\n",
        "y_pred_decode = decode_output(y_pred)\n",
        "print(y_true_decode)\n",
        "print(y_pred_decode)\n",
        "print(classification_report(y_true_decode,y_pred_decode,digits=4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['O', 'I-Person', 'O', 'O', 'O', 'B-Person', 'B-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Person', 'I-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'I-Location', 'O', 'O', 'O', 'O', 'I-Person', 'I-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'O', 'B-Organisation', 'I-Person', 'I-Organisation', 'O', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'B-Location', 'O', 'B-Location', 'O', 'I-Location', 'B-Location', 'O', 'O', 'I-Location', 'O', 'O', 'I-Person', 'O', 'O', 'B-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'I-Person', 'I-Person', 'O', 'B-Organisation', 'I-Organisation', 'O', 'I-Organisation', 'O', 'O', 'I-Organisation', 'O', 'O', 'O', 'O', 'I-Organisation', 'O', 'O', 'I-Organisation', 'O', 'O', 'O', 'O', 'B-Location', 'O', 'B-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'I-Location', 'I-Organisation', 'O', 'I-Location', 'O', 'B-Location', 'I-Location', 'O', 'I-Location', 'B-Location', 'O', 'O', 'B-Location', 'I-Location', 'O', 'B-Location', 'O', 'B-Person', 'I-Person', 'I-Organisation', 'O', 'I-Person', 'I-Organisation', 'O', 'I-Organisation', 'O', 'I-Person', 'O', 'O', 'I-Organisation', 'I-Person', 'I-Person', 'I-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'I-Location', 'O', 'B-Organisation', 'I-Location', 'O', 'O', 'O', 'B-Location', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Person', 'I-Person', 'O', 'I-Person', 'I-Person', 'O', 'O', 'O', 'I-Organisation', 'O', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'B-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'O', 'O', 'I-Organisation', 'I-Organisation', 'O', 'O', 'I-Organisation', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Location', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Person', 'O', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'O', 'O', 'I-Location', 'O', 'I-Location', 'O', 'O', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Person', 'O', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Location', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'B-Location', 'B-Location', 'I-Location', 'I-Location', 'O', 'O', 'I-Location', 'I-Organisation', 'O', 'O', 'O', 'O', 'I-Location', 'B-Location', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'O', 'B-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'O', 'O', 'O', 'O', 'I-Organisation', 'I-Organisation', 'O', 'I-Organisation', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'O', 'B-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'O', 'O', 'O', 'O', 'B-Location', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'O', 'O', 'O', 'O', 'I-Person', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'B-Location', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'B-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Person', 'I-Person', 'O', 'O', 'I-Person', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'I-Organisation', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'I-Location', 'I-Location', 'O', 'B-Location', 'O', 'B-Location', 'B-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'I-Organisation', 'O', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'O', 'B-Person', 'I-Person', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'I-Organisation', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Location', 'I-Location', 'O', 'O', 'O', 'B-Location', 'O', 'O', 'B-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'I-Organisation', 'I-Organisation', 'O', 'O', 'I-Person', 'I-Person', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'O', 'O', 'O', 'I-Organisation', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'I-Person', 'O', 'O', 'O', 'I-Organisation', 'O', 'O', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'I-Organisation', 'O', 'O', 'O', 'B-Location', 'O', 'O', 'O', 'O', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'I-Location', 'O', 'O', 'O', 'I-Location', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'O', 'O', 'O', 'B-Location', 'O', 'O', 'O', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'I-Person', 'I-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'O', 'O', 'I-Person', 'O', 'O', 'O', 'O', 'I-Location', 'O', 'I-Location', 'O', 'O', 'B-Location', 'O', 'O', 'B-Location', 'B-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'O', 'O', 'O', 'O', 'I-Organisation', 'O', 'O', 'B-Location', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Person', 'O', 'O', 'I-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'O', 'B-Location', 'O', 'O', 'O', 'B-Person', 'O', 'O', 'O', 'B-Organisation', 'B-Organisation', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'B-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'B-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'I-Organisation', 'O', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'I-Organisation', 'O', 'I-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Location', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Location', 'I-Location', 'O', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'O', 'O', 'O', 'B-Location', 'O', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'B-Location', 'O', 'O', 'B-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'O', 'O', 'O', 'B-Organisation', 'B-Location', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Location', 'O', 'I-Location', 'O', 'O', 'I-Organisation', 'O', 'O', 'O', 'O', 'B-Location', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Person', 'O', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Location', 'O', 'O', 'O', 'I-Location', 'I-Location', 'O', 'B-Organisation', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'I-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'O', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'I-Location', 'I-Person', 'O', 'O', 'O', 'O', 'O', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'I-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Location', 'O', 'B-Location', 'I-Location', 'I-Location', 'O', 'O', 'O', 'I-Location', 'O', 'B-Location', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'I-Location', 'O', 'I-Location', 'I-Location', 'O', 'B-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Location', 'O', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'B-Location', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Location', 'O', 'O', 'O', 'O', 'I-Location', 'O', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'I-Person', 'I-Person', 'I-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'I-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'I-Organisation', 'O', 'O', 'B-Location', 'I-Location', 'I-Location', 'O', 'B-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'O', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'O', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Person', 'O', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'O', 'O', 'O', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'B-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'O', 'O', 'B-Organisation', 'I-Location', 'O', 'O', 'O', 'B-Location', 'B-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'I-Person', 'I-Person', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Person', 'O', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'O', 'O', 'B-Person', 'O', 'B-Person', 'I-Person', 'I-Location', 'I-Person', 'I-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'B-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'O', 'B-Location', 'O', 'O', 'B-Location', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Person', 'I-Organisation', 'O', 'O', 'O', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'O', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'B-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'O', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'I-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'B-Location', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'I-Organisation', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'O', 'B-Person', 'O', 'O', 'O', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'O', 'I-Organisation', 'O', 'O', 'O', 'I-Organisation', 'I-Organisation', 'O', 'O', 'B-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'I-Organisation', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'I-Person', 'I-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'O', 'O', 'B-Location', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Location', 'O', 'O', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'I-Organisation', 'O', 'O', 'O', 'O', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'O', 'O', 'O', 'O', 'I-Location', 'I-Location', 'O', 'I-Location', 'O', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Location', 'O', 'O', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'O', 'B-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Person', 'O', 'O', 'O', 'B-Person', 'I-Person', 'I-Person', 'I-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'O', 'B-Organisation', 'O', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'I-Organisation', 'O', 'O', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'O', 'O', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Person', 'O', 'O', 'O', 'O', 'I-Organisation', 'O', 'O', 'O', 'I-Organisation', 'I-Organisation', 'O', 'I-Person', 'I-Person', 'O', 'I-Person', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'I-Person', 'I-Person', 'O', 'O', 'O', 'O', 'I-Organisation', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'O', 'O', 'O', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'O', 'I-Person', 'I-Person', 'I-Person', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'I-Location', 'I-Organisation', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'B-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'I-Organisation', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'O', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'O', 'I-Person', 'O', 'B-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'B-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Location', 'I-Organisation', 'O', 'O', 'B-Location', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'B-Location', 'O', 'B-Location', 'O', 'O', 'O', 'O', 'B-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Person', 'I-Person', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'I-Organisation', 'O', 'O', 'B-Location', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'O', 'O', 'B-Location', 'O', 'O', 'I-Person', 'O', 'O', 'O', 'O', 'I-Person', 'I-Person', 'I-Person', 'O', 'O', 'I-Organisation', 'O', 'O', 'O', 'O', 'I-Person', 'I-Person', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Location', 'O', 'O', 'B-Organisation', 'I-Location', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'I-Location', 'I-Person', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'O', 'O', 'O', 'I-Organisation', 'O', 'I-Organisation', 'O', 'I-Location', 'O', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'B-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'I-Location', 'O', 'B-Location', 'I-Location', 'I-Location', 'O', 'B-Location', 'O', 'I-Location', 'O', 'O', 'B-Location', 'I-Location', 'O', 'O', 'I-Location', 'O', 'B-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'O', 'O', 'O', 'O', 'B-Location', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'I-Location', 'O', 'O', 'I-Location', 'O', 'B-Location', 'O', 'O', 'B-Location', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Person', 'O', 'I-Person', 'O', 'I-Person', 'I-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'O', 'O', 'O', 'B-Person', 'B-Person', 'I-Person', 'I-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'O', 'I-Organisation', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'B-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Person', 'I-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Location', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'O', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'O', 'I-Organisation', 'O', 'O', 'I-Organisation', 'I-Organisation', 'O', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'O', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'O', 'B-Person', 'O', 'B-Person', 'I-Person', 'I-Location', 'I-Person', 'I-Person', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'I-Location', 'I-Location', 'O', 'I-Location', 'O', 'B-Location', 'O', 'I-Location', 'O', 'B-Location', 'O', 'O', 'B-Location', 'I-Location', 'O', 'O', 'O', 'O', 'B-Location', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'I-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'I-Person', 'I-Person', 'I-Person', 'O', 'O', 'O', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'O', 'O', 'O', 'B-Location', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'B-Location', 'O', 'I-Organisation', 'I-Organisation', 'O', 'B-Location', 'O', 'O', 'O', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Location', 'O', 'O', 'O', 'O', 'B-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'O', 'I-Person', 'I-Person', 'O', 'O', 'B-Person', 'O', 'O', 'O', 'O', 'B-Person', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Person', 'I-Person', 'I-Person', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'O', 'I-Location', 'O', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'B-Organisation', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'B-Location', 'O', 'O', 'O', 'O', 'B-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'B-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'O', 'O', 'B-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Location', 'O', 'B-Location', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'O', 'O', 'B-Location', 'O', 'I-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'O', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'O', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'B-Person', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'O', 'I-Organisation', 'O', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'O', 'O', 'O', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'O', 'O', 'O', 'I-Person', 'O', 'I-Organisation', 'O', 'O', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'I-Location', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Location', 'O', 'I-Location', 'O', 'I-Location', 'B-Location', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'O', 'B-Organisation', 'B-Organisation', 'O', 'O', 'O', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'O', 'O', 'O', 'I-Person', 'O', 'I-Organisation', 'I-Organisation', 'O', 'I-Organisation', 'O', 'B-Person', 'I-Person', 'I-Person', 'I-Person', 'O', 'I-Person', 'O', 'I-Person', 'O', 'O', 'B-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'O', 'O']\n",
            "['O', 'B-Quantity', 'I-Quantity', 'I-Quantity', 'I-Quantity', 'I-Quantity', 'B-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'I-Location', 'O', 'B-Quantity', 'B-Weapon', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'B-Location', 'I-Location', 'I-Location', 'I-Location', 'I-Location', 'O', 'B-Person', 'I-Person', 'O', 'O', 'B-Weapon', 'I-Weapon', 'O', 'O', 'B-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'I-Person', 'I-Person', 'I-Person', 'O', 'O', 'B-DocumentReference', 'I-DocumentReference', 'I-DocumentReference', 'I-DocumentReference', 'I-DocumentReference', 'I-DocumentReference', 'I-DocumentReference', 'I-DocumentReference', 'I-DocumentReference', 'I-DocumentReference', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'I-Location', 'I-Location', 'I-Location', 'I-Location', 'I-Location', 'I-Location', 'O', 'O', 'B-Temporal', 'I-Temporal', 'O', 'B-Person', 'O', 'O', 'B-Person', 'O', 'B-Person', 'I-Person', 'O', 'O', 'O', 'B-Person', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'I-Person', 'I-Person', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'B-Location', 'O', 'O', 'B-Organisation', 'O', 'O', 'B-Location', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'O', 'B-Temporal', 'I-Temporal', 'I-Temporal', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'B-Location', 'O', 'B-Location', 'I-Location', 'I-Location', 'I-Location', 'I-Location', 'O', 'O', 'B-Location', 'I-Location', 'I-Location', 'I-Location', 'O', 'B-Person', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'O', 'O', 'B-Location', 'I-Location', 'O', 'B-Location', 'I-Location', 'O', 'B-Location', 'I-Location', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'I-Person', 'O', 'B-Person', 'I-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Temporal', 'O', 'O', 'B-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Quantity', 'I-Quantity', 'I-Quantity', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Temporal', 'I-Temporal', 'I-Temporal', 'I-Temporal', 'I-Temporal', 'I-Temporal', 'O', 'O', 'B-Location', 'I-Location', 'I-Location', 'I-Location', 'I-Location', 'I-Location', 'I-Location', 'I-Location', 'I-Location', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'B-Organisation', 'O', 'O', 'O', 'B-Person', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'B-Location', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'I-Person', 'I-Person', 'I-Person', 'O', 'O', 'B-Location', 'I-Location', 'I-Location', 'O', 'O', 'B-Temporal', 'I-Temporal', 'O', 'O', 'B-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'B-Person', 'I-Person', 'O', 'B-Temporal', 'I-Temporal', 'O', 'B-Nationality', 'O', 'O', 'B-Location', 'I-Location', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'B-Temporal', 'I-Temporal', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'B-Location', 'I-Location', 'I-Location', 'I-Location', 'I-Location', 'O', 'B-Organisation', 'I-Organisation', 'O', 'B-Location', 'I-Location', 'I-Location', 'I-Location', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'O', 'B-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'O', 'B-Location', 'O', 'O', 'B-Temporal', 'I-Temporal', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Weapon', 'I-Weapon', 'O', 'B-Weapon', 'I-Weapon', 'I-Weapon', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'B-Location', 'I-Location', 'I-Location', 'I-Location', 'I-Location', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'O', 'B-Person', 'I-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'I-Location', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'I-Location', 'I-Location', 'I-Location', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MilitaryPlatform', 'I-MilitaryPlatform', 'I-MilitaryPlatform', 'I-MilitaryPlatform', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'I-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'O', 'B-Weapon', 'O', 'O', 'B-Location', 'I-Location', 'I-Location', 'O', 'B-Location', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'B-DocumentReference', 'I-DocumentReference', 'I-DocumentReference', 'I-DocumentReference', 'I-DocumentReference', 'I-DocumentReference', 'I-DocumentReference', 'I-DocumentReference', 'I-DocumentReference', 'I-DocumentReference', 'I-DocumentReference', 'I-DocumentReference', 'I-DocumentReference', 'I-DocumentReference', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'O', 'B-Person', 'I-Person', 'I-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'B-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'B-Person', 'I-Person', 'I-Person', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'B-Quantity', 'I-Quantity', 'I-Quantity', 'I-Quantity', 'O', 'B-Location', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'I-Location', 'I-Location', 'I-Location', 'I-Location', 'I-Location', 'O', 'O', 'B-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'B-Location', 'I-Location', 'I-Location', 'O', 'O', 'O', 'O', 'B-Temporal', 'I-Temporal', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'B-Person', 'I-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Weapon', 'O', 'B-Money', 'O', 'O', 'O', 'B-Location', 'I-Location', 'I-Location', 'I-Location', 'I-Location', 'I-Location', 'I-Location', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'O', 'B-Location', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'O', 'B-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'O', 'B-Organisation', 'O', 'O', 'O', 'B-Location', 'I-Location', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'B-Location', 'I-Location', 'O', 'O', 'O', 'B-Location', 'O', 'B-Quantity', 'O', 'O', 'B-Quantity', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'B-Quantity', 'B-Location', 'I-Location', 'O', 'B-Quantity', 'O', 'O', 'B-Location', 'I-Location', 'I-Location', 'I-Location', 'I-Location', 'O', 'O', 'B-Weapon', 'I-Weapon', 'I-Weapon', 'I-Weapon', 'I-Weapon', 'I-Weapon', 'O', 'O', 'O', 'B-Quantity', 'O', 'O', 'O', 'B-Quantity', 'B-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'I-Person', 'I-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'O', 'B-Quantity', 'O', 'O', 'B-Location', 'I-Location', 'I-Location', 'O', 'O', 'B-Temporal', 'I-Temporal', 'O', 'B-Location', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Weapon', 'I-Weapon', 'I-Weapon', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'B-Temporal', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DocumentReference', 'I-DocumentReference', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'B-Location', 'O', 'B-Location', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'I-Person', 'O', 'O', 'B-Person', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'I-Location', 'I-Location', 'I-Location', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'I-Location', 'I-Location', 'I-Location', 'I-Location', 'I-Location', 'I-Location', 'I-Location', 'I-Location', 'I-Location', 'O', 'B-Location', 'I-Location', 'I-Location', 'I-Location', 'I-Location', 'I-Location', 'I-Location', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'B-DocumentReference', 'I-DocumentReference', 'I-DocumentReference', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'B-Location', 'I-Location', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'O', 'B-Temporal', 'I-Temporal', 'O', 'O', 'B-Person', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'B-Location', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DocumentReference', 'I-DocumentReference', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'B-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'B-Weapon', 'I-Weapon', 'I-Weapon', 'I-Weapon', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'B-Quantity', 'I-Quantity', 'I-Quantity', 'I-Quantity', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Temporal', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'B-Organisation', 'O', 'O', 'B-Organisation', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'B-Temporal', 'I-Temporal', 'I-Temporal', 'I-Temporal', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'B-Person', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'I-Location', 'I-Location', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'O', 'B-Location', 'O', 'B-Temporal', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'O', 'O', 'B-Location', 'O', 'B-Location', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'O', 'O', 'O', 'B-DocumentReference', 'I-DocumentReference', 'I-DocumentReference', 'I-DocumentReference', 'I-DocumentReference', 'I-DocumentReference', 'I-DocumentReference', 'I-DocumentReference', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'I-Location', 'I-Location', 'I-Location', 'O', 'O', 'B-Organisation', 'O', 'O', 'B-Location', 'I-Location', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'O', 'O', 'O', 'O', 'B-Person', 'O', 'O', 'O', 'O', 'B-Location', 'I-Location', 'O', 'O', 'O', 'O', 'B-Location', 'I-Location', 'I-Location', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Quantity', 'I-Quantity', 'I-Quantity', 'I-Quantity', 'I-Quantity', 'I-Quantity', 'O', 'B-Location', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Quantity', 'I-Quantity', 'I-Quantity', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'B-Temporal', 'I-Temporal', 'I-Temporal', 'I-Temporal', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'B-Temporal', 'I-Temporal', 'I-Temporal', 'I-Temporal', 'I-Temporal', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'B-Temporal', 'O', 'O', 'O', 'O', 'B-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'I-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Weapon', 'I-Weapon', 'O', 'B-Organisation', 'O', 'B-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'B-Location', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'I-Person', 'I-Person', 'O', 'B-Organisation', 'O', 'O', 'O', 'B-Temporal', 'B-Organisation', 'O', 'O', 'O', 'O', 'B-Weapon', 'O', 'O', 'B-Location', 'I-Location', 'I-Location', 'I-Location', 'O', 'B-Quantity', 'O', 'B-Location', 'O', 'B-Location', 'I-Location', 'I-Location', 'O', 'B-Quantity', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'B-Quantity', 'I-Quantity', 'I-Quantity', 'I-Quantity', 'B-Location', 'I-Location', 'I-Location', 'I-Location', 'I-Location', 'I-Location', 'O', 'O', 'O', 'B-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'I-Location', 'I-Location', 'I-Location', 'I-Location', 'I-Location', 'I-Location', 'I-Location', 'O', 'B-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DocumentReference', 'I-DocumentReference', 'I-DocumentReference', 'I-DocumentReference', 'I-DocumentReference', 'I-DocumentReference', 'I-DocumentReference', 'I-DocumentReference', 'O', 'O', 'O', 'B-Location', 'I-Location', 'I-Location', 'O', 'B-Quantity', 'O', 'O', 'B-Quantity', 'O', 'O', 'O', 'O', 'B-Quantity', 'O', 'O', 'O', 'O', 'B-Quantity', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DocumentReference', 'I-DocumentReference', 'O', 'O', 'O', 'O', 'B-Weapon', 'I-Weapon', 'I-Weapon', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Weapon', 'I-Weapon', 'I-Weapon', 'I-Weapon', 'O', 'O', 'O', 'O', 'O', 'B-Temporal', 'I-Temporal', 'I-Temporal', 'I-Temporal', 'O', 'B-Person', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DocumentReference', 'I-DocumentReference', 'I-DocumentReference', 'I-DocumentReference', 'I-DocumentReference', 'I-DocumentReference', 'I-DocumentReference', 'I-DocumentReference', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Temporal', 'I-Temporal', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'B-Money', 'I-Money', 'I-Money', 'I-Money', 'I-Money', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'O', 'B-Organisation', 'O', 'B-Organisation', 'O', 'B-Organisation', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'B-Location', 'O', 'B-Location', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'B-Temporal', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'B-Quantity', 'I-Quantity', 'I-Quantity', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'O', 'B-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DocumentReference', 'I-DocumentReference', 'I-DocumentReference', 'I-DocumentReference', 'I-DocumentReference', 'O', 'O', 'O', 'O', 'B-DocumentReference', 'I-DocumentReference', 'I-DocumentReference', 'I-DocumentReference', 'I-DocumentReference', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'O', 'O', 'B-Location', 'I-Location', 'I-Location', 'O', 'B-Money', 'I-Money', 'I-Money', 'I-Money', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'B-Organisation', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'B-Person', 'I-Person', 'I-Person', 'O', 'B-Person', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'B-Person', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'O', 'B-Organisation', 'O', 'O', 'B-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'O', 'B-Person', 'O', 'O', 'O', 'B-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'B-Location', 'I-Location', 'I-Location', 'I-Location', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'B-Weapon', 'I-Weapon', 'I-Weapon', 'I-Weapon', 'I-Weapon', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'I-Location', 'I-Location', 'I-Location', 'I-Location', 'I-Location', 'O', 'O', 'B-Location', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'B-Quantity', 'B-Weapon', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'B-MilitaryPlatform', 'O', 'O', 'O', 'O', 'B-MilitaryPlatform', 'I-MilitaryPlatform', 'I-MilitaryPlatform', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'B-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'B-Temporal', 'O', 'B-DocumentReference', 'I-DocumentReference', 'I-DocumentReference', 'I-DocumentReference', 'I-DocumentReference', 'I-DocumentReference', 'I-DocumentReference', 'I-DocumentReference', 'I-DocumentReference', 'O', 'B-Person', 'I-Person', 'I-Person', 'I-Person', 'O', 'O', 'O', 'O', 'O', 'B-DocumentReference', 'I-DocumentReference', 'I-DocumentReference', 'I-DocumentReference', 'I-DocumentReference', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'B-Temporal', 'O', 'B-Temporal', 'I-Temporal', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'B-Person', 'O', 'O', 'O', 'B-Person', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'O', 'O', 'O', 'B-Temporal', 'B-Location', 'O', 'O', 'B-Temporal', 'I-Temporal', 'O', 'O', 'B-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'B-Person', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'I-Person', 'I-Person', 'I-Person', 'O', 'O', 'O', 'O', 'B-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'B-Temporal', 'I-Temporal', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'I-Location', 'I-Location', 'I-Location', 'I-Location', 'O', 'B-Temporal', 'I-Temporal', 'I-Temporal', 'I-Temporal', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'B-Organisation', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'B-DocumentReference', 'I-DocumentReference', 'O', 'B-Location', 'I-Location', 'I-Location', 'O', 'O', 'O', 'B-Location', 'I-Location', 'O', 'B-Location', 'O', 'B-Location', 'O', 'B-Location', 'O', 'B-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'I-Location', 'I-Location', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'O', 'O', 'B-Location', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Temporal', 'I-Temporal', 'O', 'B-Location', 'O', 'B-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'B-Person', 'I-Person', 'O', 'B-Person', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'O', 'O', 'B-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'I-Location', 'I-Location', 'I-Location', 'I-Location', 'I-Location', 'I-Location', 'O', 'B-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'O', 'B-Organisation', 'I-Organisation', 'O', 'B-Temporal', 'I-Temporal', 'I-Temporal', 'I-Temporal', 'I-Temporal', 'O', 'O', 'O', 'B-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'B-Person', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'B-Location', 'I-Location', 'I-Location', 'I-Location', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'B-Location', 'I-Location', 'O', 'O', 'B-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'I-Person', 'I-Person', 'O', 'O', 'O', 'B-Person', 'I-Person', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'B-Person', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'B-Organisation', 'O', 'O', 'O', 'B-Temporal', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'B-Person', 'I-Person', 'I-Person', 'I-Person', 'O', 'B-Person', 'I-Person', 'I-Person', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'B-Temporal', 'I-Temporal', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Nationality', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'O', 'B-Person', 'O', 'B-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'I-Location', 'I-Location', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'B-Quantity', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'B-Location', 'I-Location', 'I-Location', 'O', 'O', 'O', 'B-Temporal', 'I-Temporal', 'I-Temporal', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'O', 'O', 'O', 'B-Location', 'I-Location', 'I-Location', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'O', 'B-Person', 'I-Person', 'I-Person', 'O', 'O', 'O', 'B-Location', 'I-Location', 'I-Location', 'I-Location', 'O', 'B-Temporal', 'I-Temporal', 'O', 'O', 'B-Person', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'O', 'B-DocumentReference', 'I-DocumentReference', 'I-DocumentReference', 'I-DocumentReference', 'I-DocumentReference', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Nationality', 'O', 'O', 'O', 'O', 'B-DocumentReference', 'I-DocumentReference', 'I-DocumentReference', 'O', 'O', 'O', 'O', 'B-DocumentReference', 'I-DocumentReference', 'O', 'O', 'B-Nationality', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Nationality', 'O', 'O', 'O', 'B-Nationality', 'I-Nationality', 'O', 'O', 'O', 'O', 'O', 'B-DocumentReference', 'I-DocumentReference', 'I-DocumentReference', 'I-DocumentReference', 'O', 'O', 'O', 'O', 'O', 'B-Nationality', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'I-Location', 'O', 'O', 'O', 'B-Temporal', 'I-Temporal', 'I-Temporal', 'I-Temporal', 'O', 'B-Person', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'B-Location', 'I-Location', 'I-Location', 'O', 'O', 'O', 'O', 'B-Location', 'I-Location', 'I-Location', 'O', 'O', 'B-Location', 'I-Location', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'B-Person', 'O', 'B-Location', 'O', 'B-Temporal', 'I-Temporal', 'I-Temporal', 'I-Temporal', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Weapon', 'O', 'B-Quantity', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'B-Quantity', 'B-MilitaryPlatform', 'I-MilitaryPlatform', 'O', 'B-Quantity', 'O', 'B-Weapon', 'O', 'O', 'B-Quantity', 'B-Location', 'I-Location', 'I-Location', 'I-Location', 'O', 'B-Quantity', 'B-Weapon', 'I-Weapon', 'I-Weapon', 'I-Weapon', 'O', 'B-Quantity', 'B-Weapon', 'I-Weapon', 'I-Weapon', 'I-Weapon', 'I-Weapon', 'O', 'B-Quantity', 'B-Weapon', 'I-Weapon', 'I-Weapon', 'I-Weapon', 'O', 'B-Location', 'I-Location', 'O', 'B-Location', 'I-Location', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Quantity', 'B-Location', 'I-Location', 'O', 'O', 'O', 'B-Quantity', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'B-Person', 'O', 'O', 'O', 'O', 'B-Location', 'O', 'B-Quantity', 'B-Weapon', 'O', 'B-Quantity', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'B-Quantity', 'B-Location', 'I-Location', 'I-Location', 'I-Location', 'O', 'B-Quantity', 'O', 'O', 'B-Quantity', 'B-Weapon', 'O', 'O', 'O', 'B-Weapon', 'I-Weapon', 'I-Weapon', 'I-Weapon', 'O', 'B-Quantity', 'B-Location', 'I-Location', 'I-Location', 'O', 'O', 'B-Quantity', 'B-Weapon', 'O', 'O', 'B-Quantity', 'B-Location', 'I-Location', 'O', 'O', 'O', 'B-Quantity', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'I-Location', 'I-Location', 'I-Location', 'I-Location', 'I-Location', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'O', 'B-Person', 'I-Person', 'I-Person', 'I-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Temporal', 'I-Temporal', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'I-Person', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Nationality', 'O', 'O', 'O', 'O', 'B-MilitaryPlatform', 'I-MilitaryPlatform', 'I-MilitaryPlatform', 'O', 'B-Weapon', 'I-Weapon', 'I-Weapon', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Quantity', 'B-MilitaryPlatform', 'O', 'B-Quantity', 'O', 'B-Weapon', 'O', 'B-Location', 'I-Location', 'I-Location', 'I-Location', 'O', 'O', 'O', 'B-Weapon', 'I-Weapon', 'O', 'B-Location', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'I-Location', 'O', 'O', 'B-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'B-Organisation', 'B-Weapon', 'I-Weapon', 'O', 'O', 'O', 'B-Location', 'O', 'B-Location', 'I-Location', 'O', 'B-Weapon', 'O', 'B-MilitaryPlatform', 'I-MilitaryPlatform', 'O', 'B-MilitaryPlatform', 'I-MilitaryPlatform', 'O', 'B-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Money', 'I-Money', 'I-Money', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'B-Money', 'I-Money', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'O', 'B-Person', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'I-Location', 'I-Location', 'O', 'B-Location', 'O', 'B-Location', 'I-Location', 'I-Location', 'O', 'B-Location', 'O', 'O', 'B-Location', 'I-Location', 'O', 'B-Quantity', 'O', 'O', 'B-Quantity', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'B-Weapon', 'O', 'B-Quantity', 'B-Location', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Quantity', 'O', 'O', 'O', 'B-Location', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'B-Location', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MilitaryPlatform', 'I-MilitaryPlatform', 'I-MilitaryPlatform', 'I-MilitaryPlatform', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'I-Person', 'I-Person', 'I-Person', 'B-Temporal', 'O', 'O', 'B-Person', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'O', 'B-Location', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'I-Location', 'I-Location', 'I-Location', 'I-Location', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DocumentReference', 'I-DocumentReference', 'I-DocumentReference', 'I-DocumentReference', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'B-Location', 'I-Location', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'B-Location', 'O', 'B-Person', 'I-Person', 'O', 'O', 'B-Person', 'O', 'B-Organisation', 'I-Organisation', 'O', 'B-Person', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'O', 'B-Temporal', 'I-Temporal', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'B-Temporal', 'I-Temporal', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'I-Person', 'I-Person', 'O', 'O', 'B-Weapon', 'O', 'O', 'O', 'O', 'B-Location', 'I-Location', 'I-Location', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Temporal', 'I-Temporal', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'B-Location', 'I-Location', 'I-Location', 'O', 'O', 'B-Temporal', 'I-Temporal', 'I-Temporal', 'I-Temporal', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'O', 'B-Location', 'I-Location', 'I-Location', 'O', 'O', 'B-Temporal', 'I-Temporal', 'I-Temporal', 'I-Temporal', 'O', 'O', 'O', 'B-Location', 'I-Location', 'O', 'B-Quantity', 'B-Weapon', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'I-Location', 'I-Location', 'O', 'B-Location', 'I-Location', 'O', 'B-Quantity', 'B-Weapon', 'I-Weapon', 'O', 'O', 'B-Weapon', 'O', 'O', 'B-MilitaryPlatform', 'O', 'O', 'B-MilitaryPlatform', 'O', 'O', 'B-Weapon', 'O', 'O', 'B-MilitaryPlatform', 'I-MilitaryPlatform', 'O', 'O', 'B-MilitaryPlatform', 'I-MilitaryPlatform', 'O', 'O', 'B-MilitaryPlatform', 'O', 'O', 'O', 'B-MilitaryPlatform', 'I-MilitaryPlatform', 'O', 'O', 'O', 'B-Location', 'I-Location', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'I-Location', 'I-Location', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'B-Location', 'O', 'B-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'B-Location', 'I-Location', 'O', 'O', 'O', 'O', 'B-Quantity', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'B-MilitaryPlatform', 'O', 'B-Quantity', 'B-Location', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Weapon', 'O', 'O', 'B-Weapon', 'I-Weapon', 'I-Weapon', 'I-Weapon', 'O', 'O', 'O', 'O', 'B-Quantity', 'I-Quantity', 'I-Quantity', 'B-DocumentReference', 'I-DocumentReference', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'O', 'B-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'B-Location', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'I-Location', 'O', 'O', 'B-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'O', 'B-Quantity', 'O', 'O', 'O', 'O', 'B-Quantity', 'B-Weapon', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'O', 'O', 'O', 'B-Organisation', 'O', 'B-Location', 'O', 'O', 'O', 'B-Location', 'I-Location', 'I-Location', 'I-Location', 'I-Location', 'I-Location', 'I-Location', 'I-Location', 'I-Location', 'I-Location', 'O', 'B-Location', 'I-Location', 'I-Location', 'I-Location', 'I-Location', 'I-Location', 'I-Location', 'I-Location', 'I-Location', 'I-Location', 'O', 'B-Location', 'I-Location', 'I-Location', 'I-Location', 'I-Location', 'I-Location', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'B-Temporal', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'O', 'B-Organisation', 'I-Organisation', 'O', 'B-Location', 'I-Location', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'B-Location', 'I-Location', 'O', 'O', 'O', 'O', 'O', 'B-Temporal', 'I-Temporal', 'O', 'B-Location', 'I-Location', 'I-Location', 'I-Location', 'O', 'B-Location', 'I-Location', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'I-Organisation', 'I-Organisation', 'O', 'B-Person', 'I-Person', 'I-Person', 'I-Person', 'O', 'O', 'O', 'B-Person', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'O', 'B-Person', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'O', 'O', 'B-Location', 'O', 'B-Temporal', 'I-Temporal', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'B-Organisation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "                     precision    recall  f1-score   support\n",
            "\n",
            "B-DocumentReference     0.0000    0.0000    0.0000         0\n",
            "         B-Location     0.3172    0.4876    0.3844       121\n",
            " B-MilitaryPlatform     0.0000    0.0000    0.0000         0\n",
            "            B-Money     0.0000    0.0000    0.0000         0\n",
            "      B-Nationality     0.0000    0.0000    0.0000         0\n",
            "     B-Organisation     0.2750    0.5620    0.3693       137\n",
            "           B-Person     0.3529    0.6923    0.4675        52\n",
            "         B-Quantity     0.0000    0.0000    0.0000         0\n",
            "         B-Temporal     0.0000    0.0000    0.0000         0\n",
            "           B-Weapon     0.0000    0.0000    0.0000         0\n",
            "I-DocumentReference     0.0000    0.0000    0.0000         0\n",
            "         I-Location     0.2226    0.4504    0.2980       131\n",
            " I-MilitaryPlatform     0.0000    0.0000    0.0000         0\n",
            "            I-Money     0.0000    0.0000    0.0000         0\n",
            "      I-Nationality     0.0000    0.0000    0.0000         0\n",
            "     I-Organisation     0.4639    0.5376    0.4981       359\n",
            "           I-Person     0.5723    0.6226    0.5964       159\n",
            "         I-Quantity     0.0000    0.0000    0.0000         0\n",
            "         I-Temporal     0.0000    0.0000    0.0000         0\n",
            "           I-Weapon     0.0000    0.0000    0.0000         0\n",
            "                  O     0.9702    0.7689    0.8579      4315\n",
            "\n",
            "           accuracy                         0.7283      5274\n",
            "          macro avg     0.1512    0.1963    0.1653      5274\n",
            "       weighted avg     0.8660    0.7283    0.7842      5274\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4wgG6R7ojLl"
      },
      "source": [
        "#### Discussion\n",
        "As we can see from above result, the performance is poor witout CRF. This indicate that the LSTM model with CRF can handle the dependency between predicted entity names, which has a significant impact to improve prediction accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHw4M_nILNzv"
      },
      "source": [
        "## 8 Predicting Test Data with the Best Model\n",
        "using which one?\n",
        "\n",
        "the best model we choosed is:\n",
        "\n",
        "BiLSRM with CRF,\n",
        "\n",
        "num_layers = 1,\n",
        "\n",
        "attention way = dot product,\n",
        "\n",
        "input embedding = pos-tagging + dependency parsing + glove-50"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EJGYHckLXdT"
      },
      "source": [
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "p_EMBEDDING_DIM = EMBEDDING_DIM_50\n",
        "p_embedding_matrix = embedding_matrix_50\n",
        "NUM_LAYERS = 1\n",
        "\n",
        "def cal_acc(model, input_index, val_POS_index ,val_DEP_index, output_index,method):\n",
        "    ground_truth = []\n",
        "    predicted = []\n",
        "    for i,idxs in enumerate(input_index):\n",
        "        ground_truth += output_index[i]\n",
        "        pos_idxs = val_POS_index[i]\n",
        "        dep_idxs = val_DEP_index[i]\n",
        "\n",
        "        postaglist_in = torch.tensor(pos_idxs, dtype=torch.long).to(device)\n",
        "        deptaglist_in = torch.tensor(dep_idxs, dtype=torch.long).to(device)\n",
        "        sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "\n",
        "        score, pred = model(sentence_in,postaglist_in,deptaglist_in,method)\n",
        "        predicted += pred\n",
        "    accuracy = sum(np.array(ground_truth) == np.array(predicted))/len(ground_truth)\n",
        "    return predicted, ground_truth, accuracy\n",
        "\n",
        "class BiLSTM_CRF(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):\n",
        "        super(BiLSTM_CRF, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "        self.tagset_size = len(tag_to_ix)\n",
        "\n",
        "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
        "        \"\"\"Here we use the embedding matrix as the initial weights of nn.Embedding\"\"\"\n",
        "        self.word_embeds.weight.data.copy_(torch.from_numpy(p_embedding_matrix))\n",
        "\n",
        "        self.postag_embeds = nn.Embedding(embedding_matrix_POS.shape[0], EMBEDDING_DIM_POS)\n",
        "        self.postag_embeds.weight.data.copy_(torch.from_numpy(embedding_matrix_POS))\n",
        "\n",
        "        self.deptag_embeds = nn.Embedding(embedding_matrix_DEP.shape[0], EMBEDDING_DIM_DEP)\n",
        "        self.deptag_embeds.weight.data.copy_(torch.from_numpy(embedding_matrix_DEP))\n",
        "        \n",
        "        self.lstm = nn.LSTM(embedding_dim+EMBEDDING_DIM_POS+EMBEDDING_DIM_DEP, hidden_dim // 2,\n",
        "                            num_layers=NUM_LAYERS, bidirectional=True)\n",
        "\n",
        "        # Maps the output of the LSTM into tag space.\n",
        "        self.hidden2tag = nn.Linear(hidden_dim*2, self.tagset_size)\n",
        "\n",
        "        # Matrix of transition parameters.  Entry i,j is the score of\n",
        "        # transitioning *to* i *from* j.\n",
        "        self.transitions = nn.Parameter(\n",
        "            torch.randn(self.tagset_size, self.tagset_size))\n",
        "\n",
        "        # These two statements enforce the constraint that we never transfer\n",
        "        # to the start tag and we never transfer from the stop tag\n",
        "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
        "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
        "\n",
        "        self.hidden = self.init_hidden()\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return (torch.randn(2*NUM_LAYERS, 1, self.hidden_dim // 2).to(device),\n",
        "                torch.randn(2*NUM_LAYERS, 1, self.hidden_dim // 2).to(device))\n",
        "    # from lab 10    \n",
        "    def cal_attention(self, decoder_hiddens, encoder_hiddens, method):\n",
        "        if method == \"Dot Product\":\n",
        "            # bmm: https://pytorch.org/docs/master/generated/torch.bmm.html\n",
        "            attn_weights = F.softmax(torch.bmm(decoder_hiddens.unsqueeze(0), encoder_hiddens.T.unsqueeze(0)),dim=-1)\n",
        "            attn_output = torch.bmm(attn_weights, encoder_hiddens.unsqueeze(0))\n",
        "            concat_output = torch.cat((attn_output[0], decoder_hiddens), 1)\n",
        "\n",
        "\n",
        "        return concat_output\n",
        "    def _forward_alg(self, feats):\n",
        "        # Do the forward algorithm to compute the partition function\n",
        "        init_alphas = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "        # START_TAG has all of the score.\n",
        "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
        "\n",
        "        # Wrap in a variable so that we will get automatic backprop\n",
        "        forward_var = init_alphas\n",
        "\n",
        "        # Iterate through the sentence\n",
        "        for feat in feats:\n",
        "            alphas_t = []  # The forward tensors at this timestep\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # broadcast the emission score: it is the same regardless of\n",
        "                # the previous tag\n",
        "                emit_score = feat[next_tag].view(\n",
        "                    1, -1).expand(1, self.tagset_size)\n",
        "                # the ith entry of trans_score is the score of transitioning to\n",
        "                # next_tag from i\n",
        "                trans_score = self.transitions[next_tag].view(1, -1)\n",
        "                # The ith entry of next_tag_var is the value for the\n",
        "                # edge (i -> next_tag) before we do log-sum-exp\n",
        "                next_tag_var = forward_var + trans_score + emit_score\n",
        "                # The forward variable for this tag is log-sum-exp of all the\n",
        "                # scores.\n",
        "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
        "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        alpha = log_sum_exp(terminal_var)\n",
        "        return alpha\n",
        "\n",
        "    def _get_lstm_features(self, sentence, postaglist, deptaglist, method):\n",
        "        self.hidden = self.init_hidden()\n",
        "        wordembeds = self.word_embeds(sentence)\n",
        "        postagembeds = self.postag_embeds(postaglist)\n",
        "        deptagembeds = self.deptag_embeds(deptaglist)\n",
        "        embeds = torch.cat((wordembeds, postagembeds, deptagembeds), 1).view(len(sentence), 1, -1)\n",
        "        \n",
        "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
        "\n",
        "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
        "        concat_output = self.cal_attention(lstm_out, lstm_out, method) #concat lstm output and attention output\n",
        "\n",
        "        lstm_feats = self.hidden2tag(concat_output)\n",
        "        return lstm_feats\n",
        "\n",
        "    def _score_sentence(self, feats, tags):\n",
        "        # Gives the score of a provided tag sequence\n",
        "        score = torch.zeros(1).to(device)\n",
        "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long).to(device), tags])\n",
        "        for i, feat in enumerate(feats):\n",
        "            score = score + \\\n",
        "                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
        "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
        "        return score\n",
        "\n",
        "    def _viterbi_decode(self, feats):\n",
        "        backpointers = []\n",
        "\n",
        "        # Initialize the viterbi variables in log space\n",
        "        init_vvars = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
        "\n",
        "        # forward_var at step i holds the viterbi variables for step i-1\n",
        "        forward_var = init_vvars\n",
        "        for feat in feats:\n",
        "            bptrs_t = []  # holds the backpointers for this step\n",
        "            viterbivars_t = []  # holds the viterbi variables for this step\n",
        "\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
        "                # previous step, plus the score of transitioning\n",
        "                # from tag i to next_tag.\n",
        "                # We don't include the emission scores here because the max\n",
        "                # does not depend on them (we add them in below)\n",
        "                next_tag_var = forward_var + self.transitions[next_tag]\n",
        "                best_tag_id = argmax(next_tag_var)\n",
        "                bptrs_t.append(best_tag_id)\n",
        "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
        "            # Now add in the emission scores, and assign forward_var to the set\n",
        "            # of viterbi variables we just computed\n",
        "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
        "            backpointers.append(bptrs_t)\n",
        "\n",
        "        # Transition to STOP_TAG\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        best_tag_id = argmax(terminal_var)\n",
        "        path_score = terminal_var[0][best_tag_id]\n",
        "\n",
        "        # Follow the back pointers to decode the best path.\n",
        "        best_path = [best_tag_id]\n",
        "        for bptrs_t in reversed(backpointers):\n",
        "            best_tag_id = bptrs_t[best_tag_id]\n",
        "            best_path.append(best_tag_id)\n",
        "        # Pop off the start tag (we dont want to return that to the caller)\n",
        "        start = best_path.pop()\n",
        "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
        "        best_path.reverse()\n",
        "        return path_score, best_path\n",
        "\n",
        "    def neg_log_likelihood(self, sentence, postaglist,deptaglist, tags,method):\n",
        "        feats = self._get_lstm_features(sentence,postaglist,deptaglist,method)\n",
        "        forward_score = self._forward_alg(feats)\n",
        "        gold_score = self._score_sentence(feats, tags)\n",
        "        return forward_score - gold_score\n",
        "\n",
        "    def forward(self, sentence, postaglist,deptaglist,method):  # dont confuse this with _forward_alg above.\n",
        "        # Get the emission scores from the BiLSTM\n",
        "        lstm_feats = self._get_lstm_features(sentence,postaglist,deptaglist,method)\n",
        "\n",
        "        # Find the best path, given the features.\n",
        "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
        "        return score, tag_seq\n",
        "\n",
        "\n",
        "import datetime\n",
        "\"\"\"Each epoch will take about 2-3 minutes\"\"\"\n",
        "def model_performance_attention(model, ATTENTION_METHOD):\n",
        "\n",
        "  for epoch in range(max_epoch):  \n",
        "      time1 = datetime.datetime.now()\n",
        "      train_loss = 0\n",
        "\n",
        "      model.train()\n",
        "      for i, idxs in enumerate(train_input_index):\n",
        "          tags_index = train_output_index[i]\n",
        "          pos_idxs = train_POS_index[i]\n",
        "          dep_idxs = train_DEP_index[i]\n",
        "\n",
        "          # Step 1. Remember that Pytorch accumulates gradients.\n",
        "          # We need to clear them out before each instance\n",
        "          model.zero_grad()\n",
        "\n",
        "          # Step 2. Get our inputs ready for the network, that is,\n",
        "          # turn them into Tensors of word indices.\n",
        "          sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "          postaglist_in = torch.tensor(pos_idxs, dtype=torch.long).to(device)\n",
        "          deptaglist_in = torch.tensor(dep_idxs, dtype=torch.long).to(device)\n",
        "\n",
        "          targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "\n",
        "          # Step 3. Run our forward pass.\n",
        "          loss = model.neg_log_likelihood(sentence_in,postaglist_in,deptaglist_in,targets,ATTENTION_METHOD)\n",
        "\n",
        "          # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "          # calling optimizer.step()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          train_loss+=loss.item()\n",
        "\n",
        "      model.eval()\n",
        "      _, _, train_acc = cal_acc(model,train_input_index,train_POS_index,train_DEP_index,train_output_index,ATTENTION_METHOD)\n",
        "      _, _, val_acc = cal_acc(model,val_input_index,val_POS_index,val_DEP_index,val_output_index,ATTENTION_METHOD)\n",
        "\n",
        "      val_loss = 0\n",
        "      for i, idxs in enumerate(val_input_index):\n",
        "          tags_index = val_output_index[i]\n",
        "          pos_idxs = val_POS_index[i]\n",
        "          dep_idxs = val_DEP_index[i]\n",
        "          sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "          postaglist_in = torch.tensor(pos_idxs, dtype=torch.long).to(device)\n",
        "          deptaglist_in = torch.tensor(dep_idxs, dtype=torch.long).to(device)\n",
        "          targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "          loss = model.neg_log_likelihood(sentence_in, postaglist_in, deptaglist_in, targets, ATTENTION_METHOD)\n",
        "          val_loss+=loss.item()\n",
        "      time2 = datetime.datetime.now()\n",
        "\n",
        "      print(\"Epoch:%d, Training loss: %.2f, train acc: %.4f, val loss: %.2f, val acc: %.4f, time: %.2fs\" %(epoch+1, train_loss,train_acc, val_loss, val_acc, (time2-time1).total_seconds()))\n",
        "\n",
        "  y_true,y_pred,_ = cal_acc(model,val_input_index,val_POS_index,val_DEP_index,val_output_index,ATTENTION_METHOD)\n",
        "  y_true_decode = decode_output(y_true)\n",
        "  y_pred_decode = decode_output(y_pred)\n",
        "\n",
        "  print(classification_report(y_true_decode,y_pred_decode,digits=4))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2KuKvrWw3qr",
        "outputId": "8758eb0b-a0b7-41bd-d8ff-2fbb721275d2"
      },
      "source": [
        "ATTENTION_METHOD = \"Dot Product\"\n",
        "\n",
        "# initialize model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_Dot_Product= BiLSTM_CRF(len(word_to_ix), tag_to_ix, p_EMBEDDING_DIM, HIDDEN_DIM).to(device)\n",
        "optimizer = optim.SGD(model_Dot_Product.parameters(), lr=0.01, weight_decay=1e-4)\n",
        "\n",
        "#model performance dot product\n",
        "model_performance_attention(model_Dot_Product,ATTENTION_METHOD)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:1, Training loss: 13884.36, train acc: 0.7761, val loss: 4404.93, val acc: 0.7184, time: 183.28s\n",
            "Epoch:2, Training loss: 7731.58, train acc: 0.8117, val loss: 3439.68, val acc: 0.7488, time: 183.66s\n",
            "Epoch:3, Training loss: 5888.50, train acc: 0.8354, val loss: 2967.22, val acc: 0.7704, time: 187.03s\n",
            "Epoch:4, Training loss: 4868.83, train acc: 0.8557, val loss: 2626.43, val acc: 0.7865, time: 188.17s\n",
            "Epoch:5, Training loss: 4199.38, train acc: 0.8699, val loss: 2430.96, val acc: 0.7964, time: 187.55s\n",
            "Epoch:6, Training loss: 3676.37, train acc: 0.8840, val loss: 2307.70, val acc: 0.8051, time: 187.35s\n",
            "Epoch:7, Training loss: 3244.92, train acc: 0.8954, val loss: 2227.89, val acc: 0.8127, time: 187.15s\n",
            "Epoch:8, Training loss: 2864.24, train acc: 0.9057, val loss: 2171.74, val acc: 0.8195, time: 183.65s\n",
            "Epoch:9, Training loss: 2571.14, train acc: 0.9140, val loss: 2155.57, val acc: 0.8189, time: 185.07s\n",
            "Epoch:10, Training loss: 2235.05, train acc: 0.9224, val loss: 2235.16, val acc: 0.8163, time: 185.51s\n",
            "Epoch:11, Training loss: 2029.83, train acc: 0.9267, val loss: 2276.88, val acc: 0.8165, time: 184.56s\n",
            "Epoch:12, Training loss: 1823.76, train acc: 0.9315, val loss: 2274.22, val acc: 0.8180, time: 184.31s\n",
            "Epoch:13, Training loss: 1691.23, train acc: 0.9349, val loss: 2367.56, val acc: 0.8127, time: 183.89s\n",
            "Epoch:14, Training loss: 1463.82, train acc: 0.9450, val loss: 2404.99, val acc: 0.8165, time: 184.03s\n",
            "Epoch:15, Training loss: 1387.13, train acc: 0.9419, val loss: 2514.58, val acc: 0.8189, time: 183.15s\n",
            "Epoch:16, Training loss: 1189.60, train acc: 0.9577, val loss: 2498.90, val acc: 0.8161, time: 183.90s\n",
            "Epoch:17, Training loss: 1201.23, train acc: 0.9503, val loss: 2564.17, val acc: 0.8163, time: 183.38s\n",
            "Epoch:18, Training loss: 1047.54, train acc: 0.9579, val loss: 2571.27, val acc: 0.8161, time: 183.99s\n",
            "Epoch:19, Training loss: 966.86, train acc: 0.9520, val loss: 2771.91, val acc: 0.8148, time: 184.06s\n",
            "Epoch:20, Training loss: 937.85, train acc: 0.9577, val loss: 2723.89, val acc: 0.8189, time: 184.32s\n",
            "                     precision    recall  f1-score   support\n",
            "\n",
            "B-DocumentReference     0.3000    0.5000    0.3750        12\n",
            "         B-Location     0.6774    0.6848    0.6811       184\n",
            " B-MilitaryPlatform     0.0000    0.0000    0.0000         1\n",
            "            B-Money     0.2000    0.5000    0.2857         2\n",
            "      B-Nationality     0.1250    0.1429    0.1333         7\n",
            "     B-Organisation     0.6321    0.7080    0.6679       250\n",
            "           B-Person     0.8529    0.8365    0.8447       104\n",
            "         B-Quantity     0.7818    0.7049    0.7414        61\n",
            "         B-Temporal     0.7447    0.7609    0.7527        46\n",
            "           B-Weapon     0.1579    0.2609    0.1967        23\n",
            "I-DocumentReference     0.3373    0.6667    0.4480        42\n",
            "         I-Location     0.5434    0.7024    0.6128       205\n",
            " I-MilitaryPlatform     0.0000    0.0000    0.0000         0\n",
            "            I-Money     0.6000    1.0000    0.7500         6\n",
            "      I-Nationality     0.0000    0.0000    0.0000         0\n",
            "     I-Organisation     0.5721    0.7604    0.6529       313\n",
            "           I-Person     0.7225    0.8446    0.7788       148\n",
            "         I-Quantity     0.2692    0.7000    0.3889        10\n",
            "         I-Temporal     0.8065    0.8065    0.8065        62\n",
            "           I-Weapon     0.1111    0.3333    0.1667        15\n",
            "                  O     0.9453    0.8546    0.8977      3783\n",
            "\n",
            "           accuracy                         0.8187      5274\n",
            "          macro avg     0.4466    0.5603    0.4848      5274\n",
            "       weighted avg     0.8547    0.8187    0.8327      5274\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIiKRrW4zdPb"
      },
      "source": [
        "save model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-U_60gkzW9u",
        "outputId": "18f90c82-9936-4722-d571-44c2bf1ff19d"
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnfNRhPGzmFR"
      },
      "source": [
        "# Save the Model\n",
        "torch.save(model_Dot_Product, '/content/gdrive/MyDrive/2021-comp5046-a2/Best_NER_Model.pt')"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymw89xbvzgIH"
      },
      "source": [
        "load model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qB8Gf4Pzpg4"
      },
      "source": [
        "# Load the model\n",
        "# Code to download file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "id = '16366bjjdy3fyEEsDGiTWcg2bXYGwk-sQ'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('Best_NER_Model.pt')\n",
        "\n",
        "Best_NER_Model = torch.load('Best_NER_Model.pt')"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ruB9320JzxJq",
        "outputId": "5d1fc968-fe22-4608-c711-5576ff37e5c7"
      },
      "source": [
        "y_true,y_pred,_ = cal_acc(Best_NER_Model,val_input_index,val_POS_index,val_DEP_index,val_output_index,ATTENTION_METHOD)\n",
        "y_true_decode = decode_output(y_true)\n",
        "y_pred_decode = decode_output(y_pred)\n",
        "\n",
        "print(classification_report(y_true_decode,y_pred_decode,digits=4))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                     precision    recall  f1-score   support\n",
            "\n",
            "B-DocumentReference     0.2500    0.5000    0.3333        10\n",
            "         B-Location     0.6667    0.6739    0.6703       184\n",
            " B-MilitaryPlatform     0.0000    0.0000    0.0000         1\n",
            "            B-Money     0.2000    0.5000    0.2857         2\n",
            "      B-Nationality     0.1250    0.1429    0.1333         7\n",
            "     B-Organisation     0.6321    0.7024    0.6654       252\n",
            "           B-Person     0.8725    0.8241    0.8476       108\n",
            "         B-Quantity     0.7636    0.6885    0.7241        61\n",
            "         B-Temporal     0.7660    0.7500    0.7579        48\n",
            "           B-Weapon     0.1579    0.2500    0.1935        24\n",
            "I-DocumentReference     0.3253    0.6923    0.4426        39\n",
            "         I-Location     0.5434    0.7236    0.6207       199\n",
            " I-MilitaryPlatform     0.0000    0.0000    0.0000         0\n",
            "            I-Money     0.6000    1.0000    0.7500         6\n",
            "      I-Nationality     0.0000    0.0000    0.0000         0\n",
            "     I-Organisation     0.5841    0.7500    0.6568       324\n",
            "           I-Person     0.7225    0.8503    0.7813       147\n",
            "         I-Quantity     0.2308    0.6667    0.3429         9\n",
            "         I-Temporal     0.8387    0.7647    0.8000        68\n",
            "           I-Weapon     0.1111    0.3125    0.1639        16\n",
            "                  O     0.9442    0.8567    0.8983      3769\n",
            "\n",
            "           accuracy                         0.8187      5274\n",
            "          macro avg     0.4445    0.5547    0.4794      5274\n",
            "       weighted avg     0.8545    0.8187    0.8326      5274\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWUwc3zYkv-h"
      },
      "source": [
        "get prediction files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYKondNqkW26"
      },
      "source": [
        "#create test predictions to submit\n",
        "test_input_index = to_index(dftest.sents, word_to_ix)\n",
        "t_test_input_index = test_POS_index\n",
        "d_test_input_index = test_DEP_index\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def cal_acc(model, input_index, val_POS_index ,val_DEP_index):\n",
        "    ground_truth = []\n",
        "    predicted = []\n",
        "    for i,idxs in enumerate(input_index):\n",
        "        pos_idxs = val_POS_index[i]\n",
        "        dep_idxs = val_DEP_index[i]\n",
        "\n",
        "        postaglist_in = torch.tensor(pos_idxs, dtype=torch.long).to(device)\n",
        "        deptaglist_in = torch.tensor(dep_idxs, dtype=torch.long).to(device)\n",
        "        sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "\n",
        "        score, pred = model(sentence_in,postaglist_in,deptaglist_in,ATTENTION_METHOD)\n",
        "        predicted += pred\n",
        "    \n",
        "    return predicted\n",
        "\n",
        "y_pred_test = cal_acc(Best_NER_Model, test_input_index,t_test_input_index,d_test_input_index)\n",
        "\n",
        "def decode_output(output_list):\n",
        "    ix_to_tag = {v:k for k,v in tag_to_ix.items()}\n",
        "    return [ix_to_tag[output] for output in output_list]\n",
        "\n",
        "test_output = decode_output(y_pred_test)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3l7uOJyDl82S"
      },
      "source": [
        "df = pd.DataFrame({'ID' :  np.arange(0, len(test_output)),'Predicted': test_output})\n",
        "df.to_csv('Best_predictions.csv',index=False)"
      ],
      "execution_count": 29,
      "outputs": []
    }
  ]
}